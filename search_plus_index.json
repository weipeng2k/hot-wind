{"./":{"url":"./","title":"热风-简介","keywords":"","body":"热风（hot-wind）         《热风》，是鲁迅先生的杂文集，所谓杂文，没有文体的限制，看似杂，实而不杂。先生说，愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火，这句话看着很有感觉。         我的《热风》不是给人打鸡血，而是提醒自己不要停下学习和分享，始终保有热度，写一些技术文章，时刻保有提交的感觉，是作为一个开发者的一部分，也是想成为的样子。纵使在知乎，文章的编写总是缺少一种提交的感觉，所以还是在github上来的舒坦。         看一些技术，总结一下，写一下，感觉是很好的。文章的组织有些杂，想到那里就写到那里，但是我会不断的收纳整理。 summary 热风-简介 热风-关于作者 热风-CAP小结 热风-补偿机制说明 热风-拜占庭将军问题 热风-共识协议：两阶段提交 热风-一种基于补偿的事务处理机制（TCC） 热风-《如何设计一个分布式数据库》观后感 热风-使用Seata来实现TCC 热风-未来的预测 热风-Seata存在的一些问题 热风-InetAddress与DNS 热风-ARP与pcap4j 热风-TcpDump与Redis客户端 热风-Java开发者看C 热风-C语言概述 热风-C语言的基本概念 热风-基础Paxos算法的9页笔记 热风-基础Paxos算法笔记(1/9) 热风-基础Paxos算法笔记(2/9) 热风-基础Paxos算法笔记(3/9) 热风-基础Paxos算法笔记(4/9) 热风-基础Paxos算法笔记(5/9) 热风-基础Paxos算法笔记(6/9) 热风-基础Paxos算法笔记(7/9) 热风-基础Paxos算法笔记(8/9) 热风-基础Paxos算法笔记(9/9) 热风-分布式锁 热风-锁是什么？ 热风-分布式锁是什么？ 热风-实现分布式锁会遇到的问题 热风-分布式锁框架 热风-拉模式的分布式锁 热风-推模式的分布式锁 热风-使用Redis实现分布式锁 热风-如何实现分布式锁 热风-Redlock能保证锁的正确性吗？ 热风-再看分布式锁 热风-更新到Eclipse Temurin 热风-使用JetBrains Toolbox来管理IDE 热风-成为更好程序员的8本书 热风-JUC总结 热风-JUC中的StampedLock文档 热风-《Linux系统高效同步》SeqLock摘译 热风-StampedLock简介 热风-StampedLock的接口与示例 热风-StampedLock的实现分析 热风-程序是怎样跑起来的 热风-对程序员来说CPU是什么 热风-数据是用二进制数表示的 热风-计算机进行小数运算时出错的原因 热风-熟练使用有棱有角的内存 热风-内存和磁盘的亲密关系 热风-亲自尝试压缩数据 热风-程序是在何种环境中运行的 热风-从源文件到可执行文件 热风-操作系统和应用的关系 热风-通过汇编语言了解程序的实际构成 热风-硬件控制方法 热风-程序！程序！程序！ 热风-程序的视角 热风-硬件的执念 热风-编译的力量 热风-数据的表达 热风-协同的奥义 热风-如何更好的做单元测试 热风-软件开发与单元测试 热风-Java程序员使用JUnit做单元测试 热风-基于Spring的单元测试 热风-维护好单元测试 热风-体验测试驱动开发 热风-感悟《计算机网络：自顶向下》 热风-感悟《计算机网络：自顶向下》（01.概述） 热风-感悟《计算机网络：自顶向下》（02.应用层） 热风-感悟《计算机网络：自顶向下》（03a.传输层） 热风-感悟《计算机网络：自顶向下》（03b.传输层） 热风-感悟《计算机网络：自顶向下》（04a.网络层） 热风-感悟《计算机网络：自顶向下》（04b.网络层） 热风-感悟《计算机网络：自顶向下》（05a.数据链路层） 热风-《计算机网络》第5章-数据链路层（b.以太网） 热风-《计算机网络》第6章-物理层（a.基本概念） 热风-聊一聊架构 热风-使用Javac's Release选项 热风-谈一谈设计范式 热风-说一说系统复杂性 热风-我怎么写概要设计？ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"ABOUTME.html":{"url":"ABOUTME.html","title":"热风-关于作者","keywords":"","body":"关于作者         魏鹏（花名：豫楚），原阿里巴巴集团高级技术专家，2009年硕士研究生毕业加入阿里巴巴中国网站技术部，2021年从阿里云离职。         12年阿里经历有三段：         第一段，提升技术能力。2009年硕士研究生毕业加入阿里巴巴中国网站技术部，曾担任中国网站交易平台架构师，主导了交易系统服务化工作，设计实现的数据迁移系统高效的完成了阿里巴巴中国网站交易数据到阿里巴巴集团的迁移工作；         第二段，耕耘技术深度。2013年在阿里巴巴中间件技术部的应用容器与服务框架组，从事（后负责）Java应用容器Pandora和服务框架HSF的相关工作，其中Java应用容器Pandora是阿里巴巴中间件运行的基础，而服务框架HSF则是阿里巴巴集团实现服务化的主要解决方案，二者在阿里巴巴拥有最为广泛的使用量；         第三段，实践技术架构。2018年在阿里巴巴新零售事业群旗下的零售通技术部，带领团队，实践微服务架构改造，建立团队技术自信，支撑复杂业务场景；         个人平时喜欢阅读技术书籍，与同事合著了《Java并发编程的艺术》一书，并翻译一些国外优秀文档，爱总结，喜分享，对Java应用容器、多线程编程、微服务架构以及分布式系统感兴趣，深知 “路漫漫其修远兮，吾将上下而求索” ，希望能够和大家多多交流共同进步。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/cap-brief-summary.html":{"url":"book/cap-brief-summary.html","title":"热风-CAP小结","keywords":"","body":"CAP小结        本文是对CAP定理的简介、证明以及思考。其中理论证明部分主要来自于对Gilbert与Lynch的两篇论文（《Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services》 和 《Perspectives on the CAP Theorem》）的理解。 CAP定理简介        CAP是一致性（Consistency）、可用性（Availability）以及分区容忍性（Partition Tolerance）的首字母缩写，而CAP定理要说明的是，针对这三种特性，一个分布式系统，同时只能满足两个。在分布式环境中，系统大都通过网络链接，彼此之间存在分区，所以分区容忍性往往是所处的现实基础，这个定理也可以这么说，在一个容易出现错误的分布式环境中，无法同时满足一致性和可用性。        由于三者不可得兼，而分区容忍性无法避免，所以业界既有很多CP和AP选择的分布式系统，比如：BASE思想（所谓软状态，最终一致性）就是AP的一种成功实践。        针对三种特性，分别做一些解释。        一致性（Consistency），表示分布式系统的不同分区的数据副本之间是一致的，这么定义有些严格，宽泛的讲，从外部请求这个分布式系统，能够有一致的存取和访问结果，先进行存储，后进行读取能够读到最新的值。如果由于分区的存在，导致分区之间数据不一致，这样一致性就无法兑现，但通过数据同步使得数据在一段延迟后重新一致，这种情况可以称之为最终一致性或者瞬时不一致。        可用性（Avaiability），表示分布式系统在任何时刻都能够提供服务的能力。如果分布式系统能够在理论上满足任何时刻可以提供服务，那么这个分布式系统具备良好的可用性。如果由于分区的存在，导致系统从整体上看，有稳定性风险，那么该分布式系统的可用性是不足的。        分区容忍性（Partition Tolerance），表示分布式系统多个分区（或者副本）之间能够封闭运行，可以在一个分区出现通信问题或者分区之间存在通信问题的情况下，系统对外表现工作良好。如果一个分布式系统，能够在分区或者分区之间通信不稳定的情况下稳定工作，那么表明该系统具备良好的分区容忍性。 CAP定理证明        在2000年的分布式计算原理会议（PODC）上，Brewer提出了一个猜想：对一个Web服务，是无法同时保证一致性、可用性和分区容忍性的。MIT的Lynch等人针对这个猜想做了详细的论证和分析，一定程度上形式化证明了该猜想，并基于该猜想，探讨了分布式环境下，如何能够更实际的认识、面对和处理这三者之间的关系。 异步网络模型下证明CAP        定理1. 在一个异步网络模型的分布式系统中，是无法同时保证可用性和一致性的。        在异步网络模型下的分布式系统，各节点没有时钟协调，仅通过收到远程消息并进行本地计算来完成工作。异步网络模型的特点是被动的接受消息并处理，而结果也通过消息进行传递，这个过程可能存在消息的丢失或者消息（由于投递而）延迟。简单可以把它理解为一个完全有异步消息连接的分布式系统，各个节点如同一个个的任务，不断的消费消息，并产生消息，异步网络的效率高，但是相对可靠性差，原因就是没有过多的约束来强化可靠性。        证明：使用反证法。假定存在一个算法，在异步网络的不同的节点 {G1, G2}，能够同时满足CAP特性，其中G1和G2保存的数据为初始值V0。假定存在两个操作，其中：操作A，写请求G1，将V0改为V1；操作B，读请求G2，获取值。操作A先于操作B，由于当前算法满足CAP，所以操作B获取到的值是V1。由于G1和G2之间是异步网络，G1和G2之间的数据同步消息存在丢失和延迟，操作B返回的值很有可能还是V0，这样实际情况就违反了一致性，因此不存在有类似算法在异步网络模型的分布式系统中，同时满足CAP特性。证明的参考过程如下图所示：        通过证明可以看出，在异步网络模型下，CAP特性无法同时满足，但是做到其中两个，还是可以的。如果选择CP，那么被交换的就是A，通过消息传递时的相互确认，能够确保一致性得以保证，特别是建立在分布式锁或者分布式多写方案上的数据存储服务，大都属于CP类型。当然它们对于A的交换并不是视而不见，而是通过一些备份的手段加以保障，比如：一主多备的方式，来提升一定可用性，避免长时间的宕机，这种模式一般叫做CP+高可用。如果选择CA，那么被交换的就是P，这样就在一个中心化系统结构下，是比较容易做到CA的，在局域网中运作的系统，往往就是这种，比如：局域网中部署一个MySQL实例，处于同一局域网的应用服务对其进行访问。如果选择AP，那么被交换的就是C，一个高可靠和能够分区的分布式系统是很有价值的，比如：Web服务中的缓存（或者CDN），这种选择并不是对于一致性放任不管，而是允许弱一致性，或者说在一段时间后数据一定会在不同分区形成一致，这就是我们常说的最终一致性。 部分同步网络模型下证明CAP        定理2. 在一个部分同步网络模型的分布式系统中，是无法同时保证可用性和一致性的。        部分同步网络模型下的分布式系统，各个节点拥有一个时钟，时钟之间没有同步，是各个节点私有的，只是跳动频率一致，这点看起来仿佛比异步网络模型没有强多少，但是在实际功能上还是比异步网络模型要多一些。比如：部分同步网络由于节点时钟的存在，就可以知道发送一个消息后，如果多长时间没有回执，那么就可以判定这个消息可能无效，而异步网络模型无法做到这点。在部分同步网络模型下，可以实现确定性的请求和应答机制，以及随之而来的，“无效”的请求和应答机制（也就是调用超时，对方可能没有收到，判定当次请求无效）。        证明方式同异步网络模型类似，都是由于通信的不可靠性，在部分同步网络模型的分布式系统中，不存在一个算法同时满足CAP特性。        在部分同步网络模型下，传输约束高于异步网络模型，可以做到有条件的弱一致性，这个有条件表示的是可预期的延迟。由于该网络模型下能够确定消息传输的确定性，因此在一段（或者说可以预知的）时间延迟下，能够在P的前提下达成C。当然A也是可以做到的，因为在分区的节点之间，一个节点的访问，不会要求其他节点强参与，这样就做到了A。C是要求所有节点的数据能够一致，这点在部分同步网络模型下，分布式系统节点之间数据完成同步传输的操作（一般是同步消息传递）是可以预期的，所谓预期就是可预知时间的延迟，也就是说能够建立一个在特定时长后可以保证数据一致性的（弱一致性）分布式系统。 CAP定理思考        分区容忍性需要考虑不同分区之间是无法访问的。一般意义上，分布式系统都会有P，那么在分布式系统中剩下的就是C和A选择哪个了。如果是TDDL（淘宝分布式数据访问层）类似的系统，多个原子数据库之间不能相互访问，那么就剩下C和A的选择了，目前看选择了C。 对一致性的思考        一致性在不同的场景上，实际上有不同的要求，有强弱的区别。按照不同的场景，从服务的角度看一致性，如下表所示： 服务类型 描述 场景 不重要的服务 在服务集群中，节点之间不需要通信，它们只是只读的返回信息 地址服务或者一些元知识服务，这种服务对于一致性的要求就很低，在一定程度上会避免落入CAP的矛盾选择中 弱一致性服务 在分布式服务中，更加看重可用性，一致性最终会达成 AP类型的存储服务，比如：分布式缓存服务 简单服务 提供了简单操作的一致性保证 CP类型的存储服务，比如：HBase、MongoDB 复杂服务 有复杂的分布式协调或者事务语义的服务，也可能是多个简单服务或其他类型服务的叠加 对分区容忍性的思考        在分区一定会存在的分布式环境中，对于分区，有以下若干模式，如下表所示： 分区模式 描述 场景 数据分区 从数据类型下手，不同的数据类型对于一致性要求是不一样的，在不同的数据类型对应的系统设计时，根据它的实际场景，考量取舍 在一个电商系统中，购物车和商品这两个数据类型（或者说支撑它们的分布式系统），对于一致性的要求一定是不同的，商品对于一致性的要求比购物车低。因此商品系统可以多考虑缓存技术，而购物车系统需要关注CP类的存储 操作分区 根据操作维度进行分区，对于数据的不同访问方法，提供不同的一致性和可用性保证，使整个系统对外表现更好 对于数据的更改，需要做到一致性，而数据的某些读取场景，可能不需要那么实时，这种常见的场景就是分布式缓存和关系数据库的组合，对于写，直接作用于数据库，它是强一致性的，但是读，往往先经过缓存，这时可用性会得到保证 用户分区 按照用户的地理位置或者相关的业务逻辑，将用户数据分散在不同的分区中，使用户能够就近分区访问，提升访问效率 CDN服务就是一个用户分区的例子，用户访问Web服务，系统会根据用户的地理位置，将用户对于资源获取的请求转派到离用户更近的区域网络，从而使用户获得更好的访问体验 层次分区 多个分布式系统会组成一个整体，但是不同的系统提供的功能和层次都是不一样的，根据系统的层次来进行分区 不同的分布式系统会组成一个统一的整体，就像一颗树，根节点变化少，而叶子节点离用户近，不同的层次的节点对一致性和可用性有不同的倾向性 一致性和可用性的矛盾        一致性和可用性存在矛盾，在分布式环境（也就是存在分区）的前提下，分区已经成为既定事实，因为分区间的通信是不能保证绝对意义的可靠，因此：        （1）如果看重一致性，那么就要求多个分区之间的数据在更新时强一致，解法就是在更新时锁定多个分区，这就会导致可用性降低；        （2）如果保证可用性，就必须做到多个分区之间的更新不相互耦合，唯一可以想到的是异步进行更新，那么结果就是一致性不被满足。        面对这种矛盾，如果设计一个分布式数据存储系统，通过混合数据源来完成对业务需求的支持，比如：MySQL+MongoDB，利用MySQL的事务性来支持安全的写，使用MongoDB来满足复杂场景的读，数据会从MySQL向MongoDB同步。        混合数据源的方案已经使得该系统至少包含两个节点，因此分区已经成为现实，或者说在分布式系统中，往往分区容忍性是必须具备的，那么这个系统就要在一致性或者可用性中做选择了：        （1）如果选择一致性，这就要求所有的变更在多个节点（MySQL和MongoDB）中是一致的，这时会使用分布式事务来保障不同节点的数据一致性，但是节点之间的网络可能存在问题，导致整体的访问出现错误，导致可用性无法完全兑现；        （2）如果选择可用性，可以看出当MySQL发生数据变更后，会从网络同步变更到MongoDB，一般会选择消息系统，如果网络出现问题，将会在下次恢复时完成同步。虽然数据在一个时刻（或一瞬），存在短暂的不一致，无法兑现一致性，但是用户的访问的可用性能够得到保证。当然如果选择可用性，也就是AP模式，对于数据同步，可以考虑提升硬件的支持，比如：多条网线连接等方式。        在CAP定理的限定下，许多开发者仍在通过各自的努力，在不同的场景下减少CAP定理带来的麻烦，而基本思路都是在P成立的基础上，看是做到C或A，或者说侧重C或A：        （1）尽力而为的可用性，即CP类型。这种设计往往在是在一个数据中心中提供稳定的服务，如果要跨数据中心，将会导致其可用性严重下降。CP类型并不是表示对可用性不看重，以谷歌的分布式锁服务Chubby为例，Chubby集群包括若干节点，只要超过半数能够正常工作，那么Chubby就能提供稳定的服务。可以看到，只要一个数据中心中过半数的Chubby节点承认了本次请求，就认为请求被许可，而可用性也得到了最大限度的提升；        （2）尽力而为的一致性，即AP类型。在某些场景下，牺牲可用性是不可取的，用户需要立刻看到响应，比如：用户在请求一个网页，需要快速的获得响应页面，纵使它的数据可能不那么实时。AP类型常见的使用场景是缓存服务，在互联网这种读远远大于写的场景里，缓存服务被大量使用，缓存服务会缓存用户数据。用户实时数据往往存储在关系数据库中，缓存服务能够提供给前端更快速响应的同时，还大大提升了可用性，虽然数据的一致性会有挑战，但可以通过过期时间来做到用户数据的最终一致性。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/explanation-of-compensation-mechanism.html":{"url":"book/explanation-of-compensation-mechanism.html","title":"热风-补偿机制说明","keywords":"","body":"补偿机制说明        业务代码在分布式环境（单机也存在）下执行时，由于部分成功，部分失败，会导致不一致的状态，比如：业务执行过程中，存储不可用，或者连续两个服务调用，第一个成功，第二个失败（可能是超时，或者本身的系统问题）。        业务补偿本质上就是处理分布式环境数据一致性的问题，在分布式数据一致性处理常规手段上，按照强弱大致分为两种：        强一致性：XA/2PC，分布式环境下：TXC/Aliyun GTS        弱一致性（最终一致）：失败记录，消息中间件，系统重试        强一致性的场景偏向用户直接操作时，需要立刻反馈结果，且重要性很高。有相应的产品支持，基本都是两阶段的方式保证事务的完整性。其中Aliyun的GTS，支持分布式服务的事务，依靠在分布式服务请求中埋点事务ID，依靠旁路系统来推进该事务ID前进或者回滚，GTS对主流服务框架做了适配，对部署环境有一定要求。        弱一致性是一种比较经济的手段来保证数据的一致性，思路其实和GTS很类似，都是需要依托一个旁路数据，对多个阶段的业务操作保证执行的正确，不同点在于对于事务本身，弱一致性基本都是将事务向前推，很少能做到回滚，而对补偿机制的理解，我认为就是弱一致性的解决方案。 消息中间件        使用消息来进行补偿，利用了消息中间件的外部存储以及消息重新投递的特性来推动事务，当执行失败时，对业务场景进行补偿。        比如有以下业务场景：创建订单，其中涉及到三个调用：生成商品订单，生成支付订单和生成物流订单，如下： CreateOrder: CreateBizOrder; CreatePayOrder; CreateLogisticOrder;        如果其中一个失败，就需要进行重试补偿。        使用消息进行补偿，可以定义消息的数据结构为： 发送方        可以通过手工编码的方式来进行消息的发送。 CreateOrder: try { CreateBizOrder; try { CreatePayOrder; try { CreateLogisticOrder; } catch (Exception ex) { //… } } catch (Exception ex) { CompensationMessage msg = new CompensationMessage(); msg.setBizId(id); msg.setScene(“CreateOrder”); // 设置场景 msg.setPhase(“CreatePayOrder”); // 设置阶段 msg.setContext(Param); // 设置参数 MessageProducer.sendMessage(msg); } } catch (Exception ex) { CompensationMessage msg = new CompensationMessage(); msg.setBizId(id); msg.setScene(“CreateOrder”); // 设置场景 msg.setPhase(“CreateBizOrder”); // 设置阶段 msg.setContext(Param); // 设置参数 MessageProducer.sendMessage(msg); }        以上逻辑，实际就是在执行出错的时候，将场景、阶段、参数等信息以消息的形式发送出去，利用消息中间件高可用的特性，将状态保存在消息中间件上。 消费方（需要注意幂等处理）        可以是本机发消息，本机消费消息。只需要监听消息，然后按照消息格式进行处理即可。消费方逻辑（基于RocketMQ，其他消息中间件也大抵如此）： //监听消息 consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { String key=null; String msgId=null; for (MessageExt messageExt : list) { key = messageExt.getKeys(); //判读redis中有没有当前消息key if (redis.exist(key)) { // 无需继续重试 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } // 可以将key加入到redis中，防止重投后被消费 redis.add(key); msgId = messageExt.getMsgId(); try { CompensationMessage msg = Convert.convert(messageExt.getBody()); // 处理消息，根据阶段，完成补偿操作 process(msg); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } catch (Exception e){ return ConsumeConcurrentlyStatus.RECONSUME_LATER; } } }        消费方的处理策略是，接受到消息，如果消息的key能够在Redis中查询到，则消费成功，这里是保证消息的重投能够被正确的处理，只做一次。        然后将消息转换为CompensationMessage，根据消息中的场景和阶段，进行事务的推进。        基于消息中间件可以手动的编码来保证出现异常时，能够自动的将业务进行补偿，将事务推动下去，保证最终一致性。但是这种编码会比较复杂。可以尝试利用一些框架进行简化，比如：spring-aop，可以自定义注解来简化发送方的代码。        定义切面和注解，比如：@Compensation，当方法抛出异常时，会完成相关操作，比如：发送消息。 @Compensation : value() : 场景        切面处理逻辑，会根据@Compensation，将场景提取，然后将方法作为阶段，参数作为上下文，以消息的形式进行投递。 CreateOrder: @Compensation(“CreateOrder”) CreateBizOrder; @Compensation(“CreateOrder”) CreatePayOrder; @Compensation(“CreateOrder”) CreateLogisticOrder;        这样，当调用这些方法，出现问题，就会发送对应的消息，使用者不用关注是否使用了消息进行事务的补偿。        对于补偿逻辑，可以定义补偿处理的接口，比如： interface CompensationProcessor { void process(long bizId, Map context); }        如果抛出异常，将会被重试。        在配合上注解或者接口方法，来定位这个对应的实现，当前可以用注解： @CompensationProcessorConfig : String scene(); String phase();        那么对于消费方，进行失败处理的逻辑，就可以定义为：        处理创建BizOrder失败的处理器： @CompensationProcessorConfig(scene = “CreateOrder”, phase = “CreateBizOrder”) @Component public class CreateBizOrderFailOverProcess implements CompensationProcessor { void process(long bizId, Map context) { // Logic } }        处理创建PayOrder失败的处理器： @CompensationProcessorConfig(scene = “CreateOrder”, phase = “CreatePayOrder”) @Component public class CreatePayOrderFailOverProcess implements CompensationProcessor { void process(long bizId, Map context) { // Logic } }        处理创建LogisticsOrder的处理器： @CompensationProcessorConfig(scene = “CreateOrder”, phase = “CreateLogisticOrder”) @Component public class CreateLogisticsOrderFailOverProcess implements CompensationProcessor { void process(long bizId, Map context) { // Logic } }        根据这三个场景，三个处理器。对于失败处理和补偿的实现者，不需要知道是否进行消息处理，只用实现逻辑即可。而框架只需要修改消息处理的逻辑，通过消息中的内容来找到对应的CompensationProcessor即可。 try { CompensationMessage msg = Convert.convert(messageExt.getBody()); List list = applicationContext.getBeans(CompensationProcessor.class); String scene = msg.getScene(); String phase = msg.getPhase(); for (CompensationProcessor cp : list) { CompensationProcessorConfig annotation = cp.getAnnotation(CompensationProcessorConfig.class); annotation.getScene(); annotation.getPhase(); } // 找到对应的CompensationProcessor CompensationProcessor.process(msg.getId(), msg.getContxt()); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } catch (Exception e) { return ConsumeConcurrentlyStatus.RECONSUME_LATER; } 系统重试        使用消息进行补偿的本质是利用外存（消息中间件）来转储状态，成本比较低，但是对于重试场景，还有内存级别的解决方式。当处理失败时，通过内存队列进行重试以及恢复。业界有比较成熟的方案，在分布式微服务环境下，spring提供了spring-retry来应对这个场景，增强分布式环境的一致性。 依赖        在SpringBoot应用中，通过@EnableRetry，声明开启重试。 org.springframework.retry spring-retry 1.2.4.RELEASE org.aspectj aspectjweaver 1.9.4 使用 @Service public class RetryService { private Logger logger = LoggerFactory.getLogger(RetryService.class); @Retryable(value = Exception.class, maxAttempts = 3, backoff = @Backoff(delay = 2000L, multiplier = 2)) public void divide(double a, double b){ logger.info(\"开始进行除法运算\"); if (b == 0) { throw new RuntimeException(); } logger.info(\"{} / {} = {}\", a, b, a / b); } @Recover public void recover() { logger.error(\"被除数不能为0\"); } }        @Retryable注解:value: 抛出指定异常才会重试include：和value一样，默认为空，当exclude也为空时，默认所以异常exclude：指定不处理的异常maxAttempts：最大重试次数，默认3次backoff：重试等待策略，默认使用@Backoff，@Backoff的value默认为1000L；multiplier（指定延迟倍数）。        @Recover注解：当重试达到指定次数时候该注解的方法将被回调发生的异常类型需要和@Recover注解的参数一致@Retryable注解的方法不能有返回值，不然@Recover注解的方法无效。        通过声明注解到对应的方法上，如果有异常，将会尝试重试，并且根据配置进行延迟重试处理，和spring的生态整合很好，也可以基于它进行扩展。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/byzantine-generals-problem.html":{"url":"book/byzantine-generals-problem.html","title":"热风-拜占庭将军问题","keywords":"","body":"拜占庭将军问题        《拜占庭将军问题》来自于Leslie Lamport一篇在1982年发表的论文，可以通过检索the-byzantine-generals-problem来下载阅读。这篇论文的主旨是阐述了一个将军和若干中尉协作进攻一个城池的问题，通过假设和分析问题场景中的多种情况，比如：存在叛徒和信息传递丢失的问题，来设计一组算法，使得忠诚的将军（或中尉）有统一的行动计划，并将这组算法应用到可靠计算机系统的建设中。        论文整体上有些散（或者说目标多），并且在定理阐述阶段也不断的修改，比如：在假设阶段，不断的修改拜占庭将军问题的题设，从多将军协作，到将军和中尉，再到作战时间的协商，有兴趣的读者可以自己下载原文阅读，由于作者在一篇论文中尝试说明多个场景的解法，导致笔者的阅读体感并不好，而实际作为一篇论文，20页的体积还是相当大的。该论文阐述的问题和思考是有价值的，尤其在分布式环境下构建一个可靠的计算机系统有指导意义。        以下是笔者对这篇论文的（摘录）翻译和理解。如果是笔者的理解将会以来开头，以引言的形式来展示，比如： 这个是作者的理解。。。        描述内容示例。。。        分布式系统是由多个部分组成，当有部分组件或者系统出现问题，向其他组件或者系统发出具有冲突性的信息时，可靠的计算机系统必须能够正确应对这种情况。这种情况可以用这个场景来形容，一组拜占庭将军率领各自的部队包围了敌人的城池，将军们只能通过信使交流，必须就共同的作战计划达成一致。然而不幸的是，他们中的一个或多个可能是叛徒，他们会试图迷惑其他人。问题是找到一种算法来确保忠诚的将军们会达成一致。        结果表明：        （1）仅使用口头信息时，当且仅当超过三分之二的将军忠诚时，这个问题是可以解决的，所以一个叛徒可以迷惑两个忠诚的将军；        （2）当信息无法伪造时，任何数量的将军和可能的叛徒都可以解决这个问题。 口头信息 口头信息可以被伪造，叛徒收到一位将军的信息，然后可能会进行篡改，将相反的信息传递给其他人。        基于这个结论，然后讨论该结论在可靠计算机系统中的应用。 引言        一个可靠的计算机系统必须能够在系统中若干组件出错的情况下正常工作。一个出错的组件，在系统中时常表现的行为是向其他组件传递有冲突的信息。这个应对出错组件的问题被抽象为拜占庭将军问题。本文的主要内容是探讨这个问题以及如何解决这个问题，并将解法应用到如何实现一个可靠的计算机系统。        我们设想若干拜占庭军团包围了敌人的城池，每个军团都有一个将军指挥。将军之间通过信使来通信。通过对敌人的观察，将军们必须形成统一的决策，然后行动，但是将军们中间可能存在叛徒，叛徒会阻止忠诚的将军们形成决策。将军们需要一个算法来保证：        条件A. 所有忠诚的将军们有相同的行动计划。        忠诚的将军会按照算法计算所得的结果来行动，而叛徒会按照他们自己的意愿来行动。这个算法需要确保条件A能够在叛徒无论做什么行动的前提下都能够成立。        忠诚的将军们应该不仅限于达成一致决策，而且应该做到是一个合理的决策，需要确保：        条件B. 少量的叛徒不能导致忠诚的将军们采纳一个不好的决策。        条件B很难形式化表达，因为无法精确的说什么是不好的决策，而且我们也不会尝试这么做。相反，我们考虑将军们如何达成一致的决策。每个将军都会观察敌人并将他的观察结果（或者说决策）传达给其他将军。令v(i)是第i个将军传达的信息。每个将军使用某种方法将值v(1).....v(n)组合成一个唯一的行动计划，其中n是将军的数量。条件A可以通过让所有将军使用相同的方法组合信息来实现，条件B需要使用更加健壮的方法来加以实现。例如，如果唯一要做的决定是进攻还是撤退，那么v(i)就是将军i对这两个选项的最好意见，最终决定可以基于它们之间的多数票。只有当忠诚的将军在两者之间几乎平均分配时，少数叛徒才有影响决策可能性，在这种情况下，没有所谓不好的决策。 作者到当前依旧没有提出拜占庭将军问题的标准题设，当前的条件A和B对于输入没有描述，只是描述为 “通过对敌人的观察” ，毕竟对于计算机系统，需要执行的指令是需要外部触发或者给出的，因此题设经历了两轮调整来到了将军和中尉模型，这也就是大家熟知的拜占庭将军问题。        拜占庭将军问题：一个负责指挥的将军必须发送命令给他的n-1个中尉，要求：        IC1. 所有忠诚的中尉服从一致的命令。        IC2. 如果指挥官是忠诚的，那么每个忠诚的中尉都会服从他的命令。        IC1和IC2是交互一致性条件，如果IC2成立，则IC1自然成立。 不可能的结果        拜占庭将军问题看起来是有一些欺骗性的简单，其困难在于如果将军和中尉之间仅通过口头消息传递信息，那么如果将军和中尉的数量中没有超过2/3的人是忠诚的话，这个问题是无解的。如果只有一个将军和两个中尉，也就是三个参与者，只要其中出现一个叛徒，无论是将军还是中尉，都无法使忠诚的参与者们达成共识。口头消息的内容完全由发送者控制，所以一个叛徒能够传递任意可能的信息。        我们现在说明：通过口头信息，三个参与者中有一个叛徒是无解的。为简单起见，我们考虑唯一可能的决定是“进攻”或“撤退”的情况。如下图场景，其中指挥官忠诚并发送“进攻”命令，但中尉2是叛徒并向中尉1报告他收到了“撤退”命令。为了满足IC2，中尉1必须服从命令进行攻击，但中尉1面对一个“进攻”命令和一个“撤退”命令，无法做出决策。        其中红色框的头像是忠诚的参与者，而蓝色的是叛徒。        现在考虑另一个场景，如下图所示，其中指挥官是叛徒，向中尉1发送“进攻”命令，向中尉2发送“撤退”命令。        中尉1不知道叛徒是谁，他不能告诉指挥官实际上给中尉2发送了什么信息。因此，这两个场景在中尉1看来是完全相同的。如果叛徒一直在说谎，那么中尉1就无法区分这两种情况，在这种两难的境地，中尉1无法得出能够满足IC1和IC2的结论。 需要证明3m个参与者，其中m个叛徒，是无法使2m个忠诚参与者采用一致的结论或遵循忠诚将军的命令，这也就是IC1和IC2的要求。证明过程使用了反证法，将一个拜占庭将军负责的部队拆分为一组阿尔巴尼亚军团，利用递归的思路加以证明。但是笔者认为通过例证的方式会更加明确，因为对于忠诚的参与者面对的信息一定是相反且均数的。 下面模拟4个参与者，1个叛徒，也就是超过3m个参与者，首先看一下忠诚的将军和一个叛徒中尉。 忠诚的将军发起“进攻”命令，中尉1收到的信息是三个：指挥官的“进攻”、中尉2说收到指挥官的命令是“进攻”和叛徒中尉3说收到指挥官的命令是“撤退”，这样中尉1可以做出决策：进攻（2票进攻，1票撤退）。这个结论就同时满足了IC1和IC2。 如果将军是叛徒，那么他会发送给不同的中尉*以不同的命令，如下图所示： 可以看到叛徒指挥官发送了命令进攻和撤退数量是不等的，同时忠诚的中尉之间会正确的传递信息，这样忠诚的中尉会采取进攻（2票进攻，1票撤退），满足了IC1，同时对于IC2，由于指挥官是叛徒，所以也满足IC2。 接下来作者会将命题再次更改（第三次），提出了对于攻击时间的协商，实际和“进攻”或“撤退”的二元选择没有区别，因为对于最终协商的结果一定是准确和无歧义的。这里对于更改后的命题以及证明不再描述，并且更改后的命题在后文中并没有出现引用。 一种基于口头消息的解法        在前文中已经说明，对于使用口头消息来解决拜占庭将军问题以应对m个叛徒，必须至少有3m+1个参与者。我们现在给出一个适用于3m+1或更多参与者的解决方案。每个参与者都能够向其他参与者发送口头消息。口头消息的定义有以下假设（或约束）：        A1. 发送的每条消息都能被正确传递；        A2. 消息的接收者知道是谁发送的；        A3. 可以检测到缺失哪个发送者的消息。        假设A1和A2可以防止叛徒干扰其他两个参与者之间的通信，因为通过A1，叛徒不能干扰传递的消息，而通过A2，叛徒不能通过引入虚假信息来混淆他们的交流。假设A3将挫败一个试图通过不发送消息来阻止决策的叛徒。这些假设的实际实现在第6节中讨论。        本节和下一节中的算法要求每个参与者都能够直接向其他参与者发送消息。在第5节中，我们描述了没有此要求的算法。        如果指挥官是叛徒，指挥官可以决定不下达任何命令，以此来使中尉们无法达成共识。由于中尉必须服从某些命令，需要获得输入，在这种情况下他们需要某些默认命令来服从。因此让“撤退”成为这个默认命令。        定义口头消息算法 OM(m)，对于所有非负整数m，指挥官通过它向n-1名中尉发送命令。我们接下来证明OM(m)在最多m个叛徒存在的情况下解决了3m+1或更多将军的拜占庭将军问题。我们发现用中尉“获得一个值”而不是“服从命令”来描述这个算法更方便。 分布式环境下的共识，实际目的就是对于一个问题（变量）有共识（值）。        该算法假定函数majority具有以下特性：如果值v(i)的大多数等于v，则majority(v1, v2, … , vn-1)等于v。对majority(v1,v2,…,vn-1)：        1. v(i) 中的多数值，如果不存在则为“撤退”；        2. v(i) 的中位数，假设它们来自一个有序集合。 涉及到分布式环境中，一个节点的值，那么上述函数就是一种最朴素的算法，也就是取其他n-1个节点的值，然后多数值为自己的值。这点可以理解为，一个节点去获取值，如果集群中其他的节点都会返回这个值的内容，那么就取多数。        根据上述的majority算法约束，m为非负整数，对于OM算法描述如下： if (m == 0) { (1) 指挥官将他的值发送给每个中尉； (2) 每个中尉使用他从指挥官那里得到的值，如果没有收到值，则默认“撤退”。 } else { (1) 指挥官将他的值发送给每个中尉； (2) 对于每个中尉i，令vi是中尉i从指挥官那里得到的值，如果没有收到任何值，则默认“撤退”。中尉i接下来作为指挥官，运行OM(m-1)算法，将值vi发送给n-2个其他中尉； (3) 对于每个中尉i，以及每个j不等于i（也就是其他中尉），令v(j)是在步骤(2)中从中尉j那里得到的值，如果没有收到值，则默认“撤退”。中尉i使用值为majority(v1, v2, … , vn-1)。 } 针对m=1, n=4的场景，在上一节中笔者已经做了描述，这里不再赘述。        为了证明算法OM(m)对任意m的正确性，我们首先证明以下引理。        引理1. 对于任何m和k，如果有超过2k+m名将军（或参与者）和至多k名叛徒，则算法OM(m)满足IC2。        证明：使用数学归纳法通过对m的归纳来证明。很容易看出算法OM(0)在指挥官忠诚的情况下是成立的，因此当m=0时引理成立。IC1要求所有忠诚的中尉有一致的值，而IC2要求忠诚的中尉会执行忠诚将军的命令，在m=0时，IC2会达成，且IC1会随之达成。        我们现在假设m-1并且m>0成立，然后证明m时引理成立。        在步骤(1)中，忠诚的指挥官将值v发送给所有n-1名中尉。        在步骤(2)中，每个忠诚的中尉对n-1个参与者应用OM(m-1)。由于假设n>2k+m，可知n-1 > 2k+(m-1)。由于最多有k个叛徒，并且 n-1 > 2k+(m-1) >= 2k，因此，对于n-1个值i中的大多数忠诚的中尉，每个忠诚的中尉都有v(i) = v，因为他在步骤（3）中获得了 majority(v1,v2,…,vn-1)=v，所以我们可以应用归纳假设得出结论：对于每个忠诚的中尉j，v(j)=v。由此，证明了引理1满足IC2。 对于引理的证明，实际可以通过分析获得。原因在于2k+m，可以拆解为 k + k + m，对于接收到值的中尉需要面对上述的结论集合，而k与非k（叛徒的结果，权且这么称呼）相互低效，关键票就到了m，而m为大于0的，使得2k+m为真。        在此基础上，提出以下定理，算法OM(m)解决了拜占庭将军问题。        定理1. 对于任意m，如果将军（参与者）超过3m，叛徒最多m，则算法OM(m)满足条件IC1和IC2。        证明：使用数学归纳法通过对m的归纳来证明。如果没有叛徒，那么很容易看出OM(0)满足IC1和IC2。我们假设该定理对于OM(m-1)成立并证明它对于OM(m), m > 0成立。        我们首先考虑指挥官忠诚的情况。通过在引理1中取k等于m，我们看到OM(m)满足IC2。如果指挥官是忠诚的，IC1被IC2所包含，所以我们只需要在指挥官是叛徒的情况下证明IC1。        最多有m个叛徒，指挥官是其中之一，所以最多m-1个中尉是叛徒。既然有超过3m的参与者，就有不少于3m-1的中尉，3m - 1 > 3(m - 1)。因此，我们可以应用归纳假设来得出OM(m-1)满足条件IC1和IC2的结论。 Lamport与其说是一位计算机科学家，还不如说是一名数学家。这点在他的自我描述中也能感觉到，对于定理的提出而言，不是直接描述定理，而是提出引理，利用引理来道出定理，使更具实践性的定理显得更加生动，这点可以看出作者的造诣之深。 一种基于签名消息的解法        正如我们从之前场景中看到的，正是叛徒撒谎的能力使拜占庭将军问题变得如此困难。如果可以限制这种能力，问题就会变得更容易解决。一种方法是允许将军发送不可伪造的签名消息。在消息假设中新增一个约束：        A4. (a) 忠诚将军的签名不能被伪造，伪造会被（忠诚的将军或中尉收到后）发现；               (b) 任何人都可以验证消息签名的真实性。 由于在实际场景中，不常见到基于签名消息的这种一致性算法，同时它强约束，使得运用起来较为困难。基于签名消息的解法不再赘述，想了解的同学可以参考原文。 丢失通信路径        同上一节，不赘述。 可靠计算机系统        除了使用本体上可靠的电路元件之外，我们所知道的实现可靠计算机系统的唯一方法是使用几个不同的“处理器”来计算相同的问题，然后将计算结果进行收集，对其输出进行多数投票以获得单个值。投票可以在系统内执行，也可以由输出给用户，在外部执行。无论是使用冗余电路来实现可靠的个人计算机还是弹道导弹防御系统，思路都是一样的，唯一的区别在于冗余的“处理器”的规模。        使用多数投票来实现可靠性是基于所有无故障处理器将产生相同输出的假设。只要它们都使用相同的输入，这就是正确的，因为会通过计算得到相同的输出。然而输入的数据来自某个组件的输出，而这个组件一旦有问题，它会将问题输出给到一个或者多个组件作为它们的输入。此外，如果不同的处理器在值变化时读取该值，即使从一个无故障的输入单元也可以获得不同的值。例如，如果两个处理器在时钟前进的一瞬读取时间值，那么一个可能获得旧时间，另一个可能获得新时间，而这只能通过将读取与时钟的前进同步来防止。        为了让多数投票产生一个可靠的系统，应该满足以下两个条件：        1. 所有无故障处理器必须使用相同的输入值（因此它们产生相同的输出）。        2. 如果输入单元无故障，则所有无故障进程都使用它提供的值作为输入（因此它们产生正确的输出）。        这些只是我们的交互一致性条件IC1和IC2，其中“指挥官”是产生输入的单位，“中尉”是处理器，“忠诚”意味着无故障。 拜占庭将军问题的命题映射到了可靠计算机系统的构建过程。        我们已经给出了几种解决方案（口头消息和签名消息的解法），但它们是根据拜占庭将军而不是计算系统来表述的。我们现在研究如何将这些解决方案应用于可靠的计算系统。当然用处理器实现拜占庭将军问题，就需要满足原有的假设A1-A3（算法签名消息SM(m)的假设是A1-A4）的消息传递系统。我们现在按顺序考虑这些假设。        A1. 由无故障处理器发送的每条消息都被正确传递。在实际系统中，通信线路虽然会出现故障，但这基本可以保障的；        A2. 表明处理器可以确定它收到的任何消息的发送者。这里对计算机系统中传递的消息是可以做到收到消息时，知晓对端是谁的；        A3. 可以检测到消息的缺失。消息的缺失只能通过它在某个固定的时间长度内未能到达来检测——换句话说，通过使用某种超时约定。使用超时来满足A3需要两个假设：        1. 消息的生成和传输需要固定的最长时间X；        2. 发送方和接收方的时钟在某个固定的最大误差Y范围内同步。        如果接收方在X+Y的时刻后还没有收到消息，那么就认为该消息是缺失的，纵使在时刻后收到了该消息，该消息也不会认为被收到。 结论        我们已经在各种假设下提出了拜占庭将军问题的几种解决方案，并展示了如何将它们用于实现可靠的计算机系统。这些解决方案在所需的时间量和消息数量方面都是昂贵的。算法OM(m)和SM(m)都需要长度为m+1的消息路径。换句话说，每个中尉可能必须等待来自指挥官的消息，然后通过m个其他中尉进行消息交换。        在面对任何可能的故障时实现可靠性是一个难题，其解决方案似乎天生就很昂贵。降低成本的唯一方法是对可能发生的故障类型进行假设。例如：通常假设计算机可能无法响应，但永远不会错误响应。但是当需要极高的可靠性时，不能做这样的假设，需要想求解拜占庭将军问题一样花费大量的精力。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/2pc-summary.html":{"url":"book/2pc-summary.html","title":"热风-共识协议：两阶段提交","keywords":"","body":"共识协议：两阶段提交 问题        日常我们都有银行转账的经历，一个用户A从银行X的账户上转了100元到用户B银行Y的账户。如果单独分析用户A和用户B在各自银行账户的金额变化，它们就是独立的本地事务，但是由于这个场景是跨多个数据源，所以它是一个分布式事务场景，而分布式事务代表的是全局事务。        如上图所示，用户A在银行X账户的扣款以及用户B在银行Y账户的存款是一个全局事务，它也符合事务的ACID原则，尤其是要兑现原子性（Atomicity），也就是要么这两个操作全部成功，要么全部失败，没有中间状态。如果把这两个操作视作两个节点，在转账场景中，它们就形成了一个更大范畴的分布式系统（虽然银行X和Y的系统并不相同），而要确保分布式事务的原子性，需要保证：        （1）安全性（Safety），所有节点共进退，要么成功，要么失败；        （2）存活性（Liveness），所有节点正常，则成功。可以允许有异常节点，但最终需要有一个一致结果，且不能一直等待。        参与分布式事务的多个节点需要对事务的提交或回滚达成一致，其实事务的提交与否和就一个值多节点达成一致，在本质上是同一回事，这就涉及到了分布式一致性问题。根据CAP原理，由于分区已经客观存在，保证安全性和存活性的必要条件就是保证一致性（Consistency），确保分区环境下的一致性的协议有许多，常见的就是两阶段提交（Two Phase Commit）。 协议        两阶段提交协议为了为了保持跨多个节点全局事务的ACID特性，通过引入一个协调者组件来统一掌控所有节点（或称作参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。该协议的算法思路可以概括为：协调者先问询所有参与者的处理意见，参与者将结果通知到协调者；再由协调者根据所有反馈结果决定各参与者接下来是提交还是中止。        两阶段提交涉及到的角色一般有三个：应用程序、协调者（或称作事务管理器）和参与者（或称作资源管理器）。由应用程序发起操作，协调者一般是独立部署的中间件，而参与者是关系型数据库，协调者和参与者之间通过XA协议进行沟通。两阶段分为：准备阶段和提交阶段，分别应对投票和执行两个场景，其中准备阶段如下图所示：        准备阶段是由应用程序发起，通过协调者将提交事务的请求发送给所有参与者，参与者收到请求后在本地记录日志并将处理结果返回给协调者，然后等待最终命令，此时不做提交，对于外部数据变更是不可见的。处理结果只有两种，同意或中止。该阶段完成后，整体事务进入提交阶段，如下图所示：        协调者根据准备阶段收集到各参与者返回的处理结果集合进行判定，如果全部为同意，则向所有参与者发起提交命令，如果存在中止，则向所有参与者发起中止命令，当所有参与者响应命令后，整体事务完成。        可以看到两阶段提交协议是一种非常朴素的分布式一致性协议，依靠协调者来协同各参与者，确保不同参与者的状态一致。在安全性上能够确保所有节点有一致的意愿，但是在存活性上是存在一些问题的，比如：协调者在事务执行中突然崩溃导致参与者出现悬停，接下来我们运用CAP原理和拜占庭将军问题分析一下二阶段提交的一些问题。 分析        CAP原理指出，在一个分区的（分布式）系统中，无法同时满足一致性和可用性。两阶段提交协议面对分区选择了强一致性，因此在可用性上就会存在挑战和问题。选择强一致性，势必会在多个分区（或系统）之间同时锁定更多的资源，由于网络通信的不确定性，从而导致可用性降低。实际上二阶段提交最大的缺点就在于在执行时，参与节点都处于阻塞状态，节点之间相互等待对方的响应消息，而一旦某个节点锁定了某些资源后，其他（或非本事务）节点访问这些资源也会陷入阻塞状态，当然这也是保障一致性（隐性包含了可见性）的代价。        如果我们将拜占庭将军问题的拓扑模型来演绎二阶段提交的执行场景，可以看出在该场景中，协调者扮演的是指挥官，而每个参与者就是中尉，协调者下达命令，参与者执行后反馈。由于各方通过消息（或者远程调用）进行沟通，而要形成共识，就需要对假设1-3能够满足，前两个假设比较容易解决，而两阶段提交面对的问题就在假设3的兑现上，也就是如何能发现对方（协调者或参与者）的消息缺失。        从协调者的角度去看，在准备阶段发起对各个参与者的请求后，就需要等待所有参与者的响应，如果某一个参与者未响应或者响应消息丢失，则假设3无法满足，无法达成共识，进而无法满足一致性。二阶段提交解决该问题的方式是增加协调者自身的超时机制，如果超时时间到达后，存在参与者没有响应，则通知所有参与者进行中止操作。        从参与者的角度去看，在提交阶段等待参与者的最终命令时，如果协调者此时出现故障导致未发送命令或者命令消息丢失，则也会导致一致性无法被满足。参与者也需要超时机制，在超时时间到达后，不能简单的做出提交或者中止的操作，而是需要同各个参与者进行协商，具体问题具体分析。比如：协调者可能没有发出命令，或者协调者可能发出了提交的命令，已经触达到了部分参与者后出现了问题。由于各个参与者不知道其他参与者在准备阶段的响应结果，所以就需要通信协商，而一旦涉及到参与者之间的通信，就会使这个问题变得更加复杂，最终很难有一个好的解决方案。        可以看出两阶段提交通过引入协调者并利用准备和提交两个阶段简单的解决了分布式一致性问题，但在实际情况中，它却存在不少问题。两阶段提交要求对各参与者的资源占用时间横跨两个阶段，对资源占用时间过长导致吞吐量低，并且由于协调者（或部分参与者）稳定性或消息丢失问题，使得一旦出现问题，很难保证事务的一致性，只能提升协调者的可用性来减少问题的出现。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/compensation-and-tcc.html":{"url":"book/compensation-and-tcc.html","title":"热风-一种基于补偿的事务处理机制（TCC）","keywords":"","body":"一种基于补偿的事务处理机制（TCC） 介绍        随着Web服务的成熟和广泛使用，涉及到Web服务的安全、事务和稳定性相关的技术和协议就变得愈发重要。本文重点关注Web服务的事务，尤其是长时间运行事务的概念以及对Web服务实现者的影响。我们将研究事务和补偿如何满足Web服务实现者的业务模型，以及架构需要得到哪种程度的支持。 本文是翻译和总结Guy Pardon的论文《Business Transactions, Compensation and the Try- Cancel/Confirm (TCC) Approach for Web Services》，该论文没有涉及数学公式，所以读起来比较简单。在翻译的过程中，会增加笔者自己的所得。 如果是笔者的理解将会以 来开头，以引言的形式来展示。 定义        在我们开始之前，让我们确保就一些重要的术语达成一致。        事务：就本文而言，事务是一组相关的操作（或交互），它们可能由于（部分失败导致的）回滚，而需要在执行后取消。这些操作可以位于不同的地理位置，例如：Internet上的不同Web服务。 一个Web服务可能会聚合多个Web服务来完成一个具有业务含义的操作，如果对于用户而言，需要保证要么成功，要么失败，那么这些不同实现者提供的Web服务需要在一个事务语义下得到保证。        长时间运行的事务（或业务事务）：就本文而言，长时间运行事务的总持续时间超过单个操作持续时间几个数量级。由于长时间运行事务的持续时间很长，因此导致该类型的事务进行撤销时，会在相对较长的时间内取消操作。 事务除了可见性的要求，还有完整性的诉求，也就是由多个Web服务聚合的一个事务需要这些Web服务都能够一并完成，所以该事务的持续（或消耗）时间会比单个Web长很多。        示例：图1展示通过Web服务来预定旅行的航班，预定的路线是从布鲁塞尔到多伦多，路线是由两个航班组成的，这两个航班是从布鲁塞尔到华盛顿和从华盛顿到多伦多。如果这两个航班的预定是由两个航空公司提供，由两个Web服务在不同的系统中来实现。        如果第二个操作调用失败，那么我会被只有一张从布鲁塞尔到华盛顿的机票卡住（这当然很有趣，但它违背了我去多伦多旅行计划的目的）。在这种情况下，取消去往华盛顿的票可能是一个有效的选择，但是，在我决定这样做之前，我可能一直在寻找替代方案，例如：两小时左右后的另一次从华盛顿到多伦多的转机。而这可能会导致非常长时间运行的整体事务。        请注意，这里的关键点是在我决定取消第一张票时已经进行了预订。也就是说，航空公司预订系统已经接受并验证了我的第一次预订，并且至少会保留我的座位一段时间。如果没有取消事件，那结果就是：要么我付款了，白订了一张票，要么航空公司会因不需要的预订而赔钱。无论哪种方式，都会有人赔钱。因此，取消事件是有很大的驱动力（或者说价值）。        纯粹从技术上讲，取消不同位置的不同操作并不是什么新鲜事：几十年来，它一直由称为事务管理器或事务服务的东西完成。CORBA的OTS（对象事务服务）就是这种技术的一个例子，J2EE的Java事务API和Java事务服务（JTA/JTS）是另一个例子。 这里提到的取消在技术上一般称为回滚，但是这种回滚往往在业务语义上是有描述的，比如：在分支逻辑中进行了描述，如果订购不到第二张票该如何处理。因此对于产品设计而言，考虑容错性和面向失败设计，对于产品的用户体验和技术实现都会获得更好的结果。 在技术实现过程中，面对分布式事务的问题，往往是开发比较关注的，会花费更多的时间加以实现，保障其可靠性。此时产品需要明白这种局限性和挑战，在产品设计中就能够针对事务中的断裂（权且这么叫）进行贴心的设计，这样会使产品更加的贴心，同时也会让开发人员避免毫无依靠的野战。        这些技术都是基于ACID事务的概念。ACID事务要求的工作方式如下：事务访问的所有资源都被当前事务锁定，而其他并行事务无法访问被锁定的资源，直到事务提交（保存更改）或回滚（取消更改）。图2对ACID事务的锁定时间进行了说明。        对于长时间运行的事务，这并不是我们所期望的，因为只有在所有资源都保持锁定的情况下才能取消。在Web服务这种场景下，长时间的锁定资源是不能满足要求的，原因有以下几点： Web服务在取消之前可能需要很长时间的延迟，尤其是在涉及人工决策的情况下。这将意味着较长的锁定时间，从而导致资源的不可用时长变长； Web服务可能涉及彼此不认识或不信任的各方，而长时间的锁定资源会导致它们容易受到DDoS攻击。        如果ACID和回滚对我们没有帮助，那有什么办法来解决这个问题吗？您可能已经猜到，一个可能的答案是补偿。        基于补偿的方法不是将长时间运行的事务实现为一个巨大的分布式ACID事务，而是将每个Web服务调用视为一个（耗时很少的）本地ACID事务，在它执行后立即提交。以丧失ACID事务在数据层自动回滚能力为代价，但是大大的减少了各方锁定资源的时间，这意味着取消（或者概念上的回滚）必须以不同的方式来完成：也就是通过执行一个单独的本地ACID事务，如果整体成功则事务生效，否则该事务将在逻辑层面上完成取消。 业务操作需要联立多方资源，而传统的分布式事务处理方式是对所有资源进行加锁，导致对资源占用时间的延长，随之带来的出错会使得一致性收到挑战。补偿的方法是让在一定程度上降低事务的一致性级别，让业务层承担数据一致性的挑战，将大块的锁定资源方式变为离散且短小的锁定方式，反而在可用性上得到提升，最终也兑现了（弱）一致性。        补偿：就本文而言，补偿被定义为一个单独的ACID事务（在Web服务提供者本地），它通过业务逻辑的取消操作来代替旧有的ACID操作。        基于补偿的方法将数据库锁定时间保持在最低限度，从而避免了前面提到的所有缺点。让我们看看它是如何工作的。        再次考虑航班预订的例子。预订飞往华盛顿的航班本身就是一项ACID事务，在各个航空公司系统中是本地的，并且会立即提交。当需要取消时，第二个ACID事务将在逻辑上撤消预定（也就是反向操作已经创建了的预定），由此产生锁定资源的时间要短得多。        这是如何工作的？这取决于应用程序，应用程序开发者负责规范他们需要做什么。例如：取消航班预订可以通过不同的方式完成，我们可以删除预订记录（不留下任何痕迹），或者我们可以明确地将其标记为已取消。无论哪种方式都能够完成概念上的回滚，而选择哪一种是这个Web服务实现所属的业务来决定。        一般来说，我们有两种类型的补偿：        完美补偿：反向逻辑清除了原始的所有操作痕迹，也就是我们完全删除预订的情况。        不完美的补偿：如果反向逻辑留下可检测的原始痕迹，就会发生这种情况。例如：如果我们将预订记录的状态明确更新为“已取消”状态。 如果是业务层进行取消操作，那么在Web服务中，连续调用两个航空公司的Web服务是不是也是一样的？理论上是一样的，但前提是调用任意一个Web服务都不能出错。 两个连续的Web服务调用，一旦调用第二个Web服务出现错误、超时或者异常逻辑时，将无法保证事务，一旦出错就没有一个协调者能够将剩余的补偿逻辑进行执行。因此从稳定性上看，在完成补偿逻辑的触发上，一定是需要一个旁路系统进行支持的。 以业务角度去看补偿        现在我们已经了解了补偿的概念，让我们看一下它是如何来支持业务。当我们面对服务提供者的业务模式时，可以确定两个主要的不同类型。        类型1：补偿只是另一种业务逻辑        在这种情况下，就服务提供者而言，补偿并没有什么特别之处：补偿任务也可以是常规业务交互。特别是，Web服务不需要提供额外的服务实现来进行补偿。        这种类型业务的一个例子是股票经纪：如果您购买股票，那么您可以通过之后再次出售来补偿它。当然，在过去几年进行投资的每个人都知道，这种补偿模式并不完美：买入和卖出的股票价格仍然相同的可能性非常低。这意味着要求补偿的客户要么赔钱，要么赚钱。        尽管如此，这种补偿还是很有趣的，因为服务提供者不必担心补偿逻辑。它是最简单的一种基于补偿的系统，其中服务提供者是无状态的。 这种类型一般可以在两个不相关的系统交互时可以看到，A系统创建一条数据，然后向B系统进行调用，创建一条B系统中的数据，且这个操作是用户发起的。当A系统操作成功，而调用B系统失败时，只需要将对B系统调用的逻辑放置在后续操作的检查中（即没有调用则调用之），那么可以依靠用户在第一次出错后，返回到页面，再次触发来解决这个问题，其实质就是依靠用户操作来完成补偿，系统不维护状态，只是必要的检查和逻辑处理。        类型2：补偿是同一业务的第二阶段        在这种情况下，补偿是同一业务事务的明确第二阶段，涉及自定义逻辑以及要保留业务事务状态的编码。机票预订示例属于此类（与我们刚刚看到的股票经纪人示例进行对比）。我们可以将这种方法称为TRY-CANCEL/CONFIRM(TCC) 方法。TRY代表正常的业务逻辑，例如：预订座位。CANCEL表示通过补偿机制对正常业务逻辑生成的数据进行的取消，例如：根据预定编号取消座位。另一方面，如果不需要取消，则CONFIRM会被执行。CONFIRM也是一个ACID事务，用于更新数据库中的状态，例如：根据预定编号将预定转为确认。        为什么要确认CONFIRM？要理解这一点，我们需要为此在业务生命周期中保留这种状态。首次进行预订时，其业务状态为PENDING。对于供应商（航空公司）而言，这意味着预订尚未最终确定，换句话说：仍可以根据客户要求取消预订。这种状态对于航空公司的报告和预订服务很重要：它不应该在最终时间来临时，（还）保留预订。        如果航空公司愿意，它也可以设定一个超时时间，之后预定会在超时时间到达时自动取消。如果在没有确认的情况下进行预订，则可能会发生这种情况。如果CONFIRM一旦被触发，Web服务就会知道预订是永久性的并且可以向客户收费。        TCC方法非常适用于已经分为两个阶段的业务模型，例如：以某种方式进行预订的业务模型，当然服务提供者需要公开额外的接口来处理CANCEL和CONFIRM事件。 对长时间运行事务和补偿的架构支持        现在我们可以讨论一些基于补偿系统的架构含义，在前文中的两种类型需要进行分别讨论。        类型1：补偿只是另一种业务逻辑        如果补偿可以被视为正常业务的一部分（如股票经纪人的例子），那么股票经纪人就没有什么可担心的了。任何股票购买都会通过后来的销售得到补偿，股票经纪人可能会从两者中赚取佣金。绝对不涉及商业风险，因此这是最灵活的补偿案例。        通过诸如业务流程执行语言标准 (BPEL) 之类的常规工作流技术，对这种补偿的架构支持是完全可行的。可以使用远程工作流引擎对整个任务执行进行建模和执行，而购买股票只是整个过程中的一个步骤。如果需要取消流程（或长时间运行的事务），则工作流引擎会发送业务请求以出售其先前获得的股票，这里不需要其他任何东西。此外，股票经纪人并不关心补偿需要多长时间，仅仅因为对他（或她）来说这只是另一项单独的商业交易：不需要经纪人特定的状态将补偿与原始状态联系起来。        类型2：补偿是同一业务的第二阶段        对于TCC方法，实际情况将会变得复杂。特别是TCC所服务的业务方几乎不可能允许远程工作流承担其补偿的全部责任，因为在TCC过程期间存在隐式资源预留（或者说占用），而TCC事务的状态管理属于服务提供者。 或者说属于服务提供者（实现者）所依赖的某些支撑型系统，比如提供TCC能力的中间件。        比如在航班预订的例子中，只要预订仍处于PENDING状态，就会将座位处于已预订的状态，在此期间，其他潜在客户无法预订。因此这个过程需要多长时间是非常重要的，并且业务方不太可能允许远程工作流引擎一直保持未决的状态。这意味着将有一个超时，在此之后预订服务会自动取消预订。一个合乎逻辑的结果是预订服务至少需要知道如何它取消，换句话说：取消逻辑与预订服务位于同一系统中。触发取消最有可能通过带有交易令牌(ID)的事件发生，因为预订已经生成，这同样适用于CONFIRM逻辑。        就像业务模型一样，最终的架构遵循一个两阶段的协议：TRY 是第一阶段，而第二阶段包括取消请求CANCEL或确认请求CONFIRM。这些要求可以推知，支持TCC的系统需要一个两阶段事务管理器，服务提供者可以使用这样的装置来自动化其TCC事务的状态管理。        工作流引擎也可以使用这样的装置来自动化向所有相关站点发送CANCEL或CONFIRM请求的过程。例如，在BPEL引擎中，事务管理装置可以检测所有远程调用，从而了解分布式任务中的所有参与者。当CANCEL事件发生时，事务管理装置会联系每个参与的Web服务并要求它取消其部分工作。如果我们牢记每个工作流步骤都可以有它的补偿，那么这将减少大约50% 的工作流建模复杂性：而不是将每个补偿显式建模为工作流逻辑中的一个步骤，事务管理装置会跟踪哪些站点需要补偿并在需要时处理取消。这样的话，所有补偿步骤都可以被排除在工作流逻辑之外，而工作流会因此变得更加简单。 结论        正如我在本文中试图展示的，基于补偿的服务模型基本上有两种主要类型：股票经纪人类型（无状态服务）和航空机票预订类型（有状态保留类型，TCC）。对于前者，工作流就足够了。对于后者，两阶段协议更适合。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/how-to-design-a-distribute-database-feeling.html":{"url":"book/how-to-design-a-distribute-database-feeling.html","title":"热风-《如何设计一个分布式数据库》观后感","keywords":"","body":"《如何设计一个分布式数据库》观后感        看了PingCAP的CTO（黄旭东）的分享如何设计一个分布式数据库，感觉要写点文字记录一下自己的感想。        TiDB在分布式数据库上是一个新贵，虐了不少号称水平扩展能力很好的友商产品，比如：查询10倍以上的性能虐了OceanBase，那么为什么会有这么大的差距？我想看完了黄旭东的分享后，他极客的做法和思考，也就对PingCAP能够打造出碾压平凡人的产品感觉很自然了。        黄旭东的分享没有细节到分布式数据库如何去做，但是谈了作者的取舍。分享没有涉及到分布式数据库运作细节，但是谈了作者的知识结构以及需要掌握哪些分布式知识。 在介绍分享的过程中，会增加笔者自己的所得。 如果是笔者的理解将会以 来开头，以引言的形式来展示。 为什么要NewSQL 数据量级增长的越来越快 对于过程和行为数据的广泛收集，目的是更加细致和准确的了解用户。以往在系统建模上，过程数据不会过多的保留，或者以某些实体的属性存在，但现在对于用户所有操作行为的数据都需要加以建模和记录，目的就是更加完整的建立用户画像。 一个很小的业务，如果对于用户的操作能够细致的建模和存储，其实数据量级也不会很小，比如：一个健康记录程序，以往可能记录每天的身体状况，但是如果把用户每个时段的身体状况都通过装置进行存储，那无疑会使数据量增长好几个级别，如果善用存储下来的数据，将会给用户带来更加细致的指导意见。 从TB级别到今天的PB和EB级别，数据的膨胀只会随着IoT的更加普及而变得愈来愈多。传统的关系数据库面对这种量级的数据，自身由于单体部署约束，导致无法应对海量数据的存储和使用。这个过程中，通过传统关系数据库的上层策略（比如：分库分表）可以一时的解决部分问题，但是数据对于业务的侵入（比如：分库键等）以及对限制（跨库查询等）总会使人用起来有些不快。这时NoSQL的出现缓解了这个问题，通过注重于水平扩展和无限存储（权且这么叫），再加上不错的性能，使得它虽然从开发人员手中拿走了一些东西（比如：额外的学习、新增的概念以及些许简陋的功能），但是它还给开发人员手中的好处确实让人兴奋，因为它解决问题。 而结合了NoSQL以及SQL的NewSQL的出现，是否能够给到开发人员以全新的体验，这不仅是在功能上，而且会在设计范式以及解决问题的模式上再一次带来改变。 传统的RDBMS在水平扩展上存在不足 这点就是传统的关系数据库的问题，毕竟诞生的年代是在单体时代，一个单体才能够很好的兑现ACID，而在分布式环境下，受限于CAP原理的约束，很难做到，在那个时代无异于自讨没趣，没有需求。因此在那个时代的产物，兑现要求，完善的工具链，更为重要。 要水平扩展，就是要CAP中的P，而P一旦满足，C和A总是那么不好拿捏。这些年业务的发展还是在使用传统关系数据库，毕竟使用量大、易维护和功能全，实在不够就在上层搭建一些路由层，以中间件的形式解决一下。但是这种方式面对扩容时都会有些不容易，毕竟相关的操作向前侵入了业务，向后牵扯了运维（需要理解一定业务规则），中间还夹着一个中间件团队，说白了，就是不透明。 OLTP与OLAP相互割裂 这点是比较有意思的，由于OLTP无法做一些大量的分析型工作，而分析的结果又要向用户透出，结果研发又不愿意承担这个工作，甩锅给数仓团队，就造成了OLTP拉出来的数据，通过ETL倒腾到OLAP，绕一圈再送回到OLTP的嘴里，展现给用户。 这么做的坏处那是太多了，研发团队负责一块业务，数仓又是横向对多个团队，中间就会出现GAP，这里的GAP不仅是技术上的，也有在业务认知上的，导致产出的数据总是差那么些火候，用在关键场景（比如：根据金额判断某个用户，这一时刻，能不能做某些事情），就是不能用，其实就是有偏差。这里的偏差来自于之前说的理解偏差，也有技术造成的偏差，如果用的是离线计算的那种落后技术，差的就不是一星半点了（毕竟数据一天可能变了N次吧）。 人员、认知和技术的割裂就这么一直搅着这个问题，有数据研发能力的业务技术团队可以解，有超人般的数据团队也可以解，但事实上都不会有。如果能够一份数据存储，又能提供良好的AP功能，这种HTAP数据库就会好一些了，能够减少一些割裂。为什么说好一些，而不是好多了，原因是在于不可能通过这样一种形式就能解决好实时分析数据获取这件事，能解决2-3成就已经功不可没了。 Google Spanner/ F1 / Amazon Aurora F1是SQL层，Spanner是NoSQL F1是一个逻辑处理层，负责将关系数据库的SQL进行解析，将SQL翻译为Spanner可以理解的一系列指令，然后指令下达给Spanner，由它来完成数据的操作。谷歌的F1和Spanner论文需要了解一下。 F1的论文 Spanner的论文（含翻译） Aurora替换了存储引擎 以MySQL为基础，替换了存储引擎，比如：将InnoDB替换为一个具备了分布式水平扩展能力的存储引擎。不愧是具备了电商基因的云厂商，这无疑是一种见效快的方式。 如果是一般云厂商，大概率都会使用这种方式，因为用户的编程界面不用改，但是它也会有很多限制，而像谷歌那种颠覆性的做法不常见，因此也能感觉其难能可贵。 TiDB总揽 TiDB的基本架构如上图所示 底层的分布式存储层解决的是KV存储问题，如果对一般的KV存储，其实也可以使用Redis，但是TiDB要求的是分布式，因此就需要一个能够在分布式环境下工作的KV存储。 TiKV也是单独的项目，TiKV。多个实例间通过Raft一致性算法，将写入数据能够完成多写，做到高可用。这里底层没有使用分布式文件系统，比如：Ceph，HDFS，原因是如果再使用分布式文件系统，那么数据就会写的更多。Raft的3份，文件系统的3份，写9份，因此从效率和经济角度出发，TiKV底层就没有使用分布式文件系统来构造。 TiDB通过gRPC来请求TiKV，目前来看gRPC是要做到终端到服务端，服务端到服务端以及服务端到终端的全通信工具。gRPC的代码（Java版本）在2017年左右看过，写的其实很一般，比较粗糙，没有分层，就更不要提层与层的抽象隔离了，但是架不住谷歌这么一直推动。推动是多方面的，一是谷歌背书和不断的更新，二是基于它来叠罗汉，就是涉及到通信的谷歌产品都会使用它，形成了合力，使得很多开源产品也首要支持它，这点值得很多技术公司学习。 整个架构看起来很清晰，职责分离明确，伸缩性应该非常不错。无状态SQL层负责计算，而分布式存储层负责存储，状态数据在PlacementDriver。 黄旭东的另一个作品，codis。一个golang实现的Redis集群代理，能够组建redis集群。看了一下，使用方不少，关注度不错。 存储总揽        TiKV的基本架构如上图所示。客户端通过gRPC访问TiKV集群，每个TiKV节点都是一个进程，运作在一台计算实例上，在TiKV内部将存储进行了Region分区，切割成为面上使用者的大小以及适合访问的形态，外部通过Raft协议将一次写入能够写到多个TiKV节点上，借由此来提升整体可用性。 存储引擎的数据结构基于LSM Tree，日志结构化合并树，这是一种对随机添加更为友好的数据结构。相比较B+树，它的随机访问能力更好，而B+树是更加适合磁盘的访问形式，在SSD这种场景下，效率并不高。关于LSM Tree可以参考文章。 每个TiKV实际是使用了单机的KV存储引擎—RocksDB，这个是脸书基于谷歌的LevelDB的改进版本，修复了不少问题，同时PingCAP也对RocksDB有捐赠源码（包括问题修复）。 任何分布式的装置都是建立在完善的单机装置上的，基于宏观上可靠的策略，将其形成为一个容易伸缩且高可靠的整体解决方案。TiKV的整体代码是使用Rust编写，这个新一代的系统编程语言是值得关注的。未来在底层高性能软件上可能会越来越多的看到Rust的身影，而系统软件和应用软件之间的中间件将会看到更多的golang。 TiDB中的SQL生命周期        外部请求发起方依旧是MySQL客户端，TiDB会伪装成为一个MySQL Server。当TiDB收到SQL时会进行语法树解析，生成执行计划，这点和MySQL Server工作的步骤有些类似。但是在最终的逻辑执行计划会生成出可选的物理计划，比如一个SUM会发给几个物理节点进行执行，最终会进行所有结果集的SUM。之所以要做这么一个SQL层，就是要将SQL和后端的分布式存储层能够联通。 对于语法解析这段内容，想象中可以照抄MySQL的，但是PingCAP并没有这么做，而是抛弃遗留代码，自己做了，目的是在中间能够加入一些自有的优化，同时为解析出来的结果同物理执行计划之间充分分隔开，很有胆略。 一个概念意义上的SQL输入，最终被翻译成为一组分布式计算的指令。 注重测试 测试驱动开发 TiDB整体项目的单测覆盖率是非常高的，行覆盖在80%左右，很不容易了。黄旭东对于单测的看法比较注重，强调单个构件的可靠性，这点是做高质量软件的必由之路。 对于任何问题，需要在单测或者测试场景上进行复现，然后通过不断的回归测试来确保正确，同时使用社区的非常多测试样例，保证其测试的覆盖面。 非常多的开发者对于单测总是忽略，这是一种非常不成熟的表现，很难保证你写出考究的代码。 故障注入 在软/硬级别进行故障的注入，验证该环境下TiDB的工作是否能够达到预期（或者正常）。包括对磁盘、网络、CPU和时钟等多种环境的调整以及故障预设，使TiDB工作在环境不稳定的场景中，发现问题，加以改善。 分布式测试 介绍了Jepsen和Namazu两款测试软件，搜了一下，基本都和PingCAP有关，看到的是其实习生写的分享，实习生后来还去了阿里云的数据库团队。。。 看样子PingCAP对内部分享也非常在意，有点学院的意思。 分布式测试之前没有接触过，更多的单元、功能、集成、性能。。。那些传统的测试。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/tcc-using-seata.html":{"url":"book/tcc-using-seata.html","title":"热风-使用Seata来实现TCC","keywords":"","body":"使用Seata来实现TCC TCC的交互过程        TCC是TRY-CANCEL/CONFIRM的缩写（以下简称为TCC），它是一种柔性事务的代表技术，相关描述可以在 https://weipeng2k.github.io/hot-wind/book/compensation-and-tcc.html 找到。TCC本质上仍是一种两阶段提交（以下简称为2PC）的变体，也就是在TRY阶段将资源的变更已经做完，达到万事俱备只欠东风的状态，而之后所有的事务参与者如果对此无异议，则事务发起者将会请求整体提交，也就是触发CONFIRM，反之会执行CANCEL。        TCC需要事务协调者的参与来完成CANCEL或CONFIRM的触发工作。TCC的全局事务由事务参与者发起，所涉及的事务参与者都会创建本地分支事务，这点同2PC类似，而本地事务的提交和回滚操作就分别对应于CONFIRM和CANCEL。TRY-CONFIRM的过程如下图所示：        可以看到全局事务由应用程序发起，一般应用程序也是一个事务参与者，它承担全局事务管理角色，负责全局事务的提交或者回滚。由应用程序在事务逻辑中请求不同的事务参与者，收到请求的事务参与者会将本地事务作为一个分支事务和全局事务形成关联，同时也会将上述信息注册至事务协调者。如果应用程序事务逻辑执行完成，各事务参与者均响应正常，代表全局事务可以提交，应用程序则会通知事务协调者提交全局事务，事务协调者随后触发各个参与者的确认（CONFIRM）逻辑。        如果事务逻辑执行出错，则会执行TRY-CANCEL过程，该过程如下图所示：        该过程和TRY-CONFIRM过程相似，在事务逻辑中调用事务参与者出现错误，则应用程序会通知事务协调者对当前全局事务进行回滚，事务协调者随后触发各个参与者的取消（CANCEL）逻辑。 TCC的主要优势        TCC的主要优势在于有较高的吞吐量。以电商业务的商品订购场景为例，买家订购商品生成订单，同时会进行商品库存扣减，这个过程需要保证如果库存满足订购的数量，订单有效，反之则订单无效，也就是说订购过程是一个事务。简单起见，整个过程涉及三个事务参与者，分别是：交易前台、订单和商品库存三个微服务系统，交易前台会调用订单和商品库存两个微服务完成订购。        如果使用2PC来确保该分布式事务，假设：订购过程中，交易前台微服务调用订单微服务生成了订单，可随后调用商品库存微服务出现错误（库存不足或调用超时），该全局事务需要进行回滚。从资源占用的角度出发，上述过程如下图所示：        可以看到参与到该分布式事务的三个微服务，会将参与事务的资源（比如：订单数据和商品库存数据等）进行锁定，且锁定时间会横跨两个阶段。商品库存微服务反馈中止，全局事务需要中止，订单微服务依旧要等待协调者的通知才能继续，这使得订单资源被长时间锁定。在2PC模式下，整个系统的吞吐量存在短板，如果事务参与者（或某个参与者）存在比较耗时的操作，将会拖慢整个系统的响应时间。        如果换用TCC来处理这个场景，TCC事务参与者会在接受到请求后即刻提交本地事务，事务参与者之间不会由于对方处理耗时过长而相互影响，该过程对资源的占用如下图所示：        从TCC的交互过程可以看到各个事务参与者所负责的本地事务在接收到调用请求后就会开始处理，一旦完成就会提交。订单微服务在接受到交易前台微服务的调用后就会进行订单创建，不会等待商品库存微服务的处理结果。当事务协调者发送取消事件给订单微服务时，订单微服务会根据通知中的事务上下文（比如：订单ID）来取消对应的订单，需要注意，取消订单的操作也是一个本地事务的提交。        TCC对资源的锁定占用时间会比2PC短很多，呈现出一种对资源离散且短时占据的形态，而非2PC在整个事务周期内都会整块长时间的锁定资源。由于资源锁定时间变短，单位时间处理本地事务数量自然增多，使得TCC模式下，整个系统的吞吐量会有显著的提升。 在微服务架构下，可以通过适当提升TCC链路上较为耗时的微服务实例数量，使的整个系统的吞吐量进一步提升。 TCC的使用代价        TCC对资源锁定时间的减少无疑会提升系统的吞吐量，有更好的性能表现，但任何好处都会有交换的代价，而这些代价主要体现在以下两个方面。 产品交互方式的改变        在之前的商品订购场景中，2PC和TCC模式的不同之处除了在资源锁定上，在数据的可见性上也有非常大的不同。2PC在处理该场景时，当订单由于库存不足生成失败，用户（或买家）在后台是无法看到订购失败的订单，并且在数据库层面也不会出现订购失败的订单，原因是2PC追求强一致性，数据被回滚了。用户就是感觉订购失败，可能是网络或者系统不稳定，那接下来再试一下就好。        TCC在处理该场景时，订单和商品库存之间没有强依赖，虽然在一个全局事务中，但是订单数据会生成，虽然可以通过状态位等技术手段使用户无法查看到该失败订单，可是它确实在数据库中生成了，只是在等待随后对生成的数据做取消或确认操作，这个过程是一种最终一致性的体现。当然可以在随后的CANCEL事件处理中将该订单删除，但是这些特殊的处理逻辑已经侵入到了系统实现中，并不是一个好的选择。        适当改变产品的交互方式以适应TCC模式是一个更好的选择。由于TCC是两段异步的处理模式，产品需要一定程度上的面向失败设计，将订购失败认为是一种正常的情况，用户不仅可以看到失败的订单而且还能看到失败的原因。这样刚生成的订单，可以展示给用户系统在处理中的提示信息，一旦CONFIRM或者CANCEL通知完成处理后，就可以反馈给用户最终的处理结果。        产品交互方式的适当改变，增加些许面向失败和容错的设计，会使得TCC模式使用的更加自然，同时也能够获得更好的用户体验，最终为业务产品和技术实现做到了对齐。 技术实现方式的改变        2PC本质是在数据层面做分布式事务，它不需要应用代码做改造，而TCC实质是应用层面的2PC，它需要应用代码做改造来满足TCC所需要的语义。        微服务接口定义需要做出改变以适应TCC，以订单微服务的订单生成接口为例，在2PC和TCC模式下的不同如下图所示：        可以看到在2PC（上图左半部分）模式下，应用对于接口的定义不会受到约束，这点是2PC的优势，事务协调者同数据源进行协作实现分布式事务，一定程度上对应用透明。而TCC（上图右半部分）模式下，应用成为分布式事务中的主角，它需要同事务协调者进行交互，所以在接口定义上需要定义出数据的创建、取消和确认三个不同的方法来分别应对TCC中的TRY、CANCEL和CONFIRM逻辑。        对于2PC而言，如果准备阶段有事务参与者反馈了中止，则在后续的提交阶段，数据源会将数据进行回滚。TCC没有依靠数据源来完成这个工作，而是需要用户编写取消的逻辑来处理之前TRY阶段生成的数据，因此TCC的取消对于数据源而言，又是一次新的提交。        在上图中的TCC模式下，对于订单生成服务OrderCreateService定义了三个方法createOrder、cancelOrder和confirmOrder分别应对订单生成过程中的TRY、CANCEL和CONFIRM逻辑。TCC除了对应用接口定义产生了侵入，对于这些方法的实现也有隐性的要求，也就是方法实现需要做到幂等。以cancelOrder为例，在取消订单时需要先获取订单，根据订单的数据做出判断（比如：订单是新生成、没有被取消且没有被确认），符合要求的情况下才能够进行取消处理，这么做的原因在于事务协调者对于应用的通知可能会由于网络（或其他）原因出现延迟或重复通知，所以需要由应用自身的代码逻辑保证幂等。 Seata支持TCC        TCC依赖事务协调者来完成对全局事务（和分支事务）的状态维护与驱动。事务协调者接受事务参与者（也就是微服务应用）本地分支事务的注册，同时在全局事务提交或回滚时调用各个事务参与者相应的确认或取消接口。        TCC事务协调者的开源实现目前在业界有多个，其中使用广泛、功能完备且稳定可靠的参考实现当属Seata。 本文使用的Seata版本是2021年4月发布的1.4.2版本，由于讲述内容主要涉及其TCC功能的使用，如果需要更详细了解Seata，可以访问seata.io，参考其官方文档。 什么是Seata        Seata是一款开源的分布式解决方案，支持诸如：AT（类似2PC）、TCC、SAGA和XA多种事务模式。Seata是基于C/S架构的中间件，微服务应用需要依赖Seata客户端来完成和Seata服务端的通信，通信协议基于Seata自有的RPC协议。微服务应用通过Seata远程调用完成分布式事务的开启、注册，同时该通信链路也接受来自Seata服务端（由于事务状态变更而带来）的回调通知，其架构如下图所示：        使用Seata之前，需要先部署Seata服务端，服务端会将Seata服务注册到注册中心，目的是当依赖Seata客户端的微服务应用启动时，可以通过注册中心订阅到Seata服务，使Seata服务以集群高可用的方式暴露给使用者。        Seata的客户端和服务端有许多参数可以配置，比如：事务提交的重试次数或间隔时间，一般情况这些配置可以放在微服务应用或者Seata服务端上，但配置项过多带来了较高的维护成本。Seata支持将配置存放在配置中心上，通过配置中心上统一的管理起来，方便使用者进行运维。        Seata服务端可以通过依赖外部的数据存储将事务上下文等信息持久化存储起来，使Seata服务端无状态化，从而进一步提升稳定性。Seata可以选择多种开源的注册和配置中心以及数据存储，如下表所示： 类型 可选产品 功能描述 注册中心 文件、ZooKeeper、Redis、Nacos和ETCD等 Seata服务端注册Seata服务，Seata客户端进行服务发现 配置中心 文件、ZooKeeper、Nacos、ETCD和SpringCloud Config等 统一管理和维护Seata的配置信息 数据存储 文件、关系数据库和Redis 持久化存储全局事务、分支事务以及事务上下文信息        微服务应用通过依赖Seata客户端与Seata服务端进行通信，Seata客户端通过AOP以及对主流RPC框架的扩展来完成对微服务应用间远程调用的拦截。在远程调用前开启（或注册）分布式事务，当Seata服务端发现事务状态变化时，再回调部署在微服务应用中的Seata客户端来执行相应的逻辑。 Seata如何支持TCC        在TCC模式中，由事务管理器（一般也是事务参与者）开启全局事务，在事务逻辑执行过程中，该链路上所有节点（微服务应用）的分布式调用都会注册相应的分支事务，全局事务和分支事务会产生关联。当事务逻辑执行成功，代表全局事务可以提交，事务协调者则会回调各个事务参与者的确认逻辑，反之，回调其取消逻辑。        可以看到事务的开启和（节点之间的）传播是实现TCC的关键，Seata利用了AOP以及对主流RPC框架进行扩展来提供支持，接下来会简单介绍一下Seata对全局事务开启以及事务传播的相关逻辑。        在需要全局事务控制的方法上，通过添加注解GlobalTransactional将其标识为全局事务方法，该方法中的逻辑即为事务逻辑，在方法中的远程调用也会被全局事务所管理，其主要接口和类（以及部分主要方法）如下图所示：        可以看到Seata客户端通过实现spring-aop的方法拦截器来拦截用户的方法执行。Seata将全局事务抽象为GlobalTransaction，它和普通事务一样具备开始、提交和回滚等方法，当拦截到用户方法的调用（或异常）时，会触发全局事务对应的方法。Seata客户端与服务端通信底层基于netty，传输的自有RPC协议为RpcMessage，当事务管理器TransactionManager被调用时，会将相关事务操作远程通知到Seata服务端，可以认为在微服务之间进行业务远程调用的拓扑结构下还对应存在着一层Seata远程调用拓扑结构。        通过AOP以及远程调用的方式，可以让应用透明的开启全局事务，但在微服务架构下，如何能够做到当前事务在微服务之间传播呢？答案是，扩展应用使用的RPC框架。以Apache Dubbo为例（以下简称Dubbo），可以看到Seata通过扩展Dubbo过滤器的方式，使微服务之间具备了传播事务的能力，部分关键代码如下所示： @Activate(group = {DubboConstants.PROVIDER, DubboConstants.CONSUMER}, order = 100) public class ApacheDubboTransactionPropagationFilter implements Filter { @Override public Result invoke(Invoker invoker, Invocation invocation) throws RpcException { String xid = RootContext.getXID(); BranchType branchType = RootContext.getBranchType(); String rpcXid = RpcContext.getContext().getAttachment(RootContext.KEY_XID); String rpcBranchType = RpcContext.getContext().getAttachment(RootContext.KEY_BRANCH_TYPE); boolean bind = false; if (xid != null) { RpcContext.getContext().setAttachment(RootContext.KEY_XID, xid); RpcContext.getContext().setAttachment(RootContext.KEY_BRANCH_TYPE, branchType.name()); } else { if (rpcXid != null) { RootContext.bind(rpcXid); if (StringUtils.equals(BranchType.**TCC**.name(), rpcBranchType)) { RootContext.bindBranchType(BranchType.**TCC**); } bind = true; } } try { return invoker.invoke(invocation); } finally { // 略 } } }        Dubbo提供了对调用链路扩展的能力，这也说明它是一款非常成熟的RPC框架。可以看到在上述代码逻辑中，Seata的扩展点先尝试获取本地事务信息（包括：事务ID和事务模式），然后尝试获取Dubbo请求上下文中对应的远程事务信息。        如果能够获取到存储在ThreadLocal中的本地事务信息，表明当前代码运行在一个全局事务中，则尝试将事务信息放置到Dubbo请求上下文中，使之能够传递到下一个微服务节点。        如果本地事务信息没有获取到，但存在远程事务信息，这表明本次调用是Seata事务调用，则需要恢复远程事务信息到当前ThreadLocal中，将全局事务能够连接起来。        通过扩展Dubbo的Filter，使得Seata的全局事务具备了击鼓传花般的远程传输能力，事务逻辑中所有的分布式远程调用，均会在请求中“沾染”上事务信息，而这些信息也会被Seata服务端所掌握，最终在事务完成时，发起对所有事务参与者的回调。 一个基于Seata的参考示例        以商品订购场景为例，基于SpringBoot和Dubbo来实现该功能，同时依靠Seata确保分布式事务。示例中的部分业务代码仅打印了参数或结果，目的是方便读者观察执行的过程，由于示例代码（含单元测试）超过1400行，所以接下来仅针对关键代码进行介绍，应用全部代码可以在：https://github.com/weipeng2k/seata-tcc-guide 找到。 部署Seata        在运行示例前需要部署Seata服务端，Seata服务端一般以集群的方式进行部署，依赖注册和配置中心以及外部存储做到高可用。由于本文主要介绍微服务应用如何使用Seata实现TCC，简单起见采用单节点的方式进行部署。        可以选择在官网下载Seata服务端，解压后运行seata-server.sh启动，如下图所示：        默认Seata服务端（注册和配置中心以及外部存储）依赖的是本地文件。        当然也可以使用Docker进行部署，在安装了Docker的机器上运行如下命令： docker run --name seata-server -p 8091:8091 -d seataio/seata-server:latest        该命令在当前机器上启动了Seata服务端，同时暴露了Seata服务端的（默认）端口。 如果不在本机部署Seata服务端，需要记录部署了Seata服务端机器的IP，并且能够确保之后部署的微服务应用能够访问该IP。微服务应用中的配置项seata.service.grouplist.default需要配置为服务端的IP和端口。 应用代码简介        本示例中商品订购场景涉及三个微服务应用，其相关信息如下表所示： 应用 前台交易微服务 订单微服务 商品微服务 名称 trade-facade order-service product-service 领域实体 无 订单 商品库存库存占用明细 接口服务 TradeAction，商品下单接口 OrderCreateService，订单创建服务 ProductInventoryService，商品库存服务 功能描述 接收前端请求，调用OrderCreateService创建订单，同时调用ProductInventoryService扣减对应商品的库存 实现并发布OrderCreateService，维护订单模型与数据 实现并发布ProductInventoryService，维护商品库存相关模型与数据        用户订购请求通过trade-facade进入，首先会调用order-service生成订单，此时订单的是否可用状态为false，然后trade-facade调用product-service进行库存扣减，如果库存充足则减少商品预扣库存数量同时生成库存占用明细，以上为TRY阶段，相关部分代码如下： @Component(\"tradeAction\") public class TradeActionImpl implements TradeAction { @DubboReference(group = \"dubbo\", version = \"1.0.0\") private OrderCreateService orderCreateService; @DubboReference(group = \"dubbo\", version = \"1.0.0\") private ProductInventoryService productInventoryService; // fake id generator private final AtomicLong orderIdGenerator = new AtomicLong(System.currentTimeMillis()); @Override @GlobalTransactional public Long makeOrder(Long productId, Long buyerId, Integer amount) { RootContext.bindBranchType(BranchType.**TCC**); CreateOrderParam createOrderParam = new CreateOrderParam(); createOrderParam.setProductId(productId); createOrderParam.setBuyerUserId(buyerId); createOrderParam.setAmount(amount); Long orderId; try { orderId = orderIdGenerator.getAndIncrement(); orderCreateService.createOrder(createOrderParam, orderId); } catch (OrderException ex) { throw new RuntimeException(ex); } OccupyProductInventoryParam occupyProductParam = new OccupyProductInventoryParam(); try { occupyProductParam.setProductId(productId); occupyProductParam.setAmount(amount); occupyProductParam.setOutBizId(orderId); productInventoryService.occupyProductInventory(occupyProductParam, orderId.toString()); } catch (ProductException ex) { throw new RuntimeException(ex); } return orderId; } }        可以看到makeOrder方法上标注了GlobalTransactional注解，表示该方法需要事务保证，同时通过RootContext设置当前的事务模式为TCC。        对于OrderCreateService和ProductInventoryService，也需要增加Seata的注解，使得之后的CANCEL或CONFIRM通知能够调用到对应的逻辑，以OrderCreateService为例，代码如下： @LocalTCC public interface OrderCreateService { /** * 根据参数创建一笔订单 * * @param param 订单创建参数 * @param orderId 订单ID * @throws OrderException 订单异常 */ @TwoPhaseBusinessAction(name = \"orderCreateService\", commitMethod = \"confirmOrder\", rollbackMethod = \"cancelOrder\") void createOrder(CreateOrderParam param, @BusinessActionContextParameter(paramName = \"orderId\") Long orderId) throws OrderException; /** * * 根据订单ID确认订单 * * * @param businessActionContext 业务行为上下文 * @throws OrderException 订单异常 */ void confirmOrder(BusinessActionContext businessActionContext) throws OrderException; /** * * 根据订单ID作废当前订单 * * * @param businessActionContext 业务行为上下文 * @throws OrderException 订单异常 */ void cancelOrder(BusinessActionContext businessActionContext) throws OrderException; }        可以看到接口声明需要标注LocalTCC注解，同时在TRY阶段（也就是createOrder）方法上标注TwoPhaseBusinessAction注解，而其中commitMethod和rollbackMethod分别对应CONFIRM和CANCEL阶段方法。通过TwoPhaseBusinessAction注解的声明，Seata会知晓在全局事务提交或回滚时调用该接口的哪个方法。 LocalTCC、TwoPhaseBusinessAction和BusinessActionContextParameter注解需要标注在接口上才能被Seata所识别，这也是为什么TCC模式对应用的侵入性较强的一个原因。        如果订购成功，全局事务可以提交，Seata服务端会回调参与事务微服务的CONFIRM逻辑。本示例中，order-service的confirmOrder方法会被调用，订单的可用状态会被更新为true，product-service的confirmProductInventory方法会被调用，真实库存会被扣减，库存占用明细状态会更新为成功。        如果订购失败，全局事务需要回滚，失败的原因可能是调用order-service或product-service服务出现业务异常，比如：生成订单失败或库存不足，也有可能是系统异常，比如：调用超时或网络传输异常等，Seata服务端会回调参与事务微服务的CANCEL逻辑。本示例中，order-service的cancelOrder方法会被调用，订单可用状态会被更新为false，product-service的cancelProductInventory方法会被调用，预扣库存会被增加，库存占用明细状态会更新为取消。 Seata服务端会回调参与事务的微服务，这个参与代表着业务远程调用已经发起，如果没有调用则不会发起相应的回调。比如：在makeOrder方法代码中，逻辑上的事务参与者有trade-facade、order-service和product-service，但如果makeOrder方法在实际执行中，调用到order-service就抛错了，则CANCEL回调只会通知到trade-facade和order-service。 订购示例演示        订购示例的逻辑比较简单，先初始化一个商品的库存为20，然后本地模拟10个并发请求用于订购商品，每次订购的数量为3，相关代码如下所示： @SpringBootApplication @EnableDubbo @Configuration public class TradeApplication implements CommandLineRunner { private final ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(10, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue<>()); @Autowired private TradeAction tradeAction; public static void main(String[] args) { SpringApplication.run(TradeApplication.class, args); } @Override public void run(String... args) throws Exception { tradeAction.setProductInventory(1L, 20); CountDownLatch start = new CountDownLatch(1); CountDownLatch stop = new CountDownLatch(10); AtomicInteger orderCount = new AtomicInteger(); for (int i = 1; i { try { start.await(); } catch (InterruptedException e) { e.printStackTrace(); } try { tradeAction.makeOrder(1L, (long) userId, 3); orderCount.incrementAndGet(); } catch (Exception ex) { // Ignore. } finally { stop.countDown(); } }); } start.countDown(); stop.await(); Thread.sleep(1000); System.err.println(\"订单数量：\" + orderCount.get()); System.err.println(\"库存余量：\" + tradeAction.getProductInventory(1L)); } }        先启动order-service和product-service，然后运行trade-facade，可以看到输出： 订单数量：6 库存余量：2 微服务需要依赖注册中心，本示例的注册中心使用的是ZooKeeper。        输出显示成功生成了6笔订单，每笔订单包含3件商品，因此商品库存还剩2件，而这表明有4笔订单被取消。可以观察order-service的标准输出，能够看到TRY阶段的相关（部分）输出： 买家{7}购买商品{1}，数量为{3}，订单{1631264872732}生成@2021-09-10 17:07:56[DubboServerHandler-192.168.31.133:20880-thread-3] in Tx(172.18.0.3:8091:27191024100888792) . . 买家{10}购买商品{1}，数量为{3}，订单{1631264872731}生成@2021-09-10 17:07:56[DubboServerHandler-192.168.31.133:20880-thread-4] in Tx(172.18.0.3:8091:27191024100888799)        总共10条记录，可以看到每笔订单均在不同的事务（Tx）中生成，且运行的线程为Dubbo服务端线程（输出内容中的中括号所包含的为线程名）。        在TRY阶段之后，会出现CANCEL和CONFIRM阶段的（部分）输出： 买家{7}购买商品{1}，数量为{3}，订单{1631264872732}启用@2021-09-10 17:07:56[rpcDispatch_RMROLE_1_1_24] in Tx(172.18.0.3:8091:27191024100888792) . . 买家{9}购买商品{1}，数量为{3}，订单{1631264872728}取消@2021-09-10 17:07:57[rpcDispatch_RMROLE_1_8_24] in Tx(172.18.0.3:8091:27191024100888793)        其中订单启用的输出有6条，订单取消的输出有4条，同时注意到运行的线程为Seata的资源管理器线程。这说明TCC不同阶段的逻辑一般是由不同线程运行的，所以在实际使用过程中，需要注意线程安全问题。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/what-about-the-future.html":{"url":"book/what-about-the-future.html","title":"热风-未来的预测","keywords":"","body":"未来的预测 写在2018年，从现在的趋势，做一下预测        从业界这些年云计算不断的商用、分布式技术门槛的降低，越来越多的传统行业开始注重技术的投入，做了以下三个预测： 未来的公司都是软件公司        或者说混得好的公司，能够活下去的公司一定是软件公司，这不是说传统企业都会完蛋，而是说任何企业都会投资软件和IT，并且不断优化它与自己主营业务之间的关系，并由它带来收益，用它来指导方向、增强和客户之间的联系以及优化流程。        实质就是精益化的管理，这个需要IT的支持，让企业明白自己要干什么、客户是谁、当前的主要矛盾是什么，能够以数字化的方式进行体现。 来自2021的更新 2020年5月22日，国务院总理李克强在发布的2020年国务院政府工作报告中提出，全面推进“互联网+”，打造数字经济新优势。可以看到软件或者说数字化对于人们的生产生活产生的影响，以及我们知道通过追求它能够获得确定性的好处和优势。在央视的节目《大数据时代》能够很清楚的看到数字化的运作以及数据的作用已经深得民心，而大数据在Covid-19危机中体现出的价值更是不言而喻。 可以看到未来较长一段时间，行业的软件化（数字化）依旧是发展的重点，目的是可度量和可优化，而头部企业对于数据处理和治理等问题将会呈现出更大的需求，毕竟发展不是平衡的，已经做得不错的行业需要更进一步的利用好数字化带来的成果，当然也会为此更多的付费，而前者可能就是辛苦钱。 未来的技术成本持续降低        云计算的商用以及搭建一个分布式系统不再是大型IT巨头独享的专利。从前端、分布式服务、消息服务以及数据存储，包括现在的资源调度，都有开源成熟的解决方案，而且会随着时间的推移，变得愈发简单。        开源产品的功能性以及产品化的情况会越来越好，大部分小型企业甚至会基于这些产品（当然需要有懂得这些产品的研发人员）搭建自己的分布式服务，而且产品使用体验会越来越好。 来自2021的更新 阿里云、腾讯云和华为云相继推出1C/2G虚拟机一年的服务（新用户售价都在70左右），这在以往是不敢想象的，这也代表着云计算以及低成本的使用云成为可能。Docker/K8s工具链的更加易用，在云厂商的环境上搭建适合自己的分布式环境变得很容易，而且各厂商都在以兼容或者适配标准作为卖点。 面向用户不同研发阶段以及业务的混合云将会是个方向，基于开源产品的编程界面统一了用户的Codebase，在这个基础上，用户对于开发、测试、预发以及生产环境的云需求一定不同，简单说云成本问题一定会暴露。在开发阶段，低成本的ARM集群会是一个方向。世界是凌乱的，但是只要编程界面统一，就还有机会。 未来的开发者将会成为市场角力的关键        从github被微软收购以及谷歌在CNCF上的投入，各个巨头都在争夺对开发人员脑部的占有率。由于前两点的出现，开发者的权力将会在未来的发展中逐步上升，而占据开发人员的心智，将会左右开发人员在IT方面的认知，进而左右这些公司的发展。        开发者如何能够在纷繁的技术新闻中不迷失自我，公司如何在技术旋涡中不被左右，需要开发者和公司一样，能够识别出自己需要什么，当前的主要矛盾是什么，而不是沦为开源（或者背后实质公司，比如：谷歌）的马前卒。 来自2021的更新 以谷歌为代表的硅谷持续的输出技术标准，从ServiceMesh到Serverless，从人工智能到区块链，中国的云厂商只能亦步亦趋的学习。中美贸易战上升到技术层面，美国对中国的TechGiant进行狙击，信息化推进工作从原有的工信部上升到中共中央网络安全和信息化小组，而小组长是国家主席习近平，这也标着着国家重实业（基础），轻服务（应用）的开始，而这个决定是非常明智的。 2019年8月，华为推出了鸿蒙操作系统，随后在2021年5月，更新到鸿蒙2.0，基础软件的更新是得到国人的认可。基于基础软件和基础硬件的投入，势必在学科教育和成果应用等多方面影响到后续的技术人员。对国产技术，或者保持开放的基础技术（比如：Linux操作系统、计算机基础技术和ARM（RISC-V）等）的持续投入会很有价值，为摆脱未来可能出现的思想屏障奠定了基础，毕竟脚踏实地，强健国力是颠扑不破的。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/seata-problem.html":{"url":"book/seata-problem.html","title":"热风-Seata存在的一些问题","keywords":"","body":"Seata存在的一些问题 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-network-inetaddress-and-dns.html":{"url":"book/java-network-inetaddress-and-dns.html","title":"热风-InetAddress与DNS","keywords":"","body":"JavaNetwork's InetAddress与DNS        使用Java进行Socket编程，需要知晓对端的主机和端口，然后使用SocketAPI进行编程通信。以发送一个HTTP请求和获取响应为例，使用Java可以这样写： @Test public void http() throws Exception { InetAddress inetAddress = InetAddress.getByName(\"www.baidu.com\"); SocketAddress socketAddress = new InetSocketAddress(inetAddress, 80); Socket socket = new Socket(); // imply bind & connect socket.connect(socketAddress, 3000); PrintWriter out = new PrintWriter(socket.getOutputStream()); out.println(\"GET / HTTP/1.0\\r\\n\"); out.println(\"HOST: www.baidu.com\\r\\n\"); out.println(\"Accept-Encoding:gzip,deflate\\r\\n\"); out.println(\"Accept: */*\\r\\n\"); out.println(\"\\r\\n\"); out.flush(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(socket.getInputStream())); String line = null; while ((line = bufferedReader.readLine()) != null) { System.out.println(line); } }        上述示例创建了一个socket，连接到www.baidu.com，然后请求其首页，将HTTP响应以文本形式打印出来。这里我们不关注通过socket获取输入输出流的操作，而是关注socket是如何构造和初始化的。任何语言构建socket都差不多，底层都是依赖操作系统提供的协议栈，也就是一个通用的内核级别程序，由它来专门负责同网卡打交道。        在示例中的第一步，首先会通过静态方法getByName()创建InetAddress实例，然后再使用InetAddress实例再加上端口创建SocketAddress实例。构建好的Socket实例，然后通过传入SocketAddress实例和连接超时，调用connect()方法，协议栈会在绑定本地网卡和（随机）端口后，尝试同www.baidu.com:80建立TCP连接。        InetAddress代表了IP协议中的地址信息，IP地址是IP协议中的重要组成部分，网络包的路由都依托于IP地址，IP地址是一个32位(或128位）的无符号整数。InetAddress类提供了一些静态方法，如：getByName()和getAllByName()，传入主机名返回相应的IP地址。这些方法是基于DNS协议来实现的，可以通过DNS服务器来解析主机名。在使用getByName()方法时，如果主机名无法解析IP地址，则会抛出UnknownHostException异常。        客户端不能同www.baidu.com直接建立连接，而是要通过DNS解析，将域名或主机名转换为IP，这里的www.baidu.com是主机，而baidu.com是域名。因此可以推断出，在调用InetAddress的getByName()方法时，会发起网络调用，而这个网络调用就是DNS查询。 InetAddress#getByName分析        InetAddress getByName(String host)，该方法接受一个主机或者域名，返回DNS查询的结果，结果类型是InetAddress，它包括了主机名以及地址，例如： @Test public void getByNameStringInternet() throws Exception { InetAddress address = InetAddress.getByName(\"www.taobao.com\"); System.out.println(\"HostName:\" + address.getHostName()); System.out.println(\"HostAddress:\" + address.getHostAddress()); }        运行测试用例，输出如下： HostName:www.taobao.com HostAddress:61.174.43.210        既然是IP地址是查询DNS服务器获得的，那getByName()方法势必要发起远程调用，Java是怎样实现的呢？我们接下来看一下代码实现，以下代码基于JDK17。首先是调用getAllByName()，获取主机host对应的IP地址，以下是部分代码。 private static InetAddress[] getAllByName(String host, InetAddress reqAddr) throws UnknownHostException { if (host == null || host.isEmpty()) { InetAddress[] ret = new InetAddress[1]; ret[0] = impl.loopbackAddress(); return ret; } // Check and try to parse host string as an IP address literal if (IPAddressUtil.digit(host.charAt(0), 16) != -1 || (host.charAt(0) == ':')) { if(addr != null) { InetAddress[] ret = new InetAddress[1]; if (addr.length == Inet4Address.INADDRSZ) { if (numericZone != -1 || ifname != null) { // IPv4-mapped address must not contain zone-id throw new UnknownHostException(host + \": invalid IPv4-mapped address\"); } ret[0] = new Inet4Address(null, addr); } else { if (ifname != null) { ret[0] = new Inet6Address(null, addr, ifname); } else { ret[0] = new Inet6Address(null, addr, numericZone); } } return ret; } } else if (ipv6Expected) { // We were expecting an IPv6 Literal since host string starts // and ends with square brackets, but we got something else. throw invalidIPv6LiteralException(host, true); } return getAllByName0(host, reqAddr, true, true); }        可以看到该方法的实现是针对传入的host参数是IP类型的判断，没有涉及到域名解析，它主要完成了参数是IP类型的请求处理，那域名解析呢？域名解析为什么还需要接受IP类型参数？这是因为DNS解析是双向的，可以给定IP查询对应的主机名。而在最后的getAllByName0()方法中，比较关键的逻辑如下： // look-up or remove from cache Addresses addrs; if (useCache) { addrs = cache.get(host); } else { addrs = cache.remove(host); if (addrs != null) { if (addrs instanceof CachedAddresses) { // try removing from expirySet too if CachedAddresses expirySet.remove(addrs); } addrs = null; } } if (addrs == null) { // create a NameServiceAddresses instance which will look up // the name service and install it within cache... Addresses oldAddrs = cache.putIfAbsent( host, addrs = new NameServiceAddresses(host, reqAddr) ); if (oldAddrs != null) { // lost putIfAbsent race addrs = oldAddrs; } } // ask Addresses to get an array of InetAddress(es) and clone it return addrs.get().clone(); 其中Addresses接口是返回多个InetAddress的内部接口，其实完全可以使用Supplier接口来替代，这里不得不说，Java的net包下，InetAddress等相关实现类，写的不怎么好，有些乱，API和SPI傻傻的分不清楚。        可以看到获取IP地址时，会从静态缓存中获取，通过配置，是可以修改缓存过期时间的，在某些HTTP短连接场景，是一个不错的优化方案，这个我们在后面讲述。接下来就是调用DNS查询，Java对DNS服务做了抽象，定义了如下内部接口： /** * NameService provides host and address lookup service * * @since 9 */ private interface NameService { /** * Lookup a host mapping by name. Retrieve the IP addresses * associated with a host * * @param host the specified hostname * @return array of IP addresses for the requested host * @throws UnknownHostException * if no IP address for the {@code host} could be found */ InetAddress[] lookupAllHostAddr(String host) throws UnknownHostException; /** * Lookup the host corresponding to the IP address provided * * @param addr byte array representing an IP address * @return {@code String} representing the host name mapping * @throws UnknownHostException * if no host found for the specified IP address */ String getHostByAddr(byte[] addr) throws UnknownHostException; }        可以看到该服务定义了两个方法，使用主机名获取IP地址的lookupAllHostAddr()和根据IP地址获取主机名的getHostByAddr()。能抽象出一个NameService是非常有必要的，但是这是在Java9中定义的，都已经有模块化的支持了，如果要扩展NameService怎么办？这里大胆的猜测，定义NameService以及相关抽象的同学，在重构这块代码时，不太理解Java9的模块化编程方式，只是简单的使用了静态初始化的方式，用传统手艺完成了工作。        在NameService的默认实现PlatformNameService中，以IPv4为例，调用的是Inet4AddressImpl本地方法lookupAllHostAddr()来获取。 class Inet4AddressImpl implements InetAddressImpl { public native InetAddress[] lookupAllHostAddr(String hostname) throws UnknownHostException; }        Java自己不是有SocketAPI么？为什么还需要使用native方法来完成这个工作？其实好理解，如果InetAddress依赖Socket，结果Socket创建又依赖InetAddress，就有些尴尬了，没关系，我们接着看JVM的实现。        部分JVM实现代码： /* * Perform the lookup */ if ((hp = gethostbyname((char*)hostname)) != NULL) { struct in_addr **addrp = (struct in_addr **) hp->h_addr_list; int len = sizeof(struct in_addr); int i = 0; while (*addrp != (struct in_addr *) 0) { i++; addrp++; } }        可以看到通过调用系统函数gethostbyname()来解析域名，该函数背后的实现是通过UDP发起DNS查询，从而获得到主机（或域名）对应的IP列表。操作系统在启动时，会获取本地网络中的DNS服务器IP，并将其设置给协议栈，所以InetAddress不用去指定DNS服务器。我们运行测试用例，执行getByNameStringInternet()测试方法，然后使用wireshark抓包看看。        如上图所示，从上到下就是网络协议自底向上，这是抓取到从en0网卡发出的DNS查询请求与响应，图中标注了一些重点，按照序号： 说明网络包是从en0网卡发出的； 以太网头部，其中SRC和DST分别指的是来源和目标的MAC地址，因为链路层只负责点对点的数据传输； IP头部，其中协议值为17，表示在IP之上跑的传输层协议是UDP； IP头部，目标地址是DNS服务器地址； UDP头部，其中端口是53，DNS服务器默认端口是53； 应用层协议头部，DNS请求的查询参数，其中Class类型是IN(ternet)，业务类型type是主机地址，参数是：www.taobao.com。        可以看出来DNS并不是设计出来专门针对互联网，也想支持其他网络，但是现如今互联网占据了统治地位。同时DNS请求头中有Transaction ID，这个ID用来将请求和响应在客户端对应起来，因此DNS协议是异步的，效率应该挺高的。        既然192.168.31.1能够返回DNS响应，想必它存储了一些数据，一般DNS服务器会存储域名和IP的相关信息，可以理解为一张表，它记录着：domain、class、type以及data，其中data这一列中可以存域名别名或者IP等。刚发起对www.taobao.com的DNS查询，就可以理解为：select data from dns where domain=“www.taobao.com” and class=“IN” and type=“A”。 InetAddress#getHostName分析        如果是查表，能否支持根据data来查询呢？也就是使用IP查询当前的主机名，答案是可以的。接着看如下测试用例： @Test public void getByNameStringInternet1() throws Exception { InetAddress address = InetAddress.getByName(\"192.168.31.1\"); System.out.println(address.getHostName()); }        运行测试用例，输出如下： xiaoqiang        不知道为什么小米路由器的主机名为什么叫小强，八卦了一下，貌似代码都是xiaoqiang开头，参见视频。接下来看一下Java实现，其中（主要）部分如下： private static String getHostFromNameService(InetAddress addr, boolean check) { String host = null; try { // first lookup the hostname host = nameService.getHostByAddr(addr.getAddress()); InetAddress[] arr = InetAddress.getAllByName0(host, check); boolean ok = false; if(arr != null) { for(int i = 0; !ok && i        可以看到通过调用NameService的getHostByAddr()方法获取到IP对应的主机名。运行测试用例，通过wireshark抓包，通过观察DNS响应。        如上图所示，调用InetAddress实例的getHostName()方法，会发起DNS查询。通过观察DNS响应，可以看到请求和响应内容，按照序号： 查询参数，IP是192.168.31.1； 响应结果，domain是XiaoQiang。        根据某个主机（或域名）构造一个InetAddress，然后建立TCP连接后，就可以进行数据交换。如果每次连接建立都进行DNS解析，就显得有些累赘，毕竟主机（或域名）对应的IP不经常变动。因此，Java会通过一个静态缓存来存储解析成功和不成功的InetAddress。        默认情况下，Java会将解析成功的InetAddress缓存30秒，对于解析不成功的主机名会缓存10秒。如果希望增加解析成功的缓存时间，可以通过设置Java系统变量networkaddress.cache.ttl，单位是秒，如果对解析不成功的缓存时间，可以使用变量networkaddress.cache.negative.ttl，如果需要永久缓存，值可以设为-1。如果系统中有短连接的访问方式，适当的增加DNS缓存时间，对提升链路性能会有所帮助。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-network-arp-and-pcap4j.html":{"url":"book/java-network-arp-and-pcap4j.html","title":"热风-ARP与pcap4j","keywords":"","body":"JavaNetwork's ARP与pcap4j        数据链路层是网络传输的基础，在常见的TCP/IP五层（也有四层说法的）结构中，数据链路层处于物理层的上方，网络层的下方，自顶向下的倒数第二层。本文会介绍数据链路层上的ARP（Address Resolution Protocol）协议，以及它在网络数据访问过程中发挥的作用，出了会涉及到ARP以外，还会介绍路由表和路由计算。        在正式开始前，先说一个咒语“路由靠网络，传输靠链路”，如果需要具像化一些，可以改为“路由靠IP，传输靠以太”。 网络拓扑        笔者的网络环境非常典型，笔记本通过WLAN（Wireless Local Area Networks）连接到（小米）路由器，而路由器的WAN（Wide Area Network）口连接到光猫的LAN（Local Area Network）口上，如下图所示：        可以看到从笔记本发起的一次对公网的HTTP请求，比如：访问www.baidu.com，需要将请求消息转换为网络包，然后经过笔记本网卡，穿过路由器，再透过光猫，到达电信局局端接入。电信局的局端接入再通过隧道抵达ISP网络，而多个ISP通过分组交换和路由将网络包最终送到ICP的服务器。HTTP响应回复链路与请求相似，但在网络传输层面和请求链路没有任何关系，ICP服务器的根据请求生成响应，服务器会将生成的文本信息转换为网络包，根据之前的请求IP地址，反向在发送回去，再经过一路坎坷，回到笔记本。 ISP（Internet Service Provider），互联网服务提供商，比如：中国电信、中国移动或中国联通等。 ICP（Internet Content Provider），互联网内容提供商，比如：百度、阿里和腾讯等。        使用Java可以很轻松的发起HTTP请求，例如：使用Feign等客户端，可以将HTTP请求和响应的处理简化到极致。However，在这里我们基于Java提供的Socket，通过编写（HTTP协议）应用层的语法来发起请求，并输出响应，代码如下： @Test public void http() throws Exception { InetAddress inetAddress = InetAddress.getByName(\"www.baidu.com\"); SocketAddress socketAddress = new InetSocketAddress(inetAddress, 80); BufferedReader bufferedReader; try (Socket socket = new Socket()) { // imply bind & connect socket.connect(socketAddress, 3000); PrintWriter out = new PrintWriter(socket.getOutputStream()); out.println(\"GET / HTTP/1.0\\r\\n\"); out.println(\"HOST: www.baidu.com\\r\\n\"); out.println(\"Accept-Encoding:gzip,deflate\\r\\n\"); out.println(\"Accept: */*\\r\\n\"); out.println(\"\\r\\n\"); out.flush(); bufferedReader = new BufferedReader(new InputStreamReader(socket.getInputStream())); String line; while ((line = bufferedReader.readLine()) != null) { System.out.println(line); } } }        运行测试用例，控制台输出（部分）内容如下： HTTP/1.0 200 OK Accept-Ranges: bytes Cache-Control: no-cache Content-Length: 9508 Content-Type: text/html Date: Tue, 12 Dec 2023 07:11:09 GMT P3p: CP=\" OTI DSP COR IVA OUR IND COM \" P3p: CP=\" OTI DSP COR IVA OUR IND COM \" Pragma: no-cache Server: BWS/1.1        由于控制台输出内容过长，只是截取了部分HTTP响应头，正文部分都省略了。上述测试用例运行时，需要建立笔记本到www.baidu.com主机之间的连接，通过连接向对端发送HTTP消息。笔记本通过使用路由器提供的DNS服务，其实一开始就知道了www.baidu.com的IP，但是在将HTTP请求消息的网络包发送到路由器之前，还发生了很多事。        首先当然是建立连接，其次准备好的HTTP请求消息会被操作系统协议栈根据MSS（Maximum Segment Size）进行内容分割。内容分割是传输层的工作，封装好的Segment会被放置到IP分组中，这个IP分组会写上发送方，也就是笔记本的IP，以及接收方，也就是DNS服务查询到www.baidu.com的那个IP，一个IP分组就生产完成，蓄势待发了。 Route机制        笔记本连接在路由器上，路由器也连接了多个设备，当然它们也都分配了子网IP。那为什么从笔记本发起对www.baidu.com的访问，会从路由器到光猫，而根据局域网中其他设备的主机名进行访问时，会到内网设备而非光猫呢？其实本质上讲，笔记本也是路由器，只要支持IP规范的设备，理论上都会是一台路由器。        基于IP的分组交换网络，其状态性主要体现在以下两点：第一，分组信息，它描述了来源和目标等信息；第二，路由规则，它定义了当前路由设备处理分组时的策略。每一个IP分组信息都有自己的状态，每一台路由设备也都有自己的路由策略。之前不是提到笔记本也是路由器么？那么它的路由策略是什么样的？在访问www.baidu.com的过程中起了什么作用呢？        通过在控制台输入netstat -rln，可以输出当前系统的路由策略，也就是路由表。输出的列挺多，但我们只需要关注其中几列：Destination、Gateway和Netif，其中Netif应该是Net Interface的简写，也就是网络接口的意思，代表网卡。        当路由器在处理一个分组时，首先需要进行Destination精确匹配，如果没有找到，接下来会根据IP分组中目标IP的子网进行向上匹配，如果都没有命中，那么就会选择Destination为default的这一行记录，也就是默认网关。一般来说，访问外网，都会命中到这一行。        分组会通过Netif发往Gateway指定IP对应的设备，本示例中的Netif就是en0网卡，而Gateway的IP是192.168.31.1，也就是路由器的IP。可以想象，在运行测试用例时，访问www.baidu.com的IP分组在发送前，会首先依据路由表中的路由信息找到目标Gateway，然后使用Netif进行发送。        为什么要找到en0网卡？因为操作系统发送网络数据时，操作系统内核会调用网卡驱动，将数据写入到网卡设备的存储中，也就是进入发送缓冲区，接下来网卡再进行成帧以及调制（或解调）工作就是网卡自己的事情了。这里和本文所要描述的内容没有多少关系，只是简单的说明一下。 根据IP找MAC        网络接口确定了，IP分组也构建好了，就可以发送网络包了，而上述过程和动作在Java中没有对应的编程API，或者说进入到操作系统内核后，Java就管不了了。虽然Java管不了，但是起点还是在Java，Java over OS，IP over Ethernet。IP协议只是在操作协议栈中的逻辑概念，最终进行传输的时候还是需要依赖数据链路层，或者说承载网。        瑟夫和卡恩设计TCP/IP时，其目的就是为了连接不同承载网络，比如：夏威夷的ALOHA网、美国军方的ARPA网或者施乐公司的以太网等，这些网络实际都是自底向上建设的。它们会考虑物理连接，以及在这些物理连接上如何传输信号，以及信号的制式以及语法都会去定义和实现。不同的承载网络无法进行互联，因为在它们上面传输的协议格式都不统一，如果需要做到不同的网络能够互联互通，该怎么办呢？        大概率你会想到：抽取一个中间层，在这个中间层里定义了公共协议（包含数据格式）以及行为（接口），然后不同的承载网去适配它就行了。说的没错，软件工程中的依赖反转原则就是这么解决问题的，而瑟夫和卡恩就设计了这个中间层，也就是TCP/IP协议，通用的IP协议有了，应用层就可以基于它来编程了，并且应用能够跑在不同的承载网上，这也就是所谓的Everything over IP。        光包装也不能解决所有问题，毕竟还是需要底层实现的，就像Spring再厉害，还是需要Tomcat来提供HTTP服务。概念上可以理解为链路层适配了网络层的SPI，完成到IP协议的接入，但在适配逻辑中还是需要依赖链路层的具体实现。网络实现或者说承载网能不能脱离IP单独使用呢？当然可以，但是反过来，网络就无法工作。以太网（Ethernet）是现在使用最广泛的数据链路层服务，看到具备RJ45接口的网卡以及双绞线都是它的产物。        以太网依靠MAC地址来进行通信，可以理解为每一个支持以太网传输的设备都会拥有全球唯一的MAC地址，这个MAC地址是生产该设备时烧录进去的，它有48位，讲道理是非常大的。在控制台中输入：ifconfig，一般就可以看到它，以下是（部分）输出内容： weipeng2k@weipengdeNewMBP ~ % ifconfig en0: flags=8863 mtu 1500 options=6460 ether f0:18:98:1f:91:e4 inet6 fe80::c9e:ec76:dab0:24f4%en0 prefixlen 64 secured scopeid 0x6 inet 192.168.31.137 netmask 0xffffff00 broadcast 192.168.31.255 nd6 options=201 media: autoselect status: active        如输出所示，f0:18:98:1f:91:e4就是笔记本en0网卡的MAC地址，一般MAC地址还具备一些隐私性的，因为它48位数据中的部分会和设备厂商相关，比如：Apple，因此是可以通过MAC地址分析出设备的厂商，判断出大概种类。        IP分组构建好了，发送数据的网卡en0选择好了，目标Gateway的IP：192.168.31.1也拿到了，到了最终发送之前，还是要乖乖的切换到以太网上，说以太网才能听懂的话。这就需要使用将IP换成MAC地址的服务，也就是ARP的支持。ARP构建在数据链路层，也就是以太网之上，所以从层级关系上讲，它和网络层是平级的。        在以太网中传输，需要将IP分组放置到以太网能看懂的帧里，以太网支持点对点传输，来源就是本机网卡en0的MAC地址，这个在系统加载的时候就会初始化，不需要额外获取，而网关192.168.31.1的MAC地址就需要ARP的帮助了。ARP工作过程大致分为三步：第一步，使用以太网广播的方式，在局域网中，吼一声谁有这个IP；第二步，拥有这个IP的节点会点对点的发起查询的节点，我有这个IP，且MAC地址是多少；第三步，发起查询的节点会将IP与MAC地址的对应关系缓存起来，方便未来的查询。        使用wireshark，可以进行网络抓包，看一下上述过程，该过程如下图所示：        如上图所示，按照序号进行介绍。        先看第一个，wireshark的包过滤功能。可以通过输入表达式来过滤需要关注的包，如果你使用wireshark进行抓包，会发现网络包非常多，虽然只是抓在自己网卡上进出的包，但是你会发现整个（局域）网络中充斥着不同协议的包，传输形式也是多种多样，有点对点的，也有广播的，甚是嘈杂。eth.type == 0x0806，代表以太网帧中类型是ARP，当然IP协议也是一种类型，只是不同的数值而已，链路层收到数据，完成解析，然后交给操作系统协议栈，内核按照不同的类型来进行策略处理就行了。就算限定类型为ARP也会有很多包，所以通过eth.src来指定来源可以是笔记本en0网卡或者路由器，这样就更进一步过滤不关注的网络包。 对应的MAC地址需要读者根据自己的实际情况来设置，它们肯定是不同的。        顺便说一句，如果想过滤IP协议相关的，就可以使用ip.dst_host（目标IP）等不同的key来进行过滤，输入时wireshark会有提示，非常的贴心。        接下来看第二个，ARP执行过程。前文已经讲述过ARP的工作过程，这里可以更加形象的看到网络包：第381号包，笔记本进行以太网广播，问询谁拥有192.168.31.1这个IP，紧接着第382号包，也就是拥有这个IP的设备以点对点的形式，回复了它拥有这个IP，并且对应的MAC是什么。 ARP协议分析        从ARP的工作过程来看，一次广播，一次点对点，还是比较简单的。由于ARP是构建在以太网上的，所以必须按照以太网的规范来，梅特卡夫和博格斯设计的以太网需要在通信时指定源MAC地址和目标MAC地址，对于发起方来说，源MAC地址是很容易获取到的，但是目标MAC地址却不一定，比如：需要进行以太网广播的时候，MAC地址应该设置成什么呢？看一下第381号包的内容就明白了，如下图所示：        如上图所示，可以看到ARP请求报文的详细内容，按照序号进行介绍。        第一个，以太网的目标MAC地址。在进行ARP请求时，这是一个广播调用，它会将以太网帧发给局域网中所有支持以太网协议的设备，由于这是一个未知的设备集合，所以它们就有了一个专属的MAC地址，即：ff:ff:ff:ff:ff:ff，也就是48位全为1的MAC地址，向它发送帧，即向所有设备发送帧。ARP协议，选择使用以太网广播的形式，向局域网中所有的设备发起问询。        第二个，ARP问询的内容。ARP协议设计的比较小巧，Opcode代表当前报文是请求还是响应，而发送方和目标方的IP以及MAC地址紧随其后。在进行ARP请求时，发送方的IP和MAC地址是已知的，但目标方只有IP地址是确定的，其MAC地址并不知晓，因此采用全为0的MAC地址代替，也就是00:00:00:00:00:00，它代表一个未初始化的MAC地址。        ARP请求报文准备完成，封装到以太网的帧中，广播给所有设备。可以想象，收到对应帧的设备会进行解码，然后根据帧类型调用操作系统协议栈进行处理报文内容，而协议栈进行策略处理时，会交给ARP协议处理器完成处理。ARP处理过程也会比较简单，首先看目标IP是不是本机（某块网卡的）IP，如果不是就忽略，如果是就回复请求方，顺便捎带上自己的MAC地址。        第382号包为ARP响应报文，如下图所示：        如上图所示，依旧按照序号进行介绍。        第一个，以太网发送的目标、来源以及类型。该帧发自路由器，目标是笔记本en0网卡，类型是ARP，这代表着拥有192.168.31.1这个IP的设备，它应答了。        第二个，ARP响应报文。可以看到Opcode代表当前报文是响应，由于是点对点通信，发送方和目标方的IP以及MAC地址都是完备的。        笔记本通过en0网卡收到ARP响应报文后，会更新内存中IP与MAC地址的对应关系，后续再需要进行IP与MAC地址转换时，就可以从内存中直接获取，而不用发起网络调用了。如果对应关系缓存在本地，假设IP变动了怎么办？其实只要是缓存，就会有过期时间，当这个对应关系在本地存在了几十分钟后，就会系统被删除，若要获取，还需要再次发起ARP请求。 Java与ARP        ARP完成了IP到MAC地址的转换，协调了网络层和数据链路层之间的互通，是计算机网络中非常重要的组成部分。那么Java和ARP有什么关系呢？或者说使用Java能发送ARP请求和处理ARP响应吗？很遗憾，Java构建在传输层之上，使用面向传输层的socket进行编程，网络层（及其以下层次）是Java无法触及到的。当然，凡事没有绝对，使用JNI的帮助，也会有一些Java类库支持ARP协议处理，pcap4j就是其中之一。        访问其主页，可以查看到安装说明，在maven项目中依赖如下坐标： org.pcap4j pcap4j-core 1.8.2 org.pcap4j pcap4j-packetfactory-static 1.8.2        然后新建测试用例ARPTest，代码如下： import org.junit.Test; import org.pcap4j.core.BpfProgram; import org.pcap4j.core.PacketListener; import org.pcap4j.core.PcapHandle; import org.pcap4j.core.PcapNetworkInterface; import org.pcap4j.core.Pcaps; import org.pcap4j.packet.ArpPacket; import org.pcap4j.packet.EthernetPacket; import org.pcap4j.packet.Packet; import org.pcap4j.packet.namednumber.ArpHardwareType; import org.pcap4j.packet.namednumber.ArpOperation; import org.pcap4j.packet.namednumber.EtherType; import org.pcap4j.util.ByteArrays; import org.pcap4j.util.MacAddress; import java.net.InetAddress; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.TimeUnit; public class ARPTest { private static final MacAddress SRC_MAC_ADDR = MacAddress.getByName(\"f0:18:98:1f:91:e4\"); private static MacAddress resolvedAddr; @Test public void arpRequest() throws Exception { // 本机IP String strSrcIpAddress = \"192.168.31.139\"; // 目标IP，带查询MAC的IP String strDstIpAddress = \"192.168.31.58\"; InetAddress addr = InetAddress.getByName(strSrcIpAddress); // 根据IP获取到对应的Pcap网络接口，可以理解获取到了en0网卡 PcapNetworkInterface nif = Pcaps.getDevByAddress(addr); // 监听网卡的流入数据，每个包监听的长度为65536 bytes PcapHandle handle = nif.openLive(65536, PcapNetworkInterface.PromiscuousMode.PROMISCUOUS, 10); // 向网卡发送数据的入口，使用它来发送ARP请求报文 PcapHandle sendHandle = nif.openLive(65536, PcapNetworkInterface.PromiscuousMode.PROMISCUOUS, 10); // 构建监听网卡流量运行任务的线程池 ExecutorService pool = Executors.newSingleThreadExecutor(); try { // 设置监听流量的规则：监听ARP包，IP地址以及目标MAC地址是本机的包 handle.setFilter( \"arp and src host \" + strDstIpAddress + \" and dst host \" + strSrcIpAddress + \" and ether dst \" + Pcaps.toBpfString(SRC_MAC_ADDR), BpfProgram.BpfCompileMode.OPTIMIZE); // 对于ARP协议的包进行处理，且仅处理ARP响应报文，记录并打印 Task t = new Task(handle, packet -> { if (packet.contains(ArpPacket.class)) { ArpPacket arp = packet.get(ArpPacket.class); if (arp.getHeader().getOperation().equals(ArpOperation.REPLY)) { ARPTest.resolvedAddr = arp.getHeader().getSrcHardwareAddr(); System.err.println(packet); } } }); pool.execute(t); // 构建ARP报文，可以看到ARP被设计用在多种链路层上，而目标MAC地址设置为全0 ArpPacket.Builder arpBuilder = new ArpPacket.Builder(); arpBuilder .hardwareType(ArpHardwareType.ETHERNET) .protocolType(EtherType.IPV4) .hardwareAddrLength((byte) MacAddress.SIZE_IN_BYTES) .protocolAddrLength((byte) ByteArrays.INET4_ADDRESS_SIZE_IN_BYTES) .operation(ArpOperation.REQUEST) .srcHardwareAddr(SRC_MAC_ADDR) .srcProtocolAddr(InetAddress.getByName(strSrcIpAddress)) .dstHardwareAddr(MacAddress.getByAddress(new byte[]{0, 0, 0, 0, 0, 0})) .dstProtocolAddr(InetAddress.getByName(strDstIpAddress)); // 将ARP报文放置到以太网的分组中，目标MAC地址为：ff:ff:ff:ff:ff:ff EthernetPacket.Builder etherBuilder = new EthernetPacket.Builder(); etherBuilder .dstAddr(MacAddress.ETHER_BROADCAST_ADDRESS) .srcAddr(SRC_MAC_ADDR) .type(EtherType.ARP) .payloadBuilder(arpBuilder) .paddingAtBuild(true); Packet p = etherBuilder.build(); System.out.println(p); // 发送分组 sendHandle.sendPacket(p); TimeUnit.SECONDS.sleep(2); } finally { if (handle.isOpen()) { handle.close(); } if (sendHandle.isOpen()) { sendHandle.close(); } if (!pool.isShutdown()) { pool.shutdown(); } System.out.println(strDstIpAddress + \" was resolved to \" + resolvedAddr); } } private record Task(PcapHandle handle, PacketListener listener) implements Runnable { @Override public void run() { try { handle.loop(1, listener); } catch (Exception ex) { // Ignore. } } } }        上述代码做了详细注释，这里就不做赘述了，运行测试用例，可以看到如下输出：        可以看到从192.168.31.139发起的ARP查询，目标IP地址是192.168.31.58，得到其MAC地址是2a:d1:f7:53:e1:88，整个过程与wireshark抓包分析的情况类似。        最后，如果还记得wireshark抓包的截图，再回想前文提到的MAC地址和设备厂商的关系，就会发现00:00:00:00:00:00被wireshark标注的厂商前缀是xerox，这是什么公司？Xerox，即施乐公司，就是它发明了以太网，而以太网在不经意间构筑起了互联网的身躯。Xerox没有因为互联网的快速发展而赚得盆满钵满，梅特卡夫和博格斯也没有凭借以太网这项技术实现财富自由，他们只是专注在做自己喜欢的事情上，掸掸身上的尘土，无意间改变了世界。本文所提到的ARP协议也是一样，在没有多少人关注的角落里，进行着无比重要的工作，在它的帮助下，你的浏览器打开了绚丽的互联网。        待到山花烂漫时，她在丛中笑。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-network-tcpdump-and-redis-client.html":{"url":"book/java-network-tcpdump-and-redis-client.html","title":"热风-TcpDump与Redis客户端","keywords":"","body":"JavaNetwork's TcpDump与Redis客户端        在处理网络问题时，我们可以使用抓包工具进行抓包，通过分析数据包来了解问题发生时的网络状况，从而定位和解决问题。Wireshark无疑是使用最广泛的抓包工具，它具有漂亮的GUI界面，丰富的功能菜单，在本机上可以随时开启和暂停它，但是如果环境切换到服务器上，可能就没那么顺利了，毕竟你难以在服务器上安装一个wireshark，更何况现在的服务端应用大都跑在容器里。        此时你需要一款小巧的工具，它可以按照你的要求进行抓包，比如：指定目标IP或者端口等，将抓包的内容输出到文件中，你甚至可以将文件拽回到本地，再使用wireshark打开分析，这款工具就是tcpdump。在一般的Linux发行版中，都会携带tcpdump，所以使用起来很容易。tcpdump是基于libpcap来工作的，后者是抓包库，它能按照你的要求将网卡上的包发给对应的程序，而在本文中，这个程序就是tcpdump。        tcpdump提供了一套形式化的描述逻辑，用户将需求翻译成这些描述逻辑后，tcpdump会理解这些逻辑，并使用libpcap将符合要求的包记录下来给到tcpdump，用户可以选择将这些包输出到控制台或者文件中。 使用tcpdump抓取一次curl        接下来使用tcpdump抓取一次HTTP请求，首先通过ping找到www.bing.com的IP是：202.89.233.100，然后在终端运行命令：tcpdump -i en0 host 202.89.233.100 and port 80 -nn，此时终端会等待输出。新开一个终端，运行命令：curl -v www.bing.com，这表示会对www.bing.com完成域名解析，同时再对相应的IP发起HTTP请求并输出响应。 -i 用来指定网卡，比如当前系统的网卡标识是en0 host 用来指定目标IP，也就是只有目标IP是host指定的数据包才会被拦截 port 用来指定目标端口 -nn 用来取消网络设备的名称解析，这样就直接展示IP，而不是主机名 and/or 与/或表达式，如果有多个条件，需要灵活使用and和or        请求并输出内容如下：        可以看到访问www.bing.com，服务端返回了301，将请求重定向到http://cn.bing.com。此时先前运行tcpdump的终端有了输出，如下所示：        看来tcpdump已经通过libpcap拦截到了我们期望的数据包，并将相关内容输出到控制台，可以看到过滤器收到了59个数据包，而符合要求的有11个。每个数据包占一行，其中第一列为时间，接着是来源IP与端口，以及目标IP与端口。Flags是TCP报文中的控制位，其中S代表SYN报文，.代表ACK报文，P代表PSH报文，F代表FIN报文。        前三个数据包是TCP建连过程，以第一个数据包为例：seq是3143996361，mss是1460字节。接着对端回复SYNACK报文，ack是3143996362，而mss是1440字节。最后四个数据包是TCP断开连接的过程，也就是我们熟知的四次挥手。中间的4个数据包就是HTTP消息通信的过程，可以看到主机发送了GET请求，而对端回复了HTTP状态码为301的响应。 导出cap文件并用wireshark打开        使用tcpdump可以将需要拦截的数据包输出到控制台上，同样也可以将其保存在文件中，并且使用wireshark打开。假设拦截网卡en0上所有的包，并将其输出到en0.cap文件中，可以执行命令：tcpdump -i en0 -w ~/Desktop/en0.cap，执行命令后，可以打卡浏览器访问几个页面，然后退出tcpdump程序，过程如下图：        可以看到通过tcpdump拦截了近9千个数据包，其实也就是访问了两个网站，不超过五个页面，接着可以使用wireshark打开en0.cap文件，可以看到界面是这样的。        如上图所示，本地网络存在许多不同类型的设备，这就使得网络中充满了各式各样的数据包，它们使用的协议有ARP，也有DNS，还有访问资源的TCP等等，显得非常嘈杂。这样我们就可以在服务器端使用tcpdump抓去数据包，将抓到的数据包保存到文件中，然后将文件拽到本地后使用wireshark打开分析。 两个redis客户端        Redis是目前后端最流行的缓存服务，由于它是一个开放协议的server，所以不同语言有不同的客户端。以Java的Redis为例，常见的就有Jedis和Lettuce，它们都可以操作Redis服务，性能孰强孰弱呢？这里我们不去比拼实现，而是通过抓包来看一下，在抓包之前，基于两个客户端，看一下测试代码。 使用jedis        Jedis客户端选择的版本是3.2.0，其I/O基于原生Java实现，使用方式参考其文档，单线程的set和get，代码如下： /** * Jedis cost:410630ms. */ @Test public void writeAndRead() { long start = System.currentTimeMillis(); IntStream.range(0, 10_000) .forEach(i -> { jedis.set(\"redis\" + i, String.valueOf(i)); jedis.get(\"redis\" + i); }); System.out.println(\"Jedis cost:\" + (System.currentTimeMillis() - start) + \"ms.\"); }        可以看到对于1万个KEY的设置和获取，耗时在410秒左右，因为Jedis客户端是线程不安全的，所以我们只能比拼单线程场景，如果要测试多线程场景，就需要使用Jedis连接池，相较于Lettuce而言，这其实对Jedis更不利。 使用Lettuce        Lettuce客户端选择的版本是6.3.1.RELEASE，其I/O基于Netty实现，使用方式参考其文档，单线程的set和get，代码如下： /** * Lettuce cost:349684ms. */ @Test public void writeAndRead() { long start = System.currentTimeMillis(); IntStream.range(0, 10_000) .forEach(i -> { syncCommands.set(\"REDIS\" + i, String.valueOf(i)); syncCommands.get(\"REDIS\" + i); }); System.out.println(\"Lettuce cost:\" + (System.currentTimeMillis() - start) + \"ms.\"); }        可以看到对于1万个KEY的设置和获取，比Jedis快了差不多1分钟。 tcpdump分析两个redis客户端        在执行测试前，运行命令：tcpdump -i en0 dst port 23877 or src port 23877 -nn -w ~/Desktop/jedis.cap，这个命令生成一个jedis.cap的抓包文件，同时捕获来自网卡en0的数据包，如果来源或目标端口都是23877时，符合捕获的条件。        分别抓取了jedis.cap和lettuce.cap两个文件，然后通过wireshark进行分析，在统计菜单中，吞吐量一栏，可以看到如下对比：        可以看到，lettuce要比jedis表现稳定且高效的多，直接原因是Netty要比Java原生socket库更为优秀，使得在处理拥塞控制时更加出色，进而获得了更好的吞吐量。lettuce每次拥塞发生时，都会很快的拉起吞吐量，而jedis面对拥塞，有很长的拥塞避免阶段，甚至出现了在200秒到300秒之间退化到慢启动阶段的情况。综上所述，在lettuce和redis之间的选择压根不用考虑，一定是前者，至于云厂商不负责任的宣传，那是因为云厂商不规范的部署拓扑导致的，和产品无关，只需要加上TCP的keep-alive即可解决主从切换引发的无法工作问题。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-dev-learn-c.html":{"url":"book/java-dev-learn-c.html","title":"热风-Java开发者看C","keywords":"","body":"Java开发者看C        一个Java开发者，还是需要了解和熟悉底层。        Java语言具备了完善的技术栈，同时不断的更新技术特性，有良好的类型系统以及设计，更重要的是它是一个具备RUNTIME（JVM）的编程语言。        C很多都没有，或者没有Java这么完备，但是Java的一个目标，在笔者看来就是帮助开发者安全的写出C++，因为它的RUNTIME就是由C++写成的。        CS专业的学生都学习过C，但是工作语言由大都不是C，所以在工作后，再次看看C，虽然可能很枯燥，但会别有一番心境吧！ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-dev-learn-c-01.html":{"url":"book/java-dev-learn-c-01.html","title":"热风-C语言概述","keywords":"","body":"C语言概述 C语言的历史        C诞生于上个世纪70年代初的贝尔实验室，由Ken Thompson和Dennis Ritchie设计开发出来的，目的是为了重写UNIX操作系统。        也可以说C语言是UNIX系统的“副产品”。        C语言不是凭空创作出来的，而是诞生于一个演化过程。        Ken在贝尔实验室在一台DEC PDP-7机器上，使用改进后的BCPL语言，也就是B语言，设计开发了初版的UNIX系统。        BCPL语言源自ALGOL60，也就是算法语言，它长得样子大概是这样： BEGIN FILE F(KIND=REMOTE); EBCDIC ARRAY E[0:11]; REPLACE E BY \"HELLO WORLD!\"; WRITE(F, *, E); END        可以看到函数WRITE的方法调用，;结尾的语句。当然它已经具备了IF选择，FOR循环等高级语言的特性了。        BCPL语言更近一步，它是Basic Combined Programming Language的缩写，可以看到它的样子： GET \"libhdr\" LET start() = VALOF { FOR i = 1 TO 5 DO writef(\"fact(%n) = %i4*n\", i, fact(i)) RESULTIS 0 } AND fact(n) = n=0 -> 1, n*fact(n-1)        BCPL有主函数start()，有获取外部库函数的GET指令，同时有格式化输出函数writef，看起来已经很有感觉了。        Ken所作业的DEC PDP-7不是一个很强的系统，只有4K的内存。        Ken发明的B语言。 /* The following function will print a non-negative number, n, to the base b, where 2        可以看到已经和C很像了，这个时候Dennis加入了项目组，同时他们更换了一台性能强劲的新机器DEC PDP-11。        由于B不能很好的利用新机器的特性，比如一些新的指令集，因此Dennis的主要工作就是给B打patch。可是patch打到最后导致B已经不再像原有的B了，Dennis就索性推翻了重干，操刀设计出了C，并用它和Ken一起重写了UNIX。 那个年代的程序员就像刀客：荒漠里一酒家，满屋皆是恶人。外来一人执刀，躬身不惧而入，刀光闪剑影舞。独他推门而出，收刀带帽上马。马踏夕阳西下，只留无人酒家。        任何软件的目的，都是为了驱动硬件。和硬件交互最深入的软件，就是操作系统。        在Java语言中，RUNTIME（JVM）针对不同的操作系统做了不同的实现，但是它们需要兑现一个目标，为上层代码（Java代码）提供一致的标准，也就是Java Spec。可以看到很多语言及功能的特性，诸如：网络、文件和线程等，都抽象到了Java Spec中，Java开发者可以在SDK中看到这些内容，而它们需要得到RUNTIME的支持与兑现。Java应用和程序都是基于Java Spec的，而整个Java实际上就是一个平台。        在C语言中，不同的操作系统提供各自对应的编译器，而C只是定义了C Spec，具体编译和链接就需要依靠不同操作系统上的编译器。这也是C编译器在不同操作系统（如：Windows或Linux）迥异的原因了。 C语言的优缺点        C语言的优缺点。        Java的语言特性比较多，并且在不断的增加，而C只能依靠标准库，如果没有标准库，那么移植性就存在问题。        同时C的编译器能力不强，有些隐藏错误难以发现。所以对于C需要注意。        针对下面的代码。 #include void main() { printf(\"hello, world!\\n\"); }        尝试使用gcc进行编译。 ch02 % gcc -o target/warn warn.c warn.c:3:1: warning: return type of 'main' is not 'int' [-Wmain-return-type] void main() { ^ warn.c:3:1: note: change return type to 'int' void main() { ^~~~ int 1 warning generated.        编译结果出现告警。然后使用新的命令再次编译。 ch02 % gcc -O -Wall -W -pedantic -std=c99 -o target/warn warn.c warn.c:3:1: error: 'main' must return 'int' void main() { ^~~~ int 1 error generated.        编译失败，只是因为增加了-std=c99，也就是说要求编译前按照c99的标准检查代码，而对于主函数返回类型，c99要求必须为int，所以直接报错。通过增加-Wall -W，也能够看到告警，这样会让我们知道哪里可能存在风险。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/java-dev-learn-c-02.html":{"url":"book/java-dev-learn-c-02.html","title":"热风-C语言的基本概念","keywords":"","body":"C语言的基本概念 编写一个简单的C程序        编写C程序，是一个从源码到可执行文件的过程，该过程需要编译器提供帮助。        通过编译器，将C的源代码转换成为二进制可执行文件。二进制可执行文件，顾名思义，二进制，也就是CPU（或者说当前计算机体系结构）能够识别的指令集合，可运行，该文件不仅是指令的集合，同时已经和当前计算机环境做了链接，它知道访问当前操作系统的相关能力的入口（或接口）在哪里。        这点同Java的区别是很大的，Java编译出了class文件，而这个文件可以在任何装有JVM虚拟机的机器上运行，但是注意，运行的方式是：java T。这里运行的其实不是编写的T.class，而是java这个程序，依靠java程序来解释运行的T.class。        通过编译器编译的C程序，是可以直接运行的，其实编译出来的产物等同于这台机器上的java程序。 假设存在一个编译好的T.class文件。        存在下面的C程序，我们需要对它进行编译。 #include #define X 10 int main(void) { int i = X; printf(\"i的值是：%d\\n\", i); return 0; }        在Mac上，C的编译器底层实际是LLVM，如果使用gcc，也是可以，只不过它是通过llvm-gcc桥接到了LLVM。        安装完XCode后，就具备了编译C程序的能力，当然不同的系统平台需要找到各自对应的编译器。 基于LLVM的clang的编译产出相比gcc更好，同时受到版权的限制也少，目前大部分软件，包括：Chrome都选择使用clang进行编译构建。        运行命令。 % cc -o test test.c        将test.c编译成可执行文件test，然后选择运行。 % ./test i的值是：10        可以看到，这个test程序和java一样，都是二进制可执行文件，但是把它拷贝到windows机器上，就不能双击运行了，因为它包含的指令不能够被windows机器识别，同时它是和当前机器做了链接，没有同windows机器做链接。        这里说了这么多指令和链接，那么编译一个C程序需要经过哪些步骤呢？        可以看到需要经历主要的三个步骤：预处理、编译和链接。如果使用过脚本语言（或者模板引擎）的，对于预处理肯定不会陌生，它基本就是对源文件做包含和转换等操作。        C程序进行预处理都是对指令进行操作，比如：#include或者#define指令，它们都以#开头，预处理器就关注这些内容。        预处理器对#include 指令的处理，就是在编译器中找到源文件中需要的头文件stdio.h，将其包含进来。        预处理器对#define X 10指令的处理，就是将源文件中X，替换为10。        接下来编译器会将处理过的源文件进行编译，生成二进制目标代码，最终通过链接器将系统文件同目标代码进行整合链接，完成本地化，使之能够运行。        Java就不是这个套路，Java编译器只是将源码编译为class文件，而并没有链接。这个过程有些类似将源码编译成为一种中间状态的文件，然后靠各个平台的程序（完成了本地化）来解释运行这个文件。        就像一个html文件，不同平台的浏览器来解释运行它一样，这个过程就需要类似JVM的程序托着这个文件。        可以看出来C程序的编译步骤要比Java这种托管的程序来的复杂，我们可以慢动作的看一下这个过程。        运行cc -E test.c > test.i，输出预处理器处理后的文件，可以看到该文件（部分内容）： # 1 \"test.c\" # 1 \"\" 1 # 1 \"\" 3 # 368 \"\" 3 # 1 \"\" 1 # 1 \"\" 2 # 1 \"test.c\" 2 # 1 \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/stdio.h\" 1 3 4 # 64 \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/stdio.h\" 3 4 # 1 \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/_stdio.h\" 1 3 4 # 68 \"/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/_stdio.h\" 3 4 typedef union { char __mbstate8[128]; long long _mbstateL; } __mbstate_t; typedef __mbstate_t __darwin_mbstate_t; typedef long int __darwin_ptrdiff_t; FILE *fopen(const char * restrict __filename, const char * restrict __mode) __asm(\"_\" \"fopen\" ); int fprintf(FILE * restrict, const char * restrict, ...) __attribute__((__format__ (__printf__, 2, 3))); int getc(FILE *); int getchar(void); char *gets(char *); void perror(const char *) __attribute__((__cold__)); int printf(const char * restrict, ...) __attribute__((__format__ (__printf__, 1, 2))); int main(void) { int i = 10; printf(\"i的值是：%d\\n\", i); return 0; }        在文件中通过#include指令包含的头文件，内容被包含进了该文件，同时#define定义的内容已经做了替换。 #define X 10，其中X已经替换成了10。        接下来运行cc -S test.i > test.s，将预处理器处理完成的文件作为输入，输出汇编文件。 .section __TEXT,__text,regular,pure_instructions .build_version macos, 11, 0 sdk_version 11, 3 .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq %rsp, %rbp .cfi_def_cfa_register %rbp subq $16, %rsp movl $0, -4(%rbp) movl $10, -8(%rbp) movl -8(%rbp), %esi leaq L_.str(%rip), %rdi movb $0, %al callq _printf xorl %eax, %eax addq $16, %rsp popq %rbp retq .cfi_endproc ## -- End function .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"i\\347\\232\\204\\345\\200\\274\\346\\230\\257\\357\\274\\232%d\\n\" .subsections_via_symbols        可以看到汇编指令描述的程序，这里不做展开。然后运行cc -c test.s > test.o，将汇编文件编译为目标二进制文件。 ^G^@^@^A^C^@^@^@^A^@^@^@^D^@^@^@^H^B^@^@^@ ^@^@^@^@^@^@^Y^@^@^@^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@(^B^@^@^@^@^@^@^@^@^@^@^@^@^@^G^@^@^@^G^@^@^@^D^@^@^@^@^@^@^@__text^@^@^@^@^@^@^@^@^@^@__TEXT^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@/^@^@^@^@^@^@^@(^B^@^@^D^@^@^@^B^@^@^B^@^@^@^@^D^@^@^@^@^@^@^@^@^@^@^@^@^@__cstring^@^@^@^@^@^@^@__TEXT^@^@^@^@ ^@^@^@^@^@^@/^@^@^@^@^@^@^@^Q^@^@^@^@^@^@^@W^B^@^@^@^@^@^@^@^@^@^@^@^@^@^@^B^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@__compact_unwind__LD^@^@^@^@^@^@^@^@^@^@^@^@@^@^@^@^@^@^@^@ ^@^@^@^@^@^@^@h^B^@^@^C^@^@^@^B^@ ^@^A^@^@^@^@^@^@^B^@^@^@^@^@^@^@^@^@^@^@^@__eh_frame^@^@^@^@^@^@__TEXT^@^@^@^@^@^@^@^@^@^@`^@^@^@^@^@^@^@@^@^@^@^@^@^@^@^B^@^@^C^@^@^@^@^@^@^@^@^@^@^@^K^@^@h^@^@^@^@^@^@^@^@^@^@^@^@2^@^@^@^X^@^@^@^A^@^@^@^@^@^K^@^@^C^K^@^@^@^@^@^B^@^@^@^X^@^@^@^B^@^@^B^@^@^@^@^C^@^@^P^@^@^@^K^@^@^@P^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^A^@^@^@^A^@^@^@^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@ ^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@UHH^PE^@^@^@^@E ^@^@^@uH=^O^@^@^@^@^@^@^@^@1H^P]i的值是：%d ^@^@^@^@^@^@^@^@^@/^@^@^@^@^@^@^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^T^@^@^@^@^@^@^@^AzR^@^Ax^P^A^P^L^G^H^A^@^@$^@^@^@^\\^@^@^@/^@^@^@^@^@^@^@^@A^N^P^BC^M^F^@^@^@^@^@^@^@#^@^@^@^A^@^@-^\\^@^@^@^B^@^@^U^@^@^@^@^A^@^@^F^A^@^@^@^O^A^@^@^@^@^@^@^@^@^@^@^G^@^@^@^A^@^@^@^@^@^@^@^@^@^@^@^@_main^@_printf^@^@ test.o (END)        这里面内容就是机器指令了，人类已经无法阅读，但是它还是不能执行，需要同当前系统环境进行链接。        运行cc test.o -o test，将目标二进制文件进行链接，生成可执行文件test。 % ./test i的值是：10 简单程序的一般形式        C程序实际就是一堆函数的集合，其程序代码可以看做有以下组成。        C程序就是编写函数，开发者写的是应用函数，而编译器提供的是本系统环境的系统函数，或者叫标准库，它们有能力同系统进行交互，处于程序调用的底层。        看一个C程序。        指令、函数和语句构成了C程序。 定义常量和变量        变量或者常量的命名同Java的规范一样，这些命名的变量和常量可以被称为标识符。        C程序是函数的集合，同时如果以符号来看，实际也是一堆记号（符号）的集合。        从形式上看，一个C程序就是定义一些标识符（变量、常量（或宏）和函数），中间使用语言的关键字来定义数据类型，依靠字面量以及运算符来进行运算，同时需要标点符号来分割不同的记号。由此可见，C程序是简单的，这种简单是由其语言本身的简单所带来的优点，它扩充能力的方式是通过调用不同的函数库，而不是增加语言的特性（包括语法特性等）和功能。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes.html":{"url":"book/basic-paxos-9-pages-notes.html","title":"热风-基础Paxos算法的9页笔记","keywords":"","body":"基础Paxos算法的9页笔记        Paxos算法是Leslie Lamport发明的一种基于消息传递，用于解决分布式环境下共识的算法。所谓分布式环境下的共识，是指在分布式多节点环境下，如何让不同的节点就某个值达成一致的算法（或者策略），而达到这一点，需要能够可靠的应对分布式环境中出现的各种不确定性和故障。        Lamport寄期望模拟一种民主议会制度来让分布式环境中的多个节点达成共识，因此假设了一个叫做Paxos的希腊城邦，而该城邦通过民主议会制度来解决问题，这个制度就是分布式共识的解法，而这个解法（或算法）就用该城邦的名字来命名，这也是Paxos的由来。 关于Paxos算法        本文是在阅读了Lamport的《Paxos made simple》、Wiki百科对于Paxos的介绍以及一些博主分享的基础上所形成的。文中包含了笔者思考的过程，Lamport对于算法的描述是偏数学的，所以需要自己不断的做延展和梳理，否则很容易被他有些不切实际的言语带偏，比如：议员认为收到了提案，等待最终的批准，但之前有人提出了设置的值，请查看，什么是最终的批准，之前的值该怎么判定，这些内容都需要读者自己理解，所以不同人理解的Paxos在主链路上会基本一致，但到了细节，就会不一样，这也是大家认为Paxos晦涩难懂的原因之一。        我们读懂一个算法，目的是用它解决问题，这需要工程化的思考。笔者认为Lamport的工程能力不会很强，而且他对其数学表达有一种优越感，可能因为他是数学专业的。相比较而言，他没有Doug Lea那种学术和实践两开花的水平，所以在阐述Paxos时，他没有讲明它的主要数据结构有哪些，状态有哪些，所以就会造成工程实现Paxos一定是具有二义性的，因为它会融合工程实践者自身的一些思考在其中。 Raft算法对于数据结构和算法的描述就会比较到位，不同人实现的会基本类似。Raft算法也是解决分布式共识问题的，它被开发出来的原因是为了让大家更好的理解Paxos算法。        那为什么还需要了解和掌握Paxos算法呢？Google Chubby的作者Mike Burrows说过，这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。        可以看出该算法对于分布式环境下的共识问题，是具有里程碑意义的。这好比我们定义了食物的功能是什么？当然食物的种类千千万，谁先说出最基本的东西，没有人会不承认它的重要性，因为它解决和阐明了大家都能看到的、最基本和最核心的问题。不是说这个问题有多难，而是人家就是很早阐述清楚了，而这是你的必由之路，你就得拜师，有点这个意思。因此，对于分布式共识是我们需要理解和掌握的，而Paxos算法是无法回避的。        Lamport认识到Basic Paxos的低效，在Basic Paxos基础上提出了Multi Paxos和Fast Paxos，这两种变体，用于减少算法中的消息传递次数，提升算法效率。本文不涉及这两种算法，只会集中在Basic Paxos。 基础Paxos算法笔记        本文阐述的内容基本和《Paxos made simple》一致，但会增加一些笔者的思考、示例和推导，是Paxos的学习笔记。        9页笔记，记录的比较随意，但是已经可以做到从概念到初步实践的阐述（我所理解的）Paxos算法，该算法工程化的效率不会很高，因为有太多的交互，但是容错性做到了极致。在每页笔记的介绍过程中，会深入去讨论一下，原来论文中的表述存在的一些不清楚的地方，同时会结合多方资料和自己的思考，给出一个自己的认识。        比如：Proposer选择了一个提案编号N，一个chose是多么的简单，但是如何选择的？是否会涉及到远程通信？还是在本地有一行记录？这些问题如果不深入思考，实际对于该算法的了解也就只能停留在纸面。因此，要理解该算法，就需要思考这些细节，猜测细节实施的方案以及背后的代价。        接下来，按照笔记的顺序，我们开始吧！很多内容我会不止讲一遍，你一定能够跟得上！ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-01.html":{"url":"book/basic-paxos-9-pages-notes-01.html","title":"热风-基础Paxos算法笔记(1/9)","keywords":"","body":"基础Paxos算法笔记(1/9)        第一页笔记主要说明共识和一致性的关系，以及Paxos算法的主要构成角色和角色的相关职能。        关键词：共识、一致性、分布式通信和Paxos算法角色。        Paxos算法是一种共识（Consensus）算法，而非一致性（Consistency）算法。        我们常见认知中，认为Paxos就是一个能够解决一致性的神奇东西，但是它并没有那么强大，它只是一个算法策略，没有一点工程化的能力，而一致性才是工程需要考虑的问题。可以认为：一致性是一个目标，是我们在分布式环境中追寻的一个目标，而要在分布式环境中达成这个目标，就需要（分布式环境中的）多个节点，它们能够形成共识，拟人的话就是：对某件事情有一致的看法。        这里的共识就是一种手段，因此利用共识可以达成一致性，它们的关系如下图：        在共识之上，实际可以做很多应用场景，共识是基础，也是一种工具。        如果要在分布式环境中需要形成共识，那就一定需要（分布式环境中的）各个节点能够进行通信，否则那就是玄学。分布式通信方式主要有两种，一种是共享内存，一种是消息传递，前者可以认为通过远程网络让各个计算节点共享一块内存，从而做到如同单个机器中的数据交换一样，只需要做好并发控制，就能实现多机互通，而后者使用消息的方式在多个节点之间传递信息，而这种异步，看似不可靠的方式实际是分布式通信常用的方式。        消息传递是异步的，也就代表了多节点之间不用过度考虑相互依赖，使得整体系统的体系结构变得简单且容易实施。消息由于在传输中会出现延迟、丢失和重投，这些由于网络不可靠的带来的问题其实不仅困扰着消息传递这种通讯方式，实际也会给对任何分布式通信带来麻烦，只是消息通信从一开始就要考虑这些问题。面向失败设计，以应对消息传递中的问题以及节点出现的故障。        共识可以简单的认为是分布式环境中各节点就某一个值达成一致。你没有看错，就是这么Low，论文里就是这么写的。但实际是这个值可能是一个字符串的值，也有可能是一条日志记录或者是一个文件。如果共识针对的值是一条日志，那么多个节点就对日志的顺序和内容产生共识，理解一致，如果将日志在不同节点进行回放，那么就算节点是一个复杂的系统，也不会导致出现不同状态的节点。如果这么说，就会觉得共识有点作用了吧？原本的论文没有提到应用，所以就做了一个值的比喻，因此会看的没有什么感觉。        Paxos是Lamport虚构的一个希腊城邦，它实行西方人最崇尚的民主议会制度，而这个民主议会制度就是Paxos共识算法，可见Lamport将该算法拟人化了，也从侧面证明了他对该算法非常喜爱。在算法中，每个分布式中的节点被城邦中的议员代替，普通的民众通过将自己的想法（也就是设置值的请求）提供给议员，由议员提交到议会进行讨论，议员们在议会中讨论这个提议，并形成大家认可的决议。        由于议员可以响应民众，也可以提起提议，更能够参与决议的表决，这么多职责很难形式化到算法中，所以Lamport认为议员实际上是有角色之分的。议员的角色包括：提议者、接受者和学习者，它们的名称和基本职能见下表： 角色 名称 职能描述 Proposer 提议者 接收请求，发起提议 Acceptor 接受者 讨论提议，形成决议 Learner 学习者 接受决议，发送响应        一个议员可以身兼多个角色，这代表一个节点可以干多种事情，从侧面上看不同角色的特定属性不会设置在议员身上，而是和角色相关联。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-02.html":{"url":"book/basic-paxos-9-pages-notes-02.html","title":"热风-基础Paxos算法笔记(2/9)","keywords":"","body":"基础Paxos算法笔记(2/9)        第二页笔记探讨了主从结构和Paxos算法角色的关系，以及算法中的提案信息结构和批准过程。        关键词：主从结构、提案、决议和批准过程。        Paxos算法的过程是由Proposer提出提案，Acceptor讨论提案形成决议，可见如果决议是一次被多节点承认的变更，那么该决议对应提案的提出则是数据变更的入口。        如果数据变更的入口是集中的，也就是一个人说了算，那么这个结构就是简单的，比如：主从模式（Master-Slave）。 主从模式是分布式环境中一种常见的节点拓扑关系，它常见于数据存储层，如：Mysql的主从模式，通过将写请求统一派发给唯一的主节点，同时将读请求离散派发到多个读节点，而数据变更首先流入到主节点，然后再异步复制到读节点，通过大量的读节点（在可以容忍极小延迟的情况下）来提升系统吞吐量。        主从模式通过将写收敛到一个节点来达成共识，如果用Paxos算法角色来描述主从模式，那么也就是Master = Proposer + Acceptor，数据变更发起以及生效，在一个节点。如果主从模式的写会发生在多个节点，主从模式将无法工作，因为无法判定各个节点中的变更，哪些是被多节点所认可的，会出现数据混乱。        有同学会问：单元化是不是可以解决主从结构这种只有单点可写的问题呢？        单元化是一种按照业务领域（比如：用户ID）分区的技术，它将部分业务数据的写控制在某一个单元（一批机器，一般在一个机房中），但数据的写也还会同步回其他单元，通过使用多单元的形式，让全量机器都参与工作，提升利用率（不用冷备模式）的同时，也提升了可用性（通过切换单元的方式），其概念如下图：        实际仔细看单元化，它也是主从模式的一个变体。因为它将可以预见的一种模式的写，比如：用户ID按10取模为5的写请求，派发到固定的机器上，通过添加了这层路由，使得写吞吐量变大，但每个单元内还是主从结构。这种技术方案只是在原有主从模式基础上，做了一个集群版，并没有真正的解决多节点写的问题。        数据变更请求 = 写请求 = 提案（Proposal）。        Paxos算法将一个写请求抽象为Proposal，如果将Proposal就视为请求的内容，也就是一个值，那多方一起写，就无法区分先后，这属于给自己找麻烦，毕竟写的发起是对方，但写的协议我们是可以控制的。因此，Proposal可以包含两个部分：编号和值，前者可以理解为一个全局唯一且顺序的编号，后者就是变更请求的内容。        当Proposal通过了多数Acceptor的表决同意后，Proposal就通过了，形成了多个节点认可的内容，我们可以称为决议（Resolution）。 决议来自于提案，但高于提案（被多方认可）。        Proposal批准的过程可以看笔记中的示例，多个Proposer可以提出Proposal，然后不同的Acceptor进行表决，如果某个Proposal得到多数Acceptor的支持，那么该Proposal就成为Resolution。        以P1提案为例，一个Proposer提出Proposal，P1到议会讨论，总共有三位Acceptor，它们接受了该Proposal，从而使Proposal成为Resolution。从这里可以看出，Proposer想要提案通过，它必须知晓有多少个Acceptor，也必须将Proposal发给多数的Acceptor寄期望它能够得以通过。 发往多数的Acceptor是论文中始终扭扭捏捏没有正面说明的，可以想象，如果你不知道这个约束，你就会认为Paxos算法是个神仙算法，随便一发，怎么就多数了。        对于提案编号没有重复，这可以理解，但提案产生的一刻会出现重复吗？也就是说不同的Proposer接收到不同客户端的并发请求后，会自然的获取到不同的提案编号，还是会获取到一样的提案编号呢？如果注意力集中在编号唯一的约束上，可能会认为获取到不同的提案编号，一定是Paxos算法要求一个单点的编号生成器，但你通读论文都不会找到它，因为实际是后者，你会看到相同的编号，只是在提案阶段，而最终成文的决议，它一定是唯一的。相同编号的提案一定会在多个Acceptor之间产生角逐和竞争，然后通过一个提案，另外一个，只能再来一遍提案批准流程。 但另外一个提案会看到前一提案形成决议的结果。        至此，我们对于提案批准过程有以下（没有在论文中提到的）隐含约束： 隐含约束1. Proposer知晓所有的Acceptor        这样它能够发起有意义的提案，也能判定自己的提案是否得到多数人的支持； 隐含约束2. Proposal的编号会产生重复        因为它的生成需要结合相应Proposer的状态信息，而批准过程会使之去重。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-03.html":{"url":"book/basic-paxos-9-pages-notes-03.html","title":"热风-基础Paxos算法笔记(3/9)","keywords":"","body":"基础Paxos算法笔记(3/9)        第三页笔记讲述了Acceptor接受提案的逻辑，以及相应的逻辑约束。        关键词：Acceptor、接受提案、提案和时间。        Proposer提出提案，Acceptor进行批准，该怎样批准呢？如果是节点之间进行相互协商，那么可以肯定，这是一个复杂的过程，最好Acceptor之间不要进行通信，而是按照一致的批准策略进行提案的审批。提案包含了编号和值，可以想象，一定是在编号上做文章，那么这个批准策略，可以是如果是更大（或更小）的提案，就一定会通过。        论文中没有明确描述批准策略的步骤，而是先给定了约束。        约束1. 一个Acceptor必须接受第一次收到的提案。        约束2. 一旦一个具有Value的提案被批准，那之后批准的提案必须具有Value。 这两个约束感觉不知所云，Lamport在自嗨，没有顾及到普通的读者，所以需要翻译一下。        Acceptor接收到一个提案，如果是第一次收到提案就会接受该提案，实际可以认为Acceptor能够识别出哪些提案它以前收到过，哪些提案它没有收到过。如果Acceptor能够进行识别，那它一定是有状态的，它至少记录了提交给它的所有提案，如下图所示：        Acceptor集群中，每个Acceptor都会记录自己收到的提案，如果只有一个Acceptor，在Paxos算法中，它一定会记录到所有的提案，但如果是多个Acceptor呢？那一定是各记录各自的提案，但是所有Acceptor的知识一定可以反映出所有的提案，只是在某一个Acceptor中，只有部分提案而已，如拼图一样，通过拼接，就可以得到完整的图画。        Lamport在约束2的基础上，做了增强。        约束2a. 一旦一个具有Value的提案被批准，那之后的Acceptor接收的提案必须具有Value。        约束2b. 一旦一个具有Value的提案被批准，那之后的Acceptor接收的提案必须具有Value。        约束2c. 如果Value V的N号提案提出，多数Acceptor要么没有接收N-1号提案，要么接收的最近一个提案（在N-1号之前）包含有Value V。        可以看到约束2a将约束推到了决议讨论阶段，而约束2b进一步反推到了提议阶段，也就是一旦满足约束2b，那就一定会满足约束2a，可以将约束2a称为约束2b的必要条件。        理解约束2及其变体，需要从时间角度去看，如下图：        可以看到，从一个Proposer角度去看，如果其提案被批准形成了决议，则之后再次发起提案时，以往的决议将能够被Proposer所见，且不仅限于该Proposer可见自己提出的提案（或决议），只要是决议，就算是其他Proposer提出的，也是能够可见的。        以数据库事务中事务隔离级别去思考这个问题也是可行的，比如：读已提交隔离级别，就和该约束有些含义上的共通点。至于约束2c，实在有些搞不太清楚想表达什么意思，先放在这里吧。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-04.html":{"url":"book/basic-paxos-9-pages-notes-04.html","title":"热风-基础Paxos算法笔记(4/9)","keywords":"","body":"基础Paxos算法笔记(4/9)        第四页笔记讲述了Proposer发起提案的约束，以及Paxos算法的两个阶段和相关流程。        关键词：Proposer、发起提案、准备阶段和批准阶段。        Proposer知晓所有Acceptor的存在，同时每次发起提案时，会将提案发往多数的Acceptor。可以看到Proposer会有Acceptor列表，同时该列表会不断的（随着Acceptor启停而）更新，同时在发起提案前，会从可用的Acceptor列表中找到一个多数的子集用于发送提案。        这里会涉及到Acceptor的注册发现诉求，如果动态新增一个Acceptor，那么所有Proposer的Acceptor列表中就会出现该元素，这样当然体感会好很多，但是这超出了本文讨论的范畴，可以认为每个Proposer静态的设置了一批Acceptor。        Lamport针对约束1做了增强。        约束1a. 当且仅当Acceptor没有回应过提案编号大于N的Prepare请求时，Acceptor才接受N号提案。        从前文中可以看到Acceptor拥有提案列表属性，因此可以在收到提案的Prepare请求后，检查当前提案列表中最大的提案，如果最大的提案编号小于收到提案编号，则回复Proposer已接受该提案。这里提到的Prepare请求，是在Paxos算法中的准备阶段进行的，算法将一次提案的批准过程分为两个阶段：准备阶段和批准阶段，论文中如下描述：        通过一个决议分为两个阶段： 准备阶段 Proposer选择一个提案编号n并将Prepare请求发送给Acceptor(s)中的一个多数派； Acceptor收到Prepare消息后，如果提案的编号大于它已经回复的所有Prepare消息(回复消息表示接受Accept)，则Acceptor将自己上次接受的提案回复给Proposer，并承诺不再回复小于n的提案； 批准阶段 当一个Proposer收到了多数Acceptor(s)对Prepare的回复后，就进入批准阶段。它要向回复Prepare请求的Acceptor(s)发送Accept请求，包括编号n和根据约束2c决定的value（如果根据约束2c没有已经接受的value，那么它可以自由决定value）。 在不违背自己向其他Proposer的承诺的前提下，Acceptor收到Accept请求后即批准这个请求。        这个过程在任何时候中断都可以保证正确性。例如：如果一个Proposer发现已经有其他Proposer(s)提出了编号更高的提案，则有必要中断这个过程。因此为了优化，在上述准备过程中，如果一个Acceptor发现存在一个更高编号的提案，则需要通知Proposer，提醒其中断这次提案。        两阶段描述完毕。        在具体分析论文描述两个阶段内容前，先厘清一个概念：提案并不是贯穿整个Paxos算法的唯一概念实体。这么说的原因在于，由客户端发起的请求，到Proposer发起的提案，到最终Acceptor同意批准以及传递给Learner知晓，其目的是为了满足客户端的请求能够被分布式环境所无歧义的理解（或者保存），这才是这个算法存在的意义。客户端并不能够发起提案，提案能够不断的更改，而提案一旦通过就无法更改，这些都不能够简简单单的依靠一个Proposal就描述能够清楚地，实际是需要结合Paxos算法中各个参与者来看这个问题，如下图：        可以看到从Citizen发起请求，到Learner获得到决议，通过提案批准的方式获得了在Acceptor(s)层面的共识，同时各方都已自己的角度进行概念传递，这点可以用网络分层传输数据的角度进行类比思考。        再回到Paxos算法两个阶段进行提案批准上来，通过准备阶段使得Proposer获得了申请资格，再通过批准阶段使Proposer能够设置期望的值。先看一下算法的准备阶段，如下图：        可以看到Proposer向多数Acceptor(s)发送了Prepare请求，这个请求包括了提案编号，比如：当前是编号N。如果接收到Prepare请求的Acceptor会根据自己的提案列表，看一下已有的最新提案编号M，假设N > M，Acceptor会接受该提案，并返回M号提案。如果M > N则可以看出Acceptor(s)已经在讨论更新的提案，当前提案没有必要再进行讨论了，此时会回复Proposer中止响应，意思是：不要再发送提案了，你这个已经过时了（，如果需要再发送，需要更新编号了）。        我们讨论了M != N的情况，那如果M = N呢？在回答这个问题之前，我们回忆一下隐含约束2。为什么不同的Proposer会发送相同提案编号的Prepare请求，原因在于算法没有描述Proposer选择提案编号的方式。        如果将Proposer内部维护一个编号指针，每次获取的时候就会递增，这无疑是一种解决办法，因为Proposer(s)获取编号的策略是稳定的。可是这种方式虽然简单的解决了编号生成方式，但是它冲突的几率会比较高，同时新加入的Proposer节点会很难适应。因此可以考虑另一种方式：Proposer在进行提案之前，首先会问询多数的Acceptor(s)它们各自最新的提案编号是多少，通过取Max(N1, N2, N3,...) + 1来得到提案编号。这个策略有些类似数据库的自增主键，通过获取多数Acceptor(s)的提案信息来决策一个合适的提案编号。        当M = N时，Acceptor需要回复Proposer已经接受的旧有提案信息，并能够告知产生冲突提案的Proposer(s)，这点有些复杂，我们在之后探讨。接下来就是算法的批准阶段，如下图：        当Proposer收到超过半数批准的Promise后，在Proposer这段的提案状态就会发生变化，由准备变为批准，此刻该Proposer已经具备了设置内容的资格。        Proposer会通过Accept请求，将值和提案信息发送给响应了批准Promise的Acceptor(s)。在Acceptor一端，会将值与提案转换为决议（Resolution），留存的同时会通知Learner，由Learner生成响应，回复客户端。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-05.html":{"url":"book/basic-paxos-9-pages-notes-05.html","title":"热风-基础Paxos算法笔记(5/9)","keywords":"","body":"基础Paxos算法笔记(5/9)        第五页笔记会结合Paxos算法的约束，再次梳理细化提案批准过程，设想Proposer和Acceptor的属性与状态。        关键词：Proposer、Acceptor、Learner、准备阶段和批准阶段。        准备阶段是Proposer发起Prepare请求给到Acceptor，请求的内容是提案（包括：提案编号和议题）。准备阶段的发起方式Proposer，但实际是Citizen，也可以理解为客户端。准备阶段的目的是为了获得设置值的资格，而其载体就是提案。以Proposer的视角来看一下，如下图：        Citizen需要设置值，将请求Request发送给Proposer。Proposer内部会具有一个提案列表，它包括了该Proposer提交的所有提案，也拥有将Request转换为Proposal的能力。        Proposer收到了Request，然后通过询问多数的Acceptor(s)，它们最新的提案编号是多少。Proposer再得到足够信息后，开始尝试将Request转换为Proposal，然后进入到提案的Prepare阶段。这里可以看出来，提案除了包含议题和编号，还会有Request的相关信息，比如：请求ID，Citizen的关键信息，以及当前Proposer的关键信息等，这样的Proposal才显得充分。        当Proposer接收到了（发起提案的）足够多表示同意的Promise后，Proposer会发起该提案的批准接受Accept请求，使得回复了同意Promise的Acceptor能够完成提案值的设置以及Resolution的生成。这里存在一个问题，两个阶段是在Proposer上还是在Proposal上？如果同一时刻一个Proposer只能提出一个提案的话，设置在Proposer上是没有问题的，但如果Proposer会同时创建多个提案，那就必须设置在Proposal上。由于Paxos算法是通过异步消息进行通信，并行处理是必然，所以阶段是设置在Proposal，也就是提案上，因此提案会有批准Acceptor的列表等信息，这些运行时信息如果能够体现出多数的Acceptor(s)同意，那么该提案就进入了批准阶段。        以Acceptor的视角来看，如下图：        Acceptor收到Proposer的Prepare请求，然后回复Promise。Acceptor有提案列表，也有决议列表，如果收到的提案编号能够在决议中找到，那就会回复忽略的Promise，因为这都已经成为决议了，没有必要再提了。提案以及回复的Promise如下表所示： 收到提案的情况 回复Promise 含义 提案编号如果能够在决议列表中找到 忽略 已经成为决议了，没有必要再提 提案编号如果大于提案列表中最新的提案编号 同意 没有收到过该提案，可以通过 提案编号如果小于提案列表中最新的提案编号 中止 已经收到过提案，该提案还没有进入批准阶段，但已经有Proposer提过，可以中止 提案编号如果等于提案列表中最新的提案编号 建议 提案编号一样，但多个Proposer(s)提出了，那回复发送请求的Proposer(s)一个建议，建议使用已经存在的提案        Proposer不断的收到Promise，如果发现其中某个提案中的支持（或同意）数量已经过了半数，那就代表该提案需要进入到批准阶段。进入批准阶段的提案，Proposer会发起Accept请求，而Acceptor一旦收到，就会按照要求将决议转换为决议，并将其发布通知给Learner。        可以看到在阶段控制上是通过Proposer来做到的，接下来看一下Learner，如下图：        Acceptor发布了Resolution，Learner收到后会记录下当前决议，同时进行广播给其他Learner(s)，最后将Resolution转换为Response，回复给Citizen。        这么长的链路，Learner怎么能够知道Citizen呢？因为Resolution包含了Proposal，而Proposal包含了Request，这样自然就能从某一个决议找到发起请求的Citizen了。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-06.html":{"url":"book/basic-paxos-9-pages-notes-06.html","title":"热风-基础Paxos算法笔记(6/9)","keywords":"","body":"基础Paxos算法笔记(6/9)        第六页笔记开始通过一个示例来演示Paxos算法，同时讲述一些细节。        关键词：Proposer、Acceptor、准备阶段和批准阶段。        在描述Paxos算法时，经常会看到这样的图：        这样的图有意义吗？当然有的，它以时序的形式来展示出算法中各个角色参与的时间以及所作出的行为，但对于时间线还是没有体现的很好。试想之前的一个场景，如果一个Acceptor收到了两个相同编号的提案，也许先收到的提案创建的时间还晚于后收到的提案创建时间，但是由于约束1的存在，实际会被接受的是这个创建时间较晚的提案。 约束1. 一个Acceptor必须接受第一次收到的提案。        这两个提案的Prepare请求对于这个Acceptor而言，不会在同一绝对时刻收到，但也可能会在一瞬收到，这就要求它能够很好的应对并发问题，更直白的说：Acceptor的提案列表需要是一个线程安全的列表。Acceptor的提案列表能够并行的处理接收到的Prepare请求，以提案编号为主键，线程安全（或者说读已提交）的处理提案的加入，不分创建时间的先后，一旦加入列表，则后续重复的提案将会被另行处理。 注意接收和接受，接收是被动的，接受是有逻辑且主动的。        回到时间这个维度上，对于准备阶段和批准阶段，以时间线的方式展示整个过程会更加的合适，如下图：        上图从Client发起请求开始，直到Client最后收到响应结束，共5个节点参与，其中有1个Proposer、5个Acceptor和2个Learner。在绝对时间轴下，各个参与者接收到请求，以及发送响应的时间点。        通过上图可以看到几个细节： (1) 具有Proposer和Acceptor角色的节点会接受自己的提案        如果一个节点同时具备了Proposer和Acceptor角色，该节点收到Client的请求后，将请求转换为提案，并发起Prepare请求。Prepare请求发起的同时，该节点首先会接受该提案，在进程内响应同意的Promise。        可以想象，如果该节点提出的提案，自己不接受，那就会产生混乱。现实中，某个议员会提出提案，同时他自然会接受自己的提案，如果自己提出的提案，自己都不接受，那就是蛇精病了。 (2) 两个阶段的分界线在于提案有了多数人支持        阶段是针对提案的，或者说提案的状态。当一个提案有超过多数Acceptor(s)支持时，它就从准备阶段到了批准阶段。        这里可以看到提案应该能够知晓有多少Acceptor(s)同意它，提案不可能知晓所有的Acceptor(s)，但是Proposer应该知晓。Proposer发现该提案已经有多数人支持了，那就代表阶段切换了。 (3) 两阶段转换的触发在Proposer接收达到多数票的最后一个Promise        Proposer不断的收到Acceptor回复的Promise，根据Promise可以清晰的看到该提案有多少Acceptor(s)支持。        当任意一个同意的Promise被Proposer收到，并发现该Promise使得该提案的同意过半，此时，该提案的状态会发生变化，从准备阶段进入到批准阶段。随后开始向所有同意的Acceptor发送Accept请求，进行值的设置。 (4) 提案进入批准阶段形成决议，而提案的Proposer还会回复Accept请求        提案支持人数过半后，提案会进入批准阶段，由于同意的Acceptor不回再次发起请求，所以当前Proposer会获取到支持该提案的所有Acceptor(s)，进行发送Accept请求。        虽然提案已经批准，但不意味着结束。每个收到Accept请求的Acceptor都会将提案转换为决议，且在收到请求的Acceptor本地进行，这也暗示多数的Acceptor(s)有对决议的共识。其他的Acceptor回复给Proposer同意的Promise时，Proposer会继续发送Accept请求，使得更多的Acceptor(s)能够进行决议的转化，达成共识。 (5) 响应Client（或Citizen）的Learner可能很多，Client需要做到幂等        当Acceptor收到Accept请求后，会在本地完成提案到决议的转化，同时会把决议保存到决议列表，当然这个数据结构也是线程安全的，同时会通知它知晓的一个Learner，目的是将决议发布给一个学习者，由它记录，由它返回。        可以想象，Learner(s)最终会保有所有的决议，但是如何让它们互通有无？这点我们在后面探讨，我们只需要知道现在Learner可能会受到发布的决议，而且不止来自于一个Acceptor。一旦Learner接收到发布的决议，会进行保存，如果保存成功，除了会通知它所知晓的Learner协助记录之外，还会将决议转换为响应，发送给Client。        在Client的视角看，一定会收到请求对应的多次响应，所幸这些响应对应的是一个概念上的决议。因此在第一次受到响应时，就算完成了整个流程，Client可以不用理会后续的响应。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-07.html":{"url":"book/basic-paxos-9-pages-notes-07.html","title":"热风-基础Paxos算法笔记(7/9)","keywords":"","body":"基础Paxos算法笔记(7/9)        第七页笔记开始通过另一个示例来演示Paxos算法是如何处理不同Proposer提出相同编号提案的问题，也是这个算法容错性的一种体现，同时会讲述一些细节。        关键词：Proposer、Acceptor、Proposal和两个阶段。        由于隐含约束2的存在，Acceptor收到的提案编号会产生重复，这点在第四页笔记中已经介绍了获取提案编号的方式。        提案编号可以由Proposer自己产生，但是考虑到分布式环境中一定存在不止一个Proposer，所以如果每个Proposer都自顾自的生成编号，那么产生冲突的概率就会很高。虽然是用一致的策略进行编号生成，但是如果没有通过协商就进行编号生成，会显得比较随意，所以在每次编号生成前，可以选择咨询多数的Acceptor(s)，由此来确定一个合适的编号。        假设在一个多Proposer的环境中，如果相同提案编号的Prepare请求发送到Acceptor会有什么效果呢？这里就要详细的讨论一下第四页笔记中遗留的M = N的问题了。        先要明确一个前提，每个Proposer都会给多数的Acceptor(s)发送Prepare请求，所以一定存在一个Acceptor集合，它们会接受到多于两次的Prepare请求，只是在绝对时间的一前一后而已。        考虑如下场景：存在2个Proposer和5个Acceptor，Proposer会将Prepare请求发往多数（示例中为3个）Acceptor(s)，这个交集就是A3。我们考虑A1的Prepare请求先到达A3，然后当先收到的提案已经被批准为提案时，如下图：        可以看到A3时间线上的蓝点是一个关键点，这个点之后，相同编号的提案，先前A3收到的（A1的提案），已经成为了决议。此时，如果收到了A5的Prepare请求，按照原有算法描述，可以直接忽略，不回复，但是出于效率，不应该这么做。        如果不进行回复，A1的提案已经成为决议，多数Acceptor(s)认可，但是A4和A5并不知晓，A5作为Proposer只会在那里傻等。要解决这种死锁，就需要Proposer具备一种自我超时机制，如果提案经过一段时间还没有收到足够的反馈，就查询一下多数Acceptor(s)的提案状况，如果发现已经在讨论更新的提案了，就废弃掉当前提案，然后重新发起提案。        默认的算法逻辑只能如此处理，但是如果假设允许A3返回一个Promise，其值是Ignore，则会使得该过程的活性得以提升，同时这部分逻辑不用依靠超时机制。        这是在决议形成之后，也就是已经收到了来自A1的Accept请求后，收到来自A5的Prepare请求。如果在A1的提案还没有变为决议前收到呢？如下图：        可以看到，A3依旧会采纳A1的提案，原因是约束1的存在。A3会回复A1和A5值为Suggest的Promise，建议双方使用已有的提案，也就是A1的提案。        对于Acceptor其实做法比较简单，如果存在了这个提案编号，则拿出已有的提案，同时通知当前提案的A5和已有提案者A1，通知两位相关的建议。如果A1和A5的两个Prepare请求很近，则需要Acceptor支持事务性，一定能够确保一个时刻，只能添加一个相同编号的提案。        对于Acceptor接收到Prepare请求，如果M = N的场景就应该这么多了，现在我们可以完整的描述一下这块逻辑了。假设：Acceptor存在proposalDAO和resolutionDAO用于访问它的提案列表和决议列表，Proposal.no表示提案编号，Proposal.proposer表示提案的Proposer，Resolution.proposalNO表示决议对应的提案编号。        Acceptor接收Prepare请求的步骤如下： Promise receiveProposal(Proposal p) { // 拿到最新的决议 Resolution localResolution = resolutionDAO.findLasted(); // 提案已经是决议了 if (localResolution.proposalNO >= p.no) { return Promise.of(Ignore); } Proposal localProposal = proposalDAO.findLasted(); if (localProposal.no > p.no) { // 本地提案新，需要提醒中止 return Promise.of(Abort); } else if (localProposal.no == p.no) { // 相同给出建议 return Promise.of(Suggest); } else { // 考虑并发控制，如果冲突，则重试 boolean result = proposalDAO.insert(p); if (result) { return Promise.of(Agree); } else { return receiveProposal(p); } } }        对于Acceptor的提案列表控制，如果出现插入失败，表明已经有相同编号的提案插入成功，则需要进行重试。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-08.html":{"url":"book/basic-paxos-9-pages-notes-08.html","title":"热风-基础Paxos算法笔记(8/9)","keywords":"","body":"基础Paxos算法笔记(8/9)        第八页笔记讨论决议的发布过程，以及决议在不同Learner中如何传播的问题，同时也会讨论Paxos算法如何避免死锁的问题，该方案在Raft中也会看到，是一种保障算法过程的常用方案。        关键词：Acceptor、Learner、广播和随机睡眠。        当Acceptor接收到Accept请求后，就会将（当前节点中的）提案转换为决议，同时会将决议通知到Learner。        论文中对于Learner的描述有限，基本都集中在Proposer和Acceptor上，但是不是说Learner不重要，我们需要看一下Learner的职责有哪些？第一，存储决议；第二，响应请求。前者是Learner的本职工作，其实这点（如果看了前面的文章就能理解到）Acceptor实际也可以做到，因为Acceptor才是策源地，但是可以理解Learner为决议的备份，有它存在，整个系统的可用性会有保证。后者是Learner的核心工作，这个工作是参与到Paxos算法过程中的，由Learner来生成响应，并将响应发回给Client。        为什么不是Acceptor来做这个工作？其实是可以的，但是如果这样来定义Acceptor，就会让它变得很重。因此，一个能够尝试对Client建链接发响应，同时存储备份一下决议，是比较符合单一职责的。这么看来Learner实际承担了Paxos算法末段关键流程了，但是论文掐头去尾只着重在中间一段（Proposer和Acceptor），对这部分内容会描写的比较简单，但涉及到问题却不能那么简单的带过，需要详细看一下。        主要的问题在于：如何确保决议在Learner中形成共识？        Learner(s)也是一个集群，每个Acceptor都会发布相同决议的通知到Learner，如何确保这些Learner(s)不漏掉，同时这个过程是高效的。这个又是一个共识问题，如果再机械的按照旧有模式解有些不现实，所以需要一个高效的方案。论文中提到了Acceptor将通知发往Learner集群中的一个子集，再由子集来通知全量的Learner，这句话很简单，但让作者自己去实现，估计他也会崩溃，毕竟节点是计算机，不是人，它需要一个具体且无歧义的方案。        Acceptor能够通知Learner，这代表Acceptor知晓Learner，可以假设每个Acceptor都具有一个Learner集合。这里选择一个集合，目的是增强通知的可靠性。当决议通知抵达Learner后，其他的Learner要么被通知，要么主动的来获取，这个过程如下图：        红色和黑色线条的区别就是选择拉或者推。两种模式都能够解决数据同步的问题，但是在解决这些问题之前，我们先要看一下Acceptor通知Learner的问题。\b这是Learner收到变化的入口，每个Acceptor都会尝试通知一个Learner集合，这样会确保通知能够下达。在这个集合中，每个节点对于新增的决议都应该有一个业务无关的主键，该主键在节点内是严格自增的，这会在同步数据时发挥作用。        如果选用推的模式，集合中的每个Learner节点都会将新增决议的通知发往其他的Learner，来自N个Acceptor的通知最终会演变成为一场风暴，这么做是有些低效的。虽然推比拉的时效性更好，但是效率会低很多，我们只需要集合中的Learner能够实时的通知Client就好，而数据同步的工作，交给其他的Learner。        如果数据同步的工作由其他的Learner来做，这就是典型的拉模式。每个Learner都应该知晓其他Learner的存在，同时会维护同步其他Learner数据的主键（或游标），这样一个Learner就有N-1个游标。每隔一段时间，会问询其他N-1个Learner，如果发现对应的游标有偏差，就获取对方的增量，完成合并与同步，同时需要更新游标，这个过程是需要幂等的。        现在可以梳理一下一个Learner的工作：如果接受到来自Acceptor的通知，将决议存储到本地（并更新主键）同时根据决议中的请求，生成响应回复Client。处理消息的同时，还需要定时的轮询其他的Learner，维护其他同步的游标记录，如果发现数据有变化，则发起同步，并更新游标。        可以看到Learner集合的概念是从Acceptor角度来看的，每个Learner的工作是相同的。集合存在的目的，除了为有效和可靠的保存决议以外，还是为了快速的通知Client，完成一次Paxos算法过程。        接下来我们掉转头，看一下Proposer生成提案编号的问题，通过前文中的描述，编号是通过获取各个Acceptor的状态数据来得到的，如下图：        上述策略存在一个死锁的问题，如果多个（超过3个）Proposer同时进行问询，很可能会得到相同的编号，然后各自将Prepare请求发往Acceptor后，得到的Promise不会是Agree，只能重新走一遍流程，结果很有可能再次撞车。        解决的方式比较简单，可以在Acceptor返回之后，随机睡眠一段时间，然后再询问各个Acceptor，这样就可以错开。当然，如果要通用化的解决，可以选择在上图红色X部分引入一个随机睡眠，也就是在Client并行请求到达Proposer后，会经过各个节点（各自）随机的睡眠，然后再进行处理。通过引入这种整流的形式，降低冲突和死锁的几率。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/basic-paxos-9-pages-notes-09.html":{"url":"book/basic-paxos-9-pages-notes-09.html","title":"热风-基础Paxos算法笔记(9/9)","keywords":"","body":"基础Paxos算法笔记(9/9)        第九页笔记讨论Paxos算法中的主要数据结构和关键逻辑行为，是该算法进行工程化实践的思考。        关键词：数据结构、逻辑和状态。        在前8节的笔记中，我们讲述了异步消息是该算法的沟通方式，从Client如何发起Request开始，到Proposer收到Request将其转换为Proposal，然后将提案发给多数的Acceptor。我们详细讨论了Acceptor收到Prepare请求后的处理逻辑，并将生成的决议发给Learner的过程。在Learner收到决议后，如何能够高效的在Learner之间完成共享，并将Response发回给Client，从而完成一次Paxos算法。        通过前面的讲述，我们应该对Paxos算法有了一个大致的了解，在开始本节的内容之前，笔者会再次罗列一下关键的功能点，这样也便于后续对该算法数据结构的讨论。以算法角色（或参与者）为视角，其功能如下表（其中顺序一栏表示该功能在算法中的使用次序，升序排列）： 角色 功能 描述 顺序 Client 发起请求 生成请求ID，包装请求，选择一个Proposer发送请求 1 Client 接受响应 收到响应，将响应中包含的请求ID提取出来，完成请求和响应的映射 13 Proposer 接收请求 收到请求，解析请求内容，随机睡眠一段时间，准备发起Prepare请求 2 Proposer 发起提案 生成提案，访问多数的Acceptor获得最新的编号，设置编号，发送Prepare请求给多数Acceptor 3 Proposer 接收承诺 收到Acceptor回复的Promise，更新提案与Acceptor的关系，包括支持数量，如果是非Agree的Promise，需要进行重试 6 Proposer 接受提案 当多数Agree的Promise收到后，向回复Agree的Acceptor发起Accept请求 7 Acceptor 接收提案 收到Prepare请求，提取出提案，进行相应处理，详细内容可以参见第7节 4 Acceptor 回应承诺 生成Promise，回复Proposer 5 Acceptor 决议发布 将决议发布到Learner集合 9 Acceptor 接受提案 接受到Proposer发送的Accept请求，生成决议并保存 8 Learner 回复响应 根据决议，构建响应，回复Client 11 Learner 接收决议 收到Acceptor发送来的决议，生成本地记录，进行处理 10 Learner 同步决议 定时查询其他Learner，获取对方的新增记录，完成增量同步 12        接下来需要看一下算法中的主要领域对象，在Paxos算法中，参与者包括：Client、Proposer、Acceptor和Learner。其中Client是入口，也可以作为使用算法的SDK，部署在使用方的应用中。        Client会具有Proposer列表，在每次发起请求时，会随机选用一个Proposer发起请求。Proposer会具有Acceptor列表，每次在Prepare请求发起前会将提案发往这个列表中的多数（或者是发往多数后，停止发送）。Acceptor会具有Learner列表，在发布决议时，选择其中一个集合进行通知。Learner会根据请求中的地址发起对Client的建链，只需要确保网络可达即可。        上述关系如下图所示：        在Client一端，具有一个请求ID到Request的Map结构，用来在请求时将数据放置其中，当Learner发回响应时，取出其中的Request来进行应答处理。每个Client都会具有Proposer列表。        Proposer具有提案列表，在发送Prepare请求前，需要先保存在本地。Acceptor具有提案和决议列表，用来接收Prepare请求，以及在Accept请求处理时，将提案转为决议并存储。        Learner除了知晓其他Learner的地址，还需要保有同步其他Learner的进度记录，只有这样才能在后续的定时同步时做到有的放矢。        除了上述参与者，算法中出现最多的就是提案、决议和请求等数据了，它们之间的关系，以及各自所具有的属性，如下图：        从请求到最终的响应，环环相扣，最终的响应能够知晓它的决议是谁？之前的提案是什么？谁发起的？只有这样，一个响应发回才能对应的上请求，同时能够给到Client以详实的信息。        请求中的Client关键信息，包括了Client的IP等关键信息，目的是能够在之后Learner回复响应时，找到发起的Client，也只有将响应回复到该节点，才有意义。如果回复到其他Client将没有任何效果，这是为什么呢？读者可以自行思考。 参考Client的数据结构。        提案中的与Acceptor的关系，来自于Acceptor回复的Promise，每个Promise都会形成一条关系。决议中的新增日志，就是为了Learner之间同步数据而准备的，每条日志都会有（自增）主键，便于其他Learner掌握同步进度。        Basic Paxos算法到这里就基本描述完了，它说起来复杂，但本质也很简单。它利用了多数原则，通过一定的冗余和决策，解决了分布式环境中的共识问题，这个思想在分布式系统中具有广泛的运用场景。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-brief-summary.html":{"url":"book/distribute-lock-brief-summary.html","title":"热风-分布式锁","keywords":"","body":"分布式锁        在微服务环境中，除了会遇到分布式事务问题，还会有并发控制的需求。\b在分布式环境下完成并发控制，如果只使用普通的JUC锁是无法完成工作的，因此只能依靠分布式锁。JUC中的锁主要是解决在一个进程内的访问控制问题，而分布式锁解决的是进程间的，如果将JUC中内存操作的指令更换为网络调用是不是就可以实现分布式锁了呢？理论上是的，但是一旦和网络沾边，考虑到网络的不可靠性，就没这么简单了。        笔者通过一系列的文章来讨论分布式锁，寄期望为这个话题画上休止符。在正式开始之前会通过锁是什么？介绍一下锁的主要特性，特别是可见性，这个容易被忽略的特性。        分布式锁和锁有什么异同之处呢？分布式锁是什么？会简要的带着读者走一遍。        开发一个分布式锁不难，但是会有很多问题需要解决，当看到这些问题的时候，你还会觉得简单吗？然后笔者设计开发了一个分布式锁框架，有文章会介绍它的设计与使用方式。        基于该框架，将Redis和ZooKeeper可以快速的整合入框架，并且该框架具备良好的扩展能力。        Redis是最常用来做分布式锁的底层实现了，但是围绕它做分布式锁的问题，Salvatore Sanfilippo和Martin Kleppmann，一个是Redis的作者，一个是分布式专家，两个人吵了一架，他们撕逼的文章笔者做了翻译和注释，有兴趣的可以看一下。 使用Redis实现分布式锁，第一篇，Salvatore教大家如何使用使用Redis做分布式锁。 如何实现分布式锁，第二篇，Martin怼Salvatore的文章，表示你的姿势不对。 Redlock能保证锁的正确性吗？，第三篇，Salvatore回怼的文章，表示自己没有问题。        可以看到，分布式锁实现起来不是那么容易，连大神们都会吵起来。不过，我是支持Martin Kleppmann的，你呢？理想是美好的，但现实总不会完美，因此需要结合自己的场景来实现和使用分布式锁。        最后，需要问自己：我真的需要使用分布式锁来解决这个问题吗？ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-what-is-lock.html":{"url":"book/distribute-lock-what-is-lock.html","title":"热风-锁是什么？","keywords":"","body":"锁是什么？        锁是一种提供了排他性和可见性的并发访问控制工具，它保证了在同一时刻，只能有一个访问者可以访问到锁保护的资源，这个资源可以是一段方法逻辑或是某个存储。锁是我们常用的一种并发访问控制工具，而排他性是它功能性最直接的体现，以至于它成了排他性的代名词，但其所隐含的可见性以及资源状态往往被我们忽视。        可见性表达的是一个访问者对数据（比如：内存中的某个值）做了修改，其他的访问者能够立即发现数据的变化，从而能够做出相应的行为。这点看起来挺简单，甚至感觉有些理所应当，但如果带入到现代计算机体系结构下，就觉得没这么容易了。        CPU和内存之间是有缓存的，这个缓存一般封装在CPU上，每个CPU核心同缓存进行沟通，当缓存中没有数据时，才会将内存中的数据载入到缓存中，然后进行处理。缓存封装在CPU内，其目的是离CPU核心更近，CPU对缓存的访问时间一般在1纳秒左右，而对内存的访问时间则在100纳秒左右。这个过程如下图所示：        可以看到，与内存访问相比，缓存更加迅捷，但问题却随之而来，数据会同时出现在缓存和内存中，这使得该架构天生就存在可见性问题。比如：一个值在内存中是A，由于程序是多线程执行，导致在某个CPU缓存中的值是旧值B，这时就出现可见性问题了。        如果要解决这个问题，就需要程序能够在访问数据之前先作废掉CPU上的缓存，从内存中加载最新的数据使用，同样也需要在数据变更后，将数据显式的刷回内存。锁就具有这个特性，锁除了能够完成排他性工作，它还能隐性的解决可见性问题。当使用锁的时候，程序会执行系统指令，将CPU中的缓存作废，然后载入内存中的最新数据，现在通过一个示例来看一下，示例代码如下所示： public class NoVisibilityTest { // 准备状态 private static boolean ready; // 数量 private static int number; private static class ReaderThread extends Thread { public void run() { while (!ready) { } System.out.println(number); } } public static void main(String[] args) throws Exception { // 获取Java线程管理MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); IntStream.range(0, 15) .forEach( i -> { ReaderThread readerThread = new ReaderThread(); readerThread.setName(\"Reader:\" + i); readerThread.start(); }); number = 42; ready = true; for (int i = 0; i        上述示例定义了两个静态变量，分别是状态ready和值number，然后启动15个线程进行执行操作，操作很简单，如果发现ready为true，则退出循环，打印并结束。        如果不存在可见性问题，ReaderThread线程应该可以发现ready值被修改，然后跳出死循环，打印并退出。实际情况会是这样吗？运行程序后，主线程会每隔1秒打印一次线程信息，重复20次。这里截取最后几个批次，如下图：        从第20次打印的线程信息可以看出，之前创建的ReaderThread大部分都存活着，难道它们都对ready值的变化视而不见吗？其实它们不是看不见，而是它们只盯着缓存中的ready值去看，对内存中的ready值变化不清楚罢了。线程运行在CPU核心上，在线程执行时，将值从内存载入到缓存中，依照缓存中的值来运行。        那怎样才能让执行的线程从内存中获取最新的数据呢？有一种简单的做法是使用volatile关键字来修饰ready变量。当然更简单的就是什么都不做，等执行的线程被操作系统交换出去后，然后当线程下一次被调度执行时，会从内存中获取数据并恢复缓存，这时该线程有几率能够恢复过来。除此之外，还有别的方式吗？有的，使用锁。因为锁的资源状态在内存中，所以需要保证访问锁的线程能够正确看到锁背后的资源，而这个保证就是对可见性的承诺。基于这个特性，我们使用一个无意义的锁来获得可见性，这里所谓的无意义，是指没有发挥出锁的排他性能力。        只需要对原有示例做出一些修改，如下所示： private static class ReaderThread extends Thread { public void run() { while (!ready) { Lock lock = new ReentrantLock(); lock.lock(); try { } finally { lock.unlock(); } } System.out.println(number); } }        可以看到，上述修改只是在ReaderThread的死循环中，创建了一个ReentrantLock，并在这个锁上调用lock方法。如果从功能角度上看这个修改，是一点作用也没有的，但重新运行修改后的程序，会看到结果，如下图所示：        ReaderThread线程读到了内存中ready的新值，它们安全的退出了。这就是锁可见性的体现，它保证在锁保护的代码块中，能够看到最新的值，不论是锁的资源状态，还是程序中的数据变量。在使用锁时，会将CPU上的缓存作废，以期望获取到内存中的值，而作废的数据不止是资源状态，因此变相的使得线程获取到了最新的ready值。        可以看到，锁是依靠可见性的保障来看清楚锁的资源状态，并在此基础上封装出能够提供排他性语义的并发控制装置。在使用锁的功能时，也会间接的享受到它对可见性的保证。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-what-is-distribute-lock.html":{"url":"book/distribute-lock-what-is-distribute-lock.html","title":"热风-分布式锁是什么？","keywords":"","body":"分布式锁是什么？        分布式锁，顾名思义，就是在分布式环境下使用的锁，它能够提供进程间（当然也包括进程内）的并发控制能力。在分布式环境中，分布式锁可以保证在同一时刻只有一个实例（或节点）能够进行工作，这个工作一般称为同步逻辑，它可能是一组计算或是对存储（以及外部API）的一些操作。 分布式锁        就功能而言，分布式锁是可以替代一般（类似JUC这种）单机锁的。JUC单机锁对于锁是否可以获取，是依靠内存中的状态值来判定的，而这个状态值称为锁的资源状态。分布式锁和单机锁一样，是依靠可见性的保障来看清楚锁的资源状态，并在此基础上封装出能够提供排他性语义的并发控制装置。二者区别在于单机锁的资源状态存储在进程内，而分布式锁是在进程外，一般是某种网络存储服务。        访问资源状态的最小单位是线程，因此分布式锁控制的粒度同单机锁一样，在多线程程序中，每一时刻只有一个实例中的一个线程能够获取到分布式锁。JUC单机锁的资源状态在实例内部，如果将其移动到进程外，对资源进行获取的系统调用换成网络调用，单机锁不就转变成了分布式锁吗？没错，确实如此，但由于链路的变化会引入一些问题。        资源状态由内到外的移动不会产生任何变化，而本地调用换成网络调用以及存储服务的加入会产生巨大的变化，主要体现在四个方面：性能、正确性、可用性以及成本问题，在实现分布式锁会遇到的问题会详细探讨它们。 使用分布式锁的原因        分布式锁是一项比较实用的技术，使用它一般来说会有两个原因：提升效率或确保正确。如果需要在工作中使用到分布式锁，可以尝试问自己一个问题：如果没有分布式锁，会出现什么问题呢？如果是数据错乱，那就是为了确保正确，如果是避免集群中所有节点重复工作，那就是为了提升效率。        找到使用分布式锁的原因是很重要的，因为它会让你对锁失败后产生的问题有了明确认识，同时对使用何种技术实现的分布式锁在选型上会考虑的更周全。        比如说：如果使用分布式锁是为了解决效率问题，那么由于偶尔锁失败造成的重复计算问题应该是可以容忍的，这时选择一个成本低的分布式锁实现会是一个好选择。如果使用分布式锁是为了解决正确性问题，而且这个问题非常关键，那就需要尽可能保证分布式锁的可用性以及正确性，这时采用高成本的分布式锁实现就变得很必要了。 分布式锁的分类        分布式锁的实现有许多种，但可以分为两类，即拉模式和推模式分布式锁，区分二者的依据在于在等待获取锁的实例（或线程）被唤醒的方式。如果需要实例自己轮询获取远端的资源状态来获取锁，这种模式就是拉模式，典型的实现是基于Redis的分布式锁。如果实例是依靠外部通知来触发它去获取锁，这就是推模式，典型的代表是基于ZooKeeper的分布式锁。        那JUC中的单机锁属于那种模式呢？回忆一下AbstractQueuedSynchronizer（以下简称为：AQS）是如何唤醒等待者的，当拥有锁的线程释放锁时，会触发AQS的release()方法，该方法会唤醒同步队列中等待的头节点，使之能够尝试去获取锁。唤醒等待锁的节点，就相当于事件通知该节点，因此，JUC中的单机锁可以被认为是推模式。        拉模式的分布式锁，需要等待获取锁的实例主动以自旋的方式去查看资源状态是否有变化，并根据变化来决定是否可以去获取锁。推模式的分布式锁，资源状态的变化会以事件的形式通知到所有等待锁的实例，当实例收到通知后，就可以去获取锁，而这个通知就相当于（JUC锁的）唤醒动作。        相比较而言，推模式具备事件驱动的特点，响应会及时一些，而拉模式则需要不断查看资源状态的变化，存在一定的无效请求。两种模式对于存储服务的诉求也是不一样的，推模式对于存储服务要求会比拉模式复杂一些。从直觉上看，推模式仿佛优于拉模式，但在实际环境中，需要综合考虑分布式锁存储服务的功能、可用性与成本等问题，并不是一个简单的选择。 以Redis和ZooKeeper分别实现的分布式锁为例：前者吞吐量高，访问延时小，可用性一般，但实施成本低；后者吞吐量低，访问延时较高，可用性高，但实施成本高。二者将会在后续的文章中进行详细探讨。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-problem.html":{"url":"book/distribute-lock-problem.html","title":"热风-实现分布式锁会遇到的问题","keywords":"","body":"实现分布式锁会遇到的问题        分布式锁与（类似JUC的）单机锁区别在于：资源状态由进程内转移到进程外，访问资源状态的方式由本地调用转换成为网络调用，这些变化会带来了问题（与挑战），主要体现在4个方面：性能、正确性、可用性以及成本。        我们先考虑一种最简单的分布式锁实现方案，它依赖一个关系数据库（比如：MySQL，以下简称为：数据库）来维护资源状态，可以称为数据库分布式锁。数据库具备事务特性，因此能够支持原子化的新增和删除，同时依靠唯一约束和Where条件，使之可以成为一个良好的分布式锁存储服务。        在数据库中，创建一张lock表，它的主要字段以及运作过程如下图所示：        可以看到图中实例A和B在尝试获取一个名为order_lock的锁，获取锁的方式就是在lock表中成功新增一行当前锁与实例对应的记录。lock表包含了两个关键字段，一个是代表锁资源的lock_name，在该字段上建有唯一约束，另一个是代表获取到锁的客户端（或实例）client。不同的分布式锁会有不同的名称，而这个名称就是锁资源，它保存在lock_name中，一旦客户端获取到了某个名称的分布式锁，它的信息会被保存在client中，这表明了这把锁的所属，可以选择保存客户端的IP。 client的值如果要确保严谨，可以选择IP+进程ID+线程ID，这里简单起见，还是选择IP。        如果客户端需要获取锁，就必须在表中成功增加一行记录，如果增加记录成功，则表示该客户端成功的获取到了锁，如果由于主键冲突错误而导致增加失败，则需要不断自旋重试。lock表在lock_name一列上存在唯一约束，所以同一时刻只会有一个实例（图中为：实例A）能够完成新增记录，从而获取到order_lock锁。        当客户端完成操作，需要释放锁时，需要客户端根据锁名称和IP删除之前创建的记录。lock表中client字段，它记录了当前客户端的IP，目的是在删除时，通过SQL语句DELETE FROM LOCK WHERE LOCK_NAME = ? AND CLIENT=?来保证只有拥有锁的实例A才能释放锁，防止其他实例误释放锁。        这个简单的分布式锁看起来（实际上也）是可以完成分布式环境下并发控制工作，接下来介绍实现分布式锁遇到的问题，并基于这些问题，再次审视这个锁。 性能问题        使用分布式锁时，会给系统带来一定的性能损失。不论是单机锁还是分布式锁，在实现锁时，都需要获取锁的资源状态，然后进行比对。如果是单机锁，只需要读取内存中的值，而分布式锁则需要网络上的一来一回获取远端的值。分布式锁的性能理论上弱于单机锁，而不同分布式锁性能与其存储服务性能以及网络协议有关。        使用网络来获取资源状态对性能有多大影响呢？这里通过访问不同设备的延迟数据来直观的感受一下，如下图所示：        上图参考自Jeff Dean发表的 《Numbers Everyone Should Know》 2020版。通过观察该图可以发现，访问CPU的缓存是纳秒级别，访问内存在百纳秒级别，而在一个数据中心内往返一次在百微秒（或毫秒）级别，一旦跨数据中心将会到达到（甚至超过）百毫秒级别。可以看到单机锁能提供纳秒级别的延迟，而分布式锁的延迟会在毫秒级别，二者存在上千倍差距。 分布式锁的存储服务一般会与应用部署在同一个数据中心。        如果网络变得更快会不会提升分布式锁的性能呢？答案是肯定，但也是存在极限的。从1990年开始到2020年，30年的时间里，计算机访问不同设备的速度有了巨大提升。访问网络的延迟虽然有了很大改善，但是趋势在逐渐变缓，也就是说硬件与工艺提升带来的红利变得很微薄，提升不足以引起质变。 历年访问不同设备的延迟数据，可以参考这里        由于分布式锁在访问存储服务上比单机锁有显著的延迟，所以就锁的性能而言，肯定是低于单机锁的。当然会有同学提出，单机锁不是解决不了分布式环境下的并发控制问题吗？没错，这里就访问延迟来比较二者的性能有所偏颇，但需要读者明白，分布式锁的引入并不是系统高性能的保证，不见得分布式会比单机更加有效率，使用分布式锁就需要接受它带来的延迟，因为它的目的是为分布式环境中水平伸缩的应用服务提供并发控制能力，保障逻辑执行的正确性。        不同的分布式锁实现会依赖不同的存储服务，比如：Redis或ZooKeeper，它们之间的性能也是存在差异的，主要体现在传输协议大小、I/O链路是否非阻塞和功能实现上。传输协议是指客户端与存储服务通信时有格式的二进制数据，一般来说相同类型操作协议体积越小，性能越好。I/O链路是指客户端与存储服务通信的模式，一般非阻塞I/O会优于同步I/O，在支持客户端数量上，前者有显著优势。功能实现主要是存储服务自身实现的复杂度，复杂度越高，性能就会越低，一般来说，越是通用的存储实现，其复杂度会更高。        对于数据库分布式锁而言，客户端与存储服务（也就是关系数据库）通信时传输的是各数据库提供商的专属协议，但由于包含了SQL，所以就协议体积而言，数据库分布式锁传输协议体积是比较大的。在I/O链路上，数据库分布式锁使用同步I/O进行通信，因此支持的客户端额定数量较少。数据库由于其通用性，其实现相对复杂，比如：涉及到SQL解析，索引选择等处理步骤，因此在耗时上较多。        可以看到数据库分布式锁的性能较差，所以它适合客户端少，并发度以及访问量较低的场景，比如：防止后台任务并行运行的工作。 正确性问题        分布式锁的正确性是指，能够保证锁在任意时刻不会被两个（或多于两个以上）客户端同时持有的特性。如果使用过类似JUC的单机锁，一定会觉得保证正确性对于锁而言，不是应该很容易做到的吗？在认为容易之前，先回顾一下在单机锁中是如何保证正确性的。单机锁是依靠CAS操作来进行锁资源状态的设置，如果客户端能够设置成功，才被认为获得了锁。 关于JUC单机锁的实现原理，在《Java并发编程的艺术》中第五章会有详细介绍，想更细致了解的同学可以选择更深入的阅读。        由于单机锁的CAS操作是由系统指令保证，链路极短且可靠，并且资源状态由锁本身维护，从而能够确保其正确性。分布式锁的资源状态在存储服务上，而存储服务会以开放形式部署在数据中心里，如果有其他客户端非法删除了资源状态，而恰好此时有另一个客户端尝试获取锁，这就造成两个客户端都能获取到锁，导致同步逻辑无法被保护。        有同学会问，我的存储服务管理的很好，只有自己的应用使用，这样是不是就没有正确性问题了？客观的讲，专有专用确实提升了正确性，但非法删除资源状态的“凶手”不见得是其他团队的应用，还有可能是存储服务自己。在使用存储服务时，出于保障可用性考虑，一般会使用主从结构的部署方式。考虑一种情况：如果主节点宕机，主从会进行（自动）切换，而主从之间的数据同步由于存在延迟，这会使得另一个客户端在访问新晋升的主节点时，有概率无法看到“已有”的某些资源状态，进而成功获取到锁，导致正确性再次被违反。 存储服务的主从切换对分布式锁正确性的影响，在拉模式的分布式锁中会详细的介绍。        可以看到，纵使有专用的存储服务，也无法完全确保分布式锁的正确性。绝对的正确性是无法做到的，不同的分布式锁实现由于其存储服务的不同，正确性保障也有强弱之分。        数据库分布式锁的正确性保障是比较高的，依托于经典的关系数据库，纵使主从复制带来的问题，也能得到较好的解决，比如：MySQL可以通过开启半同步复制，牺牲一些同步效率来确保主从数据的一致性，进而提升了分布式锁的正确性保障。 可用性问题        分布式锁可用性是指，能够保证客户端在任意时刻都可以对锁进行（获取或者释放）操作的特性。分布式锁的可用性又可以分为：个体和全局可用性，前者关注单个锁的可用性，而后者要求不能出现分布式锁服务整体不可用的情况。不论个体还是全局可用性，它都是由网络和存储服务来保障的，这个过程如下图所示：        如上图所示，分布式锁的资源状态保存在存储服务上，依靠（使用锁的）实例（或线程）通过网络来进行操作。如果网络设备出现问题，则会导致连接在故障设备上的实例无法使用分布式锁，出现全局可用性问题。针对这个问题，如果实例部署在自建数据中心，则需要专项考虑，如果部署在云上，则由云厂商负责保障。网络是基础设施，本文不做过多探讨，还是将主要视角放在存储服务上，可以狭义的认为：分布式锁的可用性基本等于存储服务的可用性。        先看一下分布式锁的个体可用性问题，在单机锁中，锁的资源状态和应用实例在同一个进程中，是一体的，而分布式锁的资源状态与应用实例相互独立。假设一个场景：实例在获取到锁后，由于发生异常导致没有释放锁（可能是：没有释放锁或实例突然宕机），但锁的资源状态显示锁依旧被持有，这会导致这把锁不会再有实例能够获取，出现了死锁。解决死锁最直接的办法是增加实例占据资源状态的超时时间，通过这一约束，纵使出现了死锁，也会在超时时间到达后完成自愈，避免出现个体不可用的窘境。        以数据库分布式锁为例，如果实例在释放锁之前发生异常，就会出现个体可用性问题。解决方案就是引入超时机制，可以通过在lock表中新增一个expire_time字段，每次实例获取锁时都需要设置超时时间，当超时时间到达时，将会删除该记录。清除超时锁记录的工作可以交给一个专属的实例去完成，该过程如下图所示：        如上图所示，通过增加分布式锁的占用超时时间，可以有效的避免由于实例异常而导致出现资源状态没有释放的问题。应用实例在获取锁时，需要将过期时间计算好，一般是系统当前时间加上一个超时时间差，设置短了，正确性被违反的几率就会增加，设置长了，可用性问题的自愈时间就会增加。至于这个时间差需要设置多少，需要结合应用获取锁后执行同步逻辑的最大耗时来考虑，也就是说时间差设置长短的问题还是抛给了分布式锁的使用者。        使用一个超时清除实例来定期删除过期的资源状态，虽然让整体架构变得清晰，但也显得有些累赘，因此可以使用Redis之类的缓存系统来维护资源状态，通过设置缓存的过期时间，能够做到过期资源状态的自动删除。        接下来再看分布式锁的全局可用性问题，存储服务一般可以通过（主从或其他）集群技术来保障其可用性，当集群中的部分节点发生宕机也不会影响到外界使用存储服务，进而保证了分布式锁的全局可用性。 以Redlock为代表的，基于法定人数（即多数）的机制，就属于非主从模式的集群技术，这种模式将在拉模式的分布式锁中进行介绍。        虽然集群技术保障了全局可用性，但是它也会带来正确性被违反的风险，比如：集群主从切换时，数据同步的延迟会导致正确性存在被违反的可能。不仅是全局可用性，为提升个体可用性而引入的超时机制，也会为给分布式锁带来“个体正确性”被违反的风险，比如：由于进程暂停或者网络延迟，可能会使获取到锁的实例出现停顿，随着超时时间到达，锁被超时释放，其他实例就可以获取到锁，同步逻辑被并行执行，导致正确性被违反。 分布式锁的超时机制与实例暂停导致正确性被违反的问题，在拉模式的分布式锁中会进行详细介绍。        可以看到可用性与正确性如同CAP原理中的可用性（A）和一致性（C）一样，在分布式环境（也就是P）中，是一对矛盾。虽然二者是矛盾，但是需要开发者能够根据实际情况作出权衡和取舍。 成本问题        与依靠JDK就能满足需求的单机锁不同，分布式锁至少需要一个存储服务，它可以是关系数据库，也可以是分布式协调系统，比如：ZooKeeper。部署存储服务，会涉及到成本问题。如果使用者依赖分布式锁去解决效率问题，比如：避免集群中出现重复计算，而且计算频度很低的话，部署和维护一个存储服务就显得成本很高，不是那么划算了。        有同学会说，存储服务可以共享呀！比如：部署一个Redis主从集群，大家一起使用，这样成本不就降下来了吗？没错，通过共享存储服务，提升其利用率，确实可以拉低成本，但是分布式锁的可用性会受到很大的挑战。可以设想：在电商场景中，交易核心链路依赖分布式锁避免库存扣减的并发问题，由于使用了共享存储服务，如果其他业务导致共享存储服务出现（链接被耗尽，容量达到阈值或负载很高的）问题，导致交易核心链路的订购成功率下跌，影响到了用户体验，这就得不偿失了。        因此，可以通过降低存储服务的规格来节省成本，而尽可能的不做共享使用，除非是很明显的边缘业务。建议以技术的产品线作为划分依据，不同产品线之间不共用存储服务，如下图所示：        如上图所示，不同的业务线会使用自己的存储服务，这样就可以根据实际需求来裁剪（或定制）它的规格。举个例子，商品产品线维护了库存服务，使用商品ID作为分布式锁名称，单日交易商品的种数并不多（假定是50万），由于单个键值不会超过1/4KB，理论128MB左右的内存空间就足够存储单日使用的分布式锁资源状态。因此，考虑一些Buffer，商品产品线可以将Redis的规格设置在1GB内存，采用主从集群模式部署。 单个键值（也就是一行资源状态，键和值的长度都在64以内）理论不超过256byte，500000 * (1/4)KB / 1024 = 122MB        虽然不同的业务线使用相互独立的存储服务，但是在代码层面需要使用同一套技术。通过分布式锁框架统一所有的使用方，除了复用率和维护性的提升以外，还可以通过统一的运维系统对数据进行观测，并定义形式统一的监控与报警策略，获得长效收益。 在分布式锁框架中，会对该框架的原因以及设计进行详细介绍。        如果某些业务不需要分布式锁提供很高的可用性保障，甚至可以部署一个单节点的存储服务，虽然存在单点问题，但通过专有专用，一样可以获得很好的效果。不同的存储服务也会有不一样的成本，比如：部署和维护ZooKeeper集群的成本会高于Redis集群。        对于数据库分布式锁而言，如果独立部署和维护一个数据库主从集群来实现分布式锁，是不太现实的。因为锁的资源状态数量有限且对磁盘容量占用很小，所以在业务线中可以选择一个访问量不高的数据库，建立独立的Schema使用会是一个比较经济的方案。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-framework.html":{"url":"book/distribute-lock-framework.html","title":"热风-分布式锁框架","keywords":"","body":"分布式锁框架 为什么需要分布式锁框架？        不同的分布式锁实现（以下简称：实现，比如：Redis或者ZooKeeper），它们之间的性能、可用性和正确性保障以及成本都会有所不同，因此需要提供一种适配多种不同实现方式的分布式锁技术方案（以下简称：方案）。虽然分布式锁的实现可以有多种，但是都需要有监控、流控以及热点锁访问优化等特性，所以该方案不仅能够提供给使用者一致的分布式锁API，还可以让开发者能够适配不同的分布式锁实现，同时还具备良好的横向功能扩展能力。        既要适配不同的实现方式，又要支持横向扩展，这一纵一横的扩展需求就需要通过设计一个简单的分布式锁框架（以下简称：框架）来达成了。该框架，不仅能够提供给使用者一致且简洁的API，而且能够快速适配不同的分布式锁实现，最关键的还可以提供给开发者以横向拦截的方式来扩展分布式锁获取与释放链路（以下简称：链路）的能力。 分布式锁框架        这个简单框架组成分为两类，共5个模块。一类是面向使用者的客户端和starter，另一类是面向开发者的SPI、实现扩展与插件扩展。如果只是想使用分布式锁，那就只需要依赖框架提供的starter，前提是应用基于springboot构建的。如果想扩展分布式锁的实现或者链路，可以通过实现框架提供的SPI以达成目的。        框架中各模块包含的主要组件以及相互关系如下图所示：        如上图所示，使用者通过依赖starter将框架引入应用，同时使用客户端来完成分布式锁的创建、获取与释放工作。开发者可以通过扩展SPI中的LockRemoteService来将不同的实现引入到框架中，同时这些实现会对使用者透明。开发者还可以通过实现SPI中的LockHandler完成对链路的扩展，这些扩展会以切面的形式嵌入到执行链路中，并且对分布式锁的实现以及使用者透明。 分布式锁客户端        面向使用者的分布式锁客户端，以工厂模式进行构建，使用者可以通过传入锁名称来获取到对应的分布式锁，并使用之。客户端通过依赖SPI将分布式锁实现与客户端分离开，其主要类图如下所示：        客户端模块主要定义了分布式锁的接口以及基本实现。分布式锁DistributeLock主要定义了两个方法，分别是：获取锁的tryLock(long waitTime, TimeUnit unit)和释放锁的unlock()。有读者一定会问：为什么没有提供类似JUC中的lock()方法呢？如果有的话，客户端调用后，会等待获取到锁后才返回，使用起来更加方便。原因是客户端使用分布式锁进行加锁时，实际会在底层会发起网络通信，由于通信的不可靠性，比如：一旦发生阻塞，将会导致长时间的阻塞应用逻辑，这可能未必是用户所期望的。所以明确的在参数中给出获取锁的超时，并要求用户传入，使用户对于获取分布式锁的开销有了更明确的感知。 如果要实现类似JUC中的lock()方法语义，可以通过传入一个较长的等待时间来近似做到。        分布式锁实现DistributeLockImpl会将客户端获取锁（以及释放锁）的调用请求委派给SPI，由SPI中的LockHandler来处理。使用者只需要从DistributeLockManager中获取锁，然后就可以对锁进行操作，示例代码如下： DistributeLock lock = distributeLockManager.getLock(\"lock_name\"); if (lock.tryLock(1, TimeUnit.SECONDS)) { try { // do sth... } finally { lock.unlock(); } }        上述代码表示获取一个名为lock_name的分布式锁，然后对其尝试加锁，超时时间为1秒。如果1秒内能够成功加锁，则执行一段逻辑后进行解锁，如果未能在1秒内加锁，则直接返回。 分布式锁SPI        面向开发者的分布式锁SPI，以责任链模式进行构建，开发者可以选择适配不同的实现或者扩展链路。分布式锁SPI是框架扩展性的体现，其主要类图如下所示：        SPI模块定义了扩展框架所需的相关接口，包含：支持分布式锁实现适配的LockRemoteResource和扩展链路的LockHandler。        LockRemoteResource定义了获取锁资源的两个方法，即：tryAcquire和release，前者声明在超时时间内对远程锁资源进行获取，并返回获取结果AcquireResult，后者声明根据资源名称和值对获取到的资源进行释放。只要具备资源获取和释放的分布式服务，都可以通过适配到LockRemoteResource，进而以分布式锁实现的形式集成到框架中。以Redis的集成作为一个例子，Redis的String数据类型，具备键值的存取功能，那就可以将键值视作分布式锁资源，键值新增作为资源获取，键值删除作为资源释放，以此将Redis转换为分布式锁实现，从而整合到框架中。        LockHandler定义了获取与释放锁的行为，分别由acquire和release两个方法来实现。框架定义了获取锁上下文AcquireContext，它由框架构建并传递给acquire方法，LockHandler处理获取锁的工作，并返回获取结果AcquireResult。获取结果AcquireResult主要描述本次获取锁的操作结果，包括：是否获取成功以及获取失败的原因。对于释放锁而言，框架提供了释放锁上下文ReleaseContext，也由框架构建并传递给release方法。        多个LockHandler会组成分布式锁获取与释放链路，开发者通过扩展LockHandler，将实现以插件的形式集成到框架中。框架默认提供了头和尾两个LockHandler节点，而开发者（或框架）提供的扩展将会穿在链路上，链路分为获取锁和释放锁两条链路，其中获取锁链路如下图所示：        如上图所示，锁的获取链路会从Head节点开始，将获取锁上下文AcquireContext传递给链路上所有LockHandler的acquire方法，最终抵达Tail节点，并由Tail节点调用LockRemoteResource的tryAcquire方法，完成远程锁资源的获取。如果链路上的扩展节点需要提前中断获取锁的请求，可以选择不调用AcquireChain的invoke方法，这会使得责任链提前返回。        锁的释放链路与获取链路相似，如下图所示：        如上图所示，由Head节点开始，穿越整条链路，抵达Tail节点，由该节点调用LockRemoteResource的release方法完成远程锁资源的释放。 如果在扩展在获取锁链路中，先于LockRemoteResource进行了操作，那么在释放锁链路中，推荐在ReleaseChain#invoke方法之后进行操作，这样扩展在两条链路上的行为就会对称。        任何节点的增加和删除，对于链路上的其他节点而言都是没有影响的，因此锁获取与释放链路的抽象提供了良好的扩展能力，后面会演示如何通过实现LockHandler来扩展框架。 实现：基于Redis的分布式锁        实现LockRemoteResource可以扩展分布式锁实现，接下来以Redis作为维护锁资源状态的存储服务，客户端选择Lettuce，它是一个基于Netty的Redis客户端，它最大的特点是基于非阻塞I/O，能够帮助开发者构建响应式应用，可以很好的替代Jedis客户端。 Lettuce版本为：6.1.2.RELEASE，Redis版本为：6.2.6。        Redis的分布式锁实现RedisLockRemoteResource会在拉模式的分布式锁中详细介绍，现在只需要知道它通过5个参数来进行构建，参数名、类型与描述如下表所示： 参数名 类型 描述 address String Redis服务端地址 timeoutMillis int 访问Redis的超时（单位：毫秒） ownSecond int 占据键值的时间（单位：秒） minSpinMillis int 自旋最小时间（单位：毫秒） randomMillis int 自旋随机增加的时间（单位：毫秒）        Lettuce客户端通过address与Redis建立长链接。在获取锁时，会尝试新增一个键值，如果新增失败，将会选择睡眠一段时间（时长为：minSpinMillis + new Random().nextInt(randomMillis)），醒后再试。如果新增成功，则代表实例成功获取到锁，同时该键值的存活时间为ownSecond，在存活时间内实例需要执行完同步逻辑，否则就会出现正确性被违反的风险。 Redis分布式锁Starter        对于分布式锁框架的使用者而言，可能不希望关注这么多细节，只需要提供一个Redis服务器地址，然后添加一下依赖和配置就可以跑起来，那就最好不过了。SpringBoot提供了良好的扩展与集成能力，只需要提供相应的starter，就可以让使用者获得这种极致的使用体验。 该starter在子项目distribute-lock-redis-spring-boot-starter中。        Redis分布式锁的starter主要包含了一个Spring配置，其主要代码如下所示： @Configuration @ConditionalOnProperty(prefix = Constants.PREFIX, name = \"address\") @ConditionalOnClass(RedisLockRemoteResource.class) @EnableConfigurationProperties(RedisProperties.class) @Import(CommonConfig.class) public class DistributeLockRedisAutoConfiguration implements EnvironmentAware { private Environment environment; @Bean(\"redisLockRemoteResource\") public LockRemoteResource lockRemoteResource() { Binder binder = Binder.get(environment); BindResult bindResult = binder.bind(Constants.PREFIX, Bindable.of(RedisProperties.class)); RedisProperties redisProperties = bindResult.get(); return new RedisLockRemoteResource(redisProperties.getAddress(), redisProperties.getOwnSecond(), redisProperties.getMinSpinMillis(), redisProperties.getRandomMillis()); } @Bean(\"redisLockHandlerFactory\") public LockHandlerFactory lockHandlerFactory(@Qualifier(\"lockHandlerFinder\") LockHandlerFinder lockHandlerFinder, @Qualifier(\"redisLockRemoteResource\") LockRemoteResource lockRemoteResource) { return new LockHandlerFactoryImpl(lockHandlerFinder.getLockHandlers(), lockRemoteResource); } @Bean(\"redisDistributeLockManager\") public DistributeLockManager distributeLockManager( @Qualifier(\"redisLockHandlerFactory\") LockHandlerFactory lockHandlerFactory) { return new DistributeLockManagerImpl(lockHandlerFactory); } @Override public void setEnvironment(Environment environment) { this.environment = environment; } }        由于在META-INF/spring.factories配置中声明了DistributeLockRedisAutoConfiguration，所以Spring容器能够扫描并识别Redis分布式锁的配置，并装配三个Bean到使用者的Spring容器中，如下表所示： BeanName 类型 描述 redisLockRemoteResource LockRemoteResource 从当前应用的环境中解析配置，装配一个类型为分布式锁实现的Bean redisLockHandlerFactory LockHandlerFactory 依赖redisLockRemoteResource，装配一个类型为LockHandlerFactory的Bean，目的是提供给分布式锁API获取LockHandler的能力 redisDistributeLockManager DistributeLockManager 使用者直接依赖该Bean，提供分布式锁获取与使用的功能        由于配置被ConditionalOnProperty注解修饰，使用者除了依赖该starter，还需要在application.properties中声明键为spring.distribute-lock.redis.address的配置，如果没有声明该配置，该starter就不会装配上述三个Bean到容器中。 Constants定义了常量PREFIX，值为：spring.distribute-lock.redis 使用分布式锁框架        该框架使用起来比较简单，通过依赖distribute-lock-redis-spring-boot-starter，然后在application.properties中如下配置： spring.distribute-lock.redis.address=redis服务端地址，比如：redis://ip:port spring.distribute-lock.redis.own-second=可选，默认10，表示键值的过期时间，单位：秒 spring.distribute-lock.redis.min-spin-millis=可选，默认10，表示自旋等待的最小时间，单位：毫秒 spring.distribute-lock.redis.random-millis=可选，默认10，表示自旋等待随机增加的时间，单位：毫秒        依赖坐标并声明配置后，该starter会装配一个DistributeLockManager到应用的Spring容器中，使用示例如下所示： @Autowired @Qualifier(\"redisDistributeLockManager\") private DistributeLockManager distributeLockManager; @Autowired private Counter counter; @Override public void run(String... args) throws Exception { DistributeLock distributeLock = distributeLockManager.getLock(\"lock_key\"); int times = CommandLineHelper.getTimes(args, 1000); DLTester dlTester = new DLTester(distributeLock, 3); dlTester.work(times, () -> { int i = counter.get(); i++; counter.set(i); }); dlTester.status(); System.out.println(\"counter value:\" + counter.get()); }        上述代码示例，尝试获取一个名称为lock_key的分布式锁，然后循环1000次（或由启动参数指定次数的）操作，每次操作都会尝试加锁（等待锁的时间为3秒），加锁成功后，获取远程Redis服务端counter的值，自增后再写回。获取-计算-写回，这个过程如果不加锁，在多进程（或并发）环境中，就会出现数据覆盖的可能，从而导致计数的错乱。        Redis中的counter已经提前初始化为0，我们用3个客户端进行操作，每个客户端循环100次，客户端的输出分别为：        客户端1输出：获取锁成功200次，失败0次，最终看到counter值为486。        客户端2输出：获取锁成功192次，失败8次，最终看到counter值为592。        客户端3输出：获取锁成功200次，失败0次，最终看到counter值为332。        登录到Redis服务端查看counter的最终值为592，它与客户端2的输出一致。可以看到基于Redis的分布式锁能够正常工作，将原有线程不安全的逻辑进行了保护，使之能够安全的运行于分布式环境中。 扩展：分布式锁访问日志        分布式锁的获取与释放会涉及到网络通信，所以该过程需要添加监控，最简单的方式是对每次分布式锁的使用打印日志，日志内容可以是获取与释放锁的关键信息，比如：锁的名称与耗时等。通过收集和分析日志，一来可以掌握分布式锁的数据指标，二来可以为可能出现的问题进行报警，缩短故障反应时间。        通过扩展SPI中的LockHandler，可以将打印访问日志的特性植入到链路中，并且该过程对使用者和锁实现透明。扩展的代码如下所示： import io.github.weipeng2k.distribute.lock.spi.AcquireContext; import io.github.weipeng2k.distribute.lock.spi.AcquireResult; import io.github.weipeng2k.distribute.lock.spi.ErrorAware; import io.github.weipeng2k.distribute.lock.spi.LockHandler; import io.github.weipeng2k.distribute.lock.spi.ReleaseContext; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.core.annotation.Order; import java.util.concurrent.TimeUnit; /** * * 日志输出Handler，打印获取锁和释放锁的日志 * * * @author weipeng2k 2021年11月27日 下午20:44:18 */ @Order(1) public class AccessLoggingLockHandler implements LockHandler, ErrorAware { private static final Logger logger = LoggerFactory.getLogger(\"DISTRIBUTE_LOCK_ACCESS_LOGGER\"); @Override public AcquireResult acquire(AcquireContext acquireContext, AcquireChain acquireChain) throws InterruptedException { AcquireResult acquireResult = acquireChain.invoke(acquireContext); logger.info(\"acquire|{}|{}|{}|{}\", acquireContext.getResourceName(), acquireContext.getResourceValue(), acquireResult.isSuccess(), TimeUnit.MILLISECONDS.convert(System.nanoTime() - acquireContext.getStartNanoTime(), TimeUnit.NANOSECONDS)); return acquireResult; } @Override public void release(ReleaseContext releaseContext, ReleaseChain releaseChain) { releaseChain.invoke(releaseContext); logger.info(\"release|{}|{}|{}\", releaseContext.getResourceName(), releaseContext.getResourceValue(), TimeUnit.MILLISECONDS.convert(System.nanoTime() - releaseContext.getStartNanoTime(), TimeUnit.NANOSECONDS)); } @Override public void onAcquireError(AcquireContext acquireContext, Throwable throwable) { logger.error(\"acquire|{}|{}|{}|{}\", acquireContext.getResourceName(), acquireContext.getResourceValue(), false, TimeUnit.MILLISECONDS.convert(System.nanoTime() - acquireContext.getStartNanoTime(), TimeUnit.NANOSECONDS), throwable); } @Override public void onReleaseError(ReleaseContext releaseContext, Throwable throwable) { logger.error(\"release|{}|{}|{}\", releaseContext.getResourceName(), releaseContext.getResourceValue(), TimeUnit.MILLISECONDS.convert(System.nanoTime() - releaseContext.getStartNanoTime(), TimeUnit.NANOSECONDS), throwable); } }        上述代码通过实现LockHandler的acquire和release方法在分布式锁使用链路上打印日志，可以看到acquire方法实现中，在获取锁结果AcquireResult返回后，打印了获取锁的名称、值、获取是否成功的结果以及耗时（单位毫秒）。释放锁的release方法与acquire实现类似。        另外AccessLoggingLockHandler实现了ErrorAware，如果在链路中出现异常，导致链路提前中断，则框架会在对应的（获取或释放）链路回调onAcquireError或onReleaseError方法。        在应用中除了依赖分布式锁的starter，再依赖扩展插件的坐标就能激活日志打印插件。由于锁的日志打印频繁，推荐将该日志同应用日志分开，所以插件提供了日志文件片段，便于用户使用。用户可以选择在日志配置中声明分布式锁的日志目录即可，配置如下： 日志系统选用的是logback。        可以看到在应用日志配置中，声明DISTRIBUTE_LOCK_LOG_DIR属性为分布式锁的日志目录，而锁访问日志将会输出在该目录中的：distribute-lock-access.log文件中。启动应用，然后使用分布式锁，可以看到（部分）日志输出，如下图：        如上图所示，基于Redis的分布式锁在获取与释放过程中会打印出访问日志，可以看到，其中获取锁的耗时基本在35毫秒左右，而释放锁也差不多是这个量级。 Redis部署在公有云，因此延迟比较大，实际在真实生产环境中会好很多。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-spin-impl.html":{"url":"book/distribute-lock-spin-impl.html","title":"热风-拉模式的分布式锁","keywords":"","body":"拉模式的分布式锁        拉模式的分布式锁，需要实例（通过客户端）以自旋的形式，主动去调用存储服务，根据调用结果来判断是否获取到了锁。 什么是拉模式？        拉模式的分布式锁，需要实例（通过客户端）以自旋的形式，主动去调用存储服务，根据调用结果来判断是否获取（或释放）到了锁。这需要存储服务提供类似ConcurrentMap中的原子新增和删除功能。在拉模式的分布式锁中，实例和存储服务的结构如下图所示：        在获取拉模式分布式锁时，需要使用addIfAbsent在存储服务中新增锁对应的资源状态（或一行记录），它需要包含能够代表获取锁的实例（或线程）标识，并且该标识能够确保全局唯一，不同获取锁的实例标识各不相同。释放锁时，需要通过当前实例（或线程）标识，使用compareAndDelete（以下简称：CAD）来删除存储服务中对应的资源状态。存储服务提供的上述功能以及描述，如下表所示： 名称 参数说明 功能 描述 addIfAbsent(key, value, ttl) key，键，对应锁的名称，全局唯一value，值，与键对应，在删除时可以提供比对功能ttl，过期时间，键新增后，在ttl时间过后，会自动删除 原子新增 如果已经存在key，则会返回新增失败，否则新增键值成功，且过期时间在ttl之后。该过程需要保证原子性 compareAndDelete(key, value) 同上 原子删除 只有待删除键key对应的值与参数值value相等，才能够删除该键值。该过程需要保证原子性        每一个分布式锁，都对应于存储服务上的一个键，可以将存储服务认为是一个巨大（且线程安全）的Map。如果通过调用addIfAbsent能够成功的新增一个键值，则代表成功获取到了锁。获取拉模式分布式锁的流程如下图所示：        如上图所示，虚线框中的流程就是获取锁的自旋过程，但在介绍它之前，先要看一下获取锁时需要确定的输入，这些输入变量的名称以及描述如下表所示： 变量 名称 描述 T 当前时间 当前系统时间 D 超时时长 实例获取锁能够等待的最长时间 TTL 过期时长 资源状态在存储服务上存活的最长时间 S 睡眠时长 新增资源状态（也就是获取锁）失败需要睡眠的时长 RN 锁资源名称 分布式锁的名称，可以是某个业务编号 RV 锁资源值 获取锁的实例标识，需要保证唯一性 R 当前执行信息 获取锁的实例以及线程信息        在进入自旋过程前，需要使用digesterFunc函数将R转换为RV。这个函数可以是简单的将实例IP与线程ID的拼接返回，也可以直接返回一个UUID，只要能够确保转换成的RV在这一时刻是全局唯一的，因为在释放锁时，当前线程需要传入该值来完成键值的删除。        在自旋过程中，首先需要判断是否已经超时，如果没有超时，则会使用addIfAbsent尝试新增当前锁的资源状态，资源状态包括名称RN和值RV。由于addIfAbsent具备原子性，多个实例对相同的RN进行新增操作时，只会有一个能够成功，这样就兑现了锁的排他性。        如果调用addIfAbsent返回失败，则表明实例没有获取到锁，此时客户端需要不断的循环重试直至成功，以此来满足实例获取锁的诉求。如果退出循环的条件只是新增资源状态成功，那会带来可用性问题。由于调用存储服务需要通过网络，稍有不慎会导致实例陷入长时间阻塞。因此，循环退出的条件还包括获取锁的超时时间到，每次新增资源状态失败可以睡眠一段时间，避免对存储服务产生过多无效请求。        当释放锁时，实例通过传入锁的资源名称RN和值RV来删除资源状态。由于CAD操作具备先比较再删除的特性，使得只有资源状态中的值与RV相同才能够删除，这就保证了只有获取到锁的实例才能删除资源状态并释放锁，同时多次执行删除操作也是无副作用的。 需要注意的点        拉模式的流程看起来是很简单的，实例通过客户端去获取锁，如果无法在存储服务中新增资源状态，就进行重试，要么超时返回，要么获取到锁。通过一个循环以及少量的时间运算与判断，通过几行代码就可以实现上述逻辑了。如果从能用的角度去看，就是这么简单，但想用的安心，就需要多考虑一点了。拉模式获取锁的主要步骤包括：访问存储服务（调用其新增接口）、时间运算与判断以及睡眠，其中访问存储服务和睡眠对实例获取锁有实际影响，接下来分析它们各自需要被关注的点。        访问存储服务需要注意的点包括：请求的I/O超时、访问存储服务耗时和过期时长设置。首先，请求需要有I/O超时，举个例子：我们经常使用HttpClient去请求Web服务来获取数据，如果Web服务很慢或者网络延迟很高，调用线程就会被挂在那里很久。访问存储服务和这个问题一样，为了避免客户端陷入未知时长的等待，对存储服务的请求需要设置I/O超时。其次，访问存储服务耗时越短越好，如果访问耗时很低，会提升客户端的响应性，当然不同的存储服务访问耗时也会不一样，基于Redis的分布式锁在访问耗时上就优于数据库分布式锁。最后，过期时长支持定制，新增资源状态时会设置过期时长，一般来说这个时长会结合同步逻辑的最大耗时来考虑，是个固定值，比如：10秒。获取锁时，实例其实可以根据当前的上下文估算出可能的耗时，比如：发现同步逻辑中处理的列表数据包含的元素数量比平均数高一倍，如果此时能够适当增加对应的过期时长，会是一个好的选择。这就需要分布式锁框架提供API，能够支持实例设置过期时长，通过设置一个更大的值，就能有效减少由于过期自动释放锁而导致的正确性问题。 过期时长的设置只会影响到本次获取的锁，是基于请求的，不是全局性的。        睡眠需要关注对存储服务产生的压力。对于睡眠而言，简单的做法是固定一个时长，比如：一旦客户端新增资源状态失败，就睡眠15毫秒。如果某个锁资源在多个实例之间有激烈的竞争，这种方式会使得未获取到锁的实例在一个较小的时间范围内同时醒来，并发起对存储服务的重试，无形中增加了存储服务的瞬时压力。如果实例中又以多线程并发的方式获取锁，会导致这个问题变得更糟，解决方式就是引入随机。可以通过指定最小睡眠时长min和随机睡眠时长random来计算本次应该睡眠的时长，每次睡眠时长不固定，只是在[min, min + random)内随机取值。通过随机睡眠会使重试变得离散，一定程度上减轻了对存储服务的压力。 Redis分布式锁实现        分布式锁使用Redis之类的缓存系统来存储锁的资源状态，可以简化其实现方式，毕竟不需要用编程的方式来清除过期的资源状态，因为缓存系统固有的过期机制可以很好的处理这项工作。通过实现LockRemoteResource接口，可以将Redis适配成为分布式锁实现，并集成到框架中。Redis能够满足拉模式分布式锁对存储服务的诉求，综合考虑性能和成本，它非常适合作为拉模式的分布式锁实现。        在分布式锁框架中，介绍了Redis分布式锁实现（RedisLockRemoteResource）的构造函数以及其涉及的参数变量，接下来从获取和释放锁两个方面来介绍实现内容。 Redis分布式锁的获取        Redis作为存储服务的新增接口，需要使用类似命令：SET $RN $RV NX PX $D。该命令通过NX选项，确保只有在键（也就是$RN）不存在的情况下才能设置（添加），同时PX选项表示该键将会在$D毫秒后过期，而值$RV需要做到所有客户端唯一。        使用Lettuce客户端，上述操作的代码如下所示： private boolean lockRemoteResource(String resourceName, String resourceValue, int ownSecond) { SetArgs setArgs = SetArgs.Builder.nx().ex(ownSecond); boolean result = false; try { String ret = syncCommands.set(resourceName, resourceValue, setArgs); // 返回是OK，则锁定成功，否则锁定资源失败 if (\"ok\".equalsIgnoreCase(ret)) { result = true; } } catch (Exception ex) { throw new RuntimeException(\"set key:\" + resourceName + \" got exception.\", ex); } return result; }        上述方法提供了基于Redis的addIfAbsent语义，且支持过期时长的一并设置。参数SetArgs使用构建者模式创建，syncCommands是Lettuce提供的RedisCommands接口，由于当前逻辑需要同步获得设置的结果，所以采用同步模式。        获取锁的方法，代码如下所示： public AcquireResult tryAcquire(String resourceName, String resourceValue, long waitTime, TimeUnit timeUnit) throws InterruptedException { // 目标最大超时时间 long destinationNanoTime = System.nanoTime() + timeUnit.toNanos(waitTime); boolean result = false; boolean isTimeout = false; Integer liveSecond = OwnSecond.getLiveSecond(); int ownTime = liveSecond != null ? liveSecond : ownSecond; AcquireResultBuilder acquireResultBuilder; try { while (true) { // 当前系统时间 long current = System.nanoTime(); // 时间限度外，直接退出 if (current > destinationNanoTime) { isTimeout = true; break; } // 远程获取到资源后，返回；否则，spin if (lockRemoteResource(resourceName, resourceValue, ownTime)) { result = true; break; } else { spin(); } } acquireResultBuilder = new AcquireResultBuilder(result); if (isTimeout) { acquireResultBuilder.failureType(AcquireResult.FailureType.TIME_OUT); } } catch (Exception ex) { acquireResultBuilder = new AcquireResultBuilder(result); acquireResultBuilder .failureType(AcquireResult.FailureType.EXCEPTION) .exception(ex); } return acquireResultBuilder.build(); }        上述方法，首先将获取锁的超时时间单位统一到纳秒，由超时时长waitTime计算出最大的超时时间destinationNanoTime。在接下来的自旋中，如果当前系统时间current大于destinationNanoTime就会超时返回。如果调用存储服务返回新增失败，则会执行spin()方法进行睡眠，而睡眠的时长为：ThreadLocalRandom.current().nextInt(randomMillis) + minSpinMillis，它会在一个时间范围内进行随机，以此避免对存储服务产生无谓的瞬时压力。        可以通过OwnSecond工具对资源状态的占用时长（也就是Redis键值的过期时间）进行自定义设置。如果需要更改本次调用对于锁的占用时长，可以在调用锁的tryLock()方法之前，执行OwnSencond.setLiveSecond(int second)方法，该工具依靠ThreadLocal将实例设置的占用时长传递给框架。 Redis分布式锁的释放        释放锁可以通过资源名称RN和资源值RV删除对应的资源状态即可，但该过程必须是原子化的。如果是先根据RN查出资源状态，再比对RV与资源状态中的值是否一样，最后使用del命令删除对应键值，这样的两步走会导致锁有被误释放的可能，该过程如下图所示：        如上图所示，客户端A在锁的有效期（也就是占用时长）快结束时调用了unlock()方法。如果采用两步走逻辑，在使用del命令删除键值前，锁由于超时时间到而自动释放，此时客户端B成功获取到了锁，并开始执行同步逻辑。客户端A由于（旧）值比对通过，使用del命令删除了资源状态对应的键值，这时运行在客户端B上的同步逻辑就不会再受到锁的保护，因为其他实例可以获取到锁并执行。        Redis可以通过Lua脚本做到原子化CAD的支持，脚本如下： if redis.call(\"get\", KEYS[1]) == ARGV[1] then return redis.call(\"del\", KEYS[1]) else return 0 end        可以看到其实就是两步走逻辑的Lua版本，只是Redis对于Lua脚本的执行是确保原子性的。 如果使用阿里云的RDB缓存服务，可以使用其cad扩展命令，不使用上述脚本。        释放锁的方法，代码如下所示： public void release(String resourceName, String resourceValue) { try { syncCommands.eval( \"if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end\", ScriptOutputType.INTEGER, new String[]{resourceName}, resourceValue); } catch (Exception ex) { // Ignore. } }        上述方法通过调用RedisCommands的eval()方法执行CAD脚本来安全的删除资源状态来完成锁的释放。 Redis分布式锁存在的问题 主从切换带来的问题        如果存储服务出现故障，则会导致分布式锁不可用，为了确保其可用性，一般会将多个存储服务节点组成集群。以Redis为例，使用主从集群可以提升分布式锁服务的可用性，但是它会带来正确性被违反的风险。        Redis主从集群会在所有节点上保有全量数据，主节点负责数据的写入，然后将变更异步同步到从节点。这种同步模式会导致在主从切换的一段时间内，由于新旧主节点上的数据不对等，导致部分分布式锁存在可能被多个实例同时获取到的风险，该过程如下图所示：        上图中，主从集群可以提升分布式锁的可用性，避免出现由于Redis节点挂掉后导致的不可用。虽然可用性提升了，但是正确性会下降。在步骤1到2过程中，实例A先成功的在切换前的Redis主节点上新增了记录，也就是获取到了order_lock锁。随后Redis主节点挂掉，集群进行主从切换，数据仍在异步的向新晋升的Redis主节点上同步。步骤5到6，实例B刚好此时获取锁，它会尝试在新晋Redis主节点上新增记录，由于数据并未在此时完成同步，所以实例B成功的新增了记录并获取到了order_lock锁。在这一刻，实例A和B都会宣称获取到了order_lock锁，而相应的同步逻辑也会被并行执行，锁的正确性被违反。 看似完美的Redlock        Redis单节点是CP型存储服务，使用它可以满足分布式锁对于正确性的诉求，但存在可用性问题。使用Redis主从集群技术后，会转为AP型存储服务，虽然提升了分布式锁的可用性，但正确性又会存在风险。面对可用性和正确性两难的局面，Redis作者（Salvatore）设计了不基于Redis主从技术的Redlock算法，该算法使用多个Redis节点，使用基于法定人数过半（Quorum）的策略，以期望该算法能做到正确性与可用性的兼得。        Redlock需要使用多个Redis节点来实现分布式锁，节点数量一般是奇数，并且至少要5个节点才能使其具备良好的可用性。该算法是一个客户端算法，也就是说它在每个客户端上运行方式是一致的，且客户端之间不会进行相互通信。接下来以5个节点为例，Redlock算法过程如下图所示：        如上图所示，首先Redlock算法会获取当前时间T，然后使用相同的锁资源名称$RN和资源值$RV并行的向5个Redis节点进行操作。执行的操作与获取单节点Redis分布式锁的操作一致，如果对应的Redis节点上不存在$RN则会成功设置，且过期时长为$D。对5个Redis节点进行设置的结果分别为R1至R5，再取当前时间T’，如果结果成功数量大于等于3且 (T’-T)小于$D，表明在多数Redis节点上成功新增了$RN且这些键均没有过期，则代表客户端获取到了锁，有效期为ET1至ET5的最小值减去T。        如果设置结果成功数量小于3，表明在多个Redis节点上，对于新增$RN没有寻得共识。如果 (T’-T)大于等于$D，表明当前客户端获取的锁已经超时。上述两种情况只要出现一个，则客户端获取锁失败。此时需要在所有Redis节点上运行无副作用的删除脚本，将当前客户端创建的记录（如果有的话，就）删除，避免记录要等到超时才能被清除。 分布式锁框架通过使用Redisson客户端，可以很容的将Redlock集成到框架中，该分布式锁实现代码可以参见分布式锁项目中的distribute-lock-redlock-support模块。        Redlock算法看起来能够在分布式锁的可用性和正确性之间寻得平衡，少量Redis节点挂掉，不会引起分布式锁的可用性问题，同时正确性又得以保证。理想情况下，Redlock看似很完美，但在分布式环境中，进程的暂停或网络的延迟，会打破该算法，使之失效。以Java应用为例，如果在算法判定获取到锁，客户端执行同步逻辑时引入GC暂停，则会可能导致该算法对于正确性的保证失效，该过程如下图所示：        如上图所示，客户端A获取到了锁，然后开始执行锁保护的同步逻辑，该逻辑在同一时刻只能有一个客户端能够执行。当客户端A开始执行逻辑时，由于GC导致进程出现停顿（GC暂停，即stop-the-world，不会由于运行的是业务线程而对其特殊对待，它会一律暂停Java虚拟机中除GC外的线程），而暂停时长超出了锁的有效期，此时锁已经由于超时而释放。        客户端B在锁超时后获取到了锁，然后开始执行同步逻辑，客户端A由于GC结束而恢复执行，此时原本被锁保护的同步逻辑被并发执行，锁的正确性被违反。        可以看到虽然Redlock算法通过基于法定人数的设计，在理论上确保了正确性和可用性，但是在真实的分布式环境中，会出现正确性无法被保证的风险。有同学会问，如果使用没有GC特性的编程语言来开发应用，是不是就可以了？实际上除了GC导致进程暂停，如果同步逻辑中有网络交互，也可能由于TCP重传等问题导致实际的执行时间超出了锁的有效期，最终导致两个客户端又有可能并发的执行同步逻辑。        单节点（或主从集群）的Redis分布式锁也存在上述问题，本质在于基于Redis实现的分布式锁，对于锁的释放存在超时时间的假设。虽然超时避免了死锁，但是会导致锁超时（释放）的一刻，两个客户端有同时进行操作的可能，这是能够在理论模型上推演出来的，毕竟释放锁的不是锁的持有者，而是锁自己。 扩展：本地热点锁        拉模式分布式锁需要依靠不断的对存储服务进行自旋调用，来判断是否能够获取到锁，因此会产生大量的无效调用，平添了存储服务的压力。对于分布式锁而言，竞争的最小单位不是进程，而是线程，由于实际情况中的（应用）实例都是以多线程模式运行的，导致竞争会更加激烈。        在激烈的竞争中，如果遇到热点锁，情况会变得更糟。比如：使用商品ID作为锁的资源名称，对于爆款商品而言，多机多线程就会给存储服务带来巨大的压力，该问题如下图所示：        如上图所示，实例内通过多线程并发的方式获取锁。在单个实例内，假设获取商品锁的并发度是10，那么两个实例就能够给存储服务带来20个并发的调用。以线程的角度来看这20个并发，是合理的，虽然每次请求绝大部分都是无功而返（没有获取到锁），但是这都是为了保证锁的正确性，纵使再高的并发，也只能通过不断的扩容存储服务来抵消增长的压力。        可以想象，存储服务上处理的请求，基本全都是无效的，不断的扩容存储服务显得不现实，是否有其他方法可以优化这个过程呢？答案就是通过本地热点锁来解决。通过使用（单机）本地锁可以有效的降低对存储服务产生的压力，该过程如下图所示：        如上图所示，实例中的多线程应该先尝试竞争本地（基于JUC的单机）锁，成功获取到本地锁的线程才能参与到实例间的分布式锁竞争。从实例的角度去看，如果都是获取同一个分布式锁，在同一时刻只能由一个实例中的一个线程获取到锁，因此理论上对存储服务的并发上限只需要和实例数一致，也就是2个并发就可以了。        可以通过在分布式锁前端增加一个本地锁就能得以实现，但事实上并没有这么容易，因为实例中的多线程需要使用同一把本地锁才会有意义，所以需要有一个Map结构来保存锁资源名称到本地锁的映射。如果对该结构管理不当，对任意分布式锁的访问都会创建并保有本地锁，那就会使实例有OOM的风险。一个比较现实的做法就是针对某些热点锁进行优化，只创建热点锁对应的本地锁来有效减少对存储服务产生的压力。 在实际工作场景中，可以根据生产数据发现实际的热点数据，比如：爆款商品ID或热卖商家ID等，将其提前（或动态）设置到分布式锁框架中，通过将分布式锁“本地化”，来优化这个过程。        由于本地锁的获取是在分布式锁之前，通过扩展分布式锁框架的LockHandler就可以很好的支持这一特性，对应的LockHandler扩展（的部分）代码如下： package io.github.weipeng2k.distribute.lock.plugin.local.hotspot; import io.github.weipeng2k.distribute.lock.spi.AcquireContext; import io.github.weipeng2k.distribute.lock.spi.AcquireResult; import io.github.weipeng2k.distribute.lock.spi.ErrorAware; import io.github.weipeng2k.distribute.lock.spi.LockHandler; import io.github.weipeng2k.distribute.lock.spi.ReleaseContext; import io.github.weipeng2k.distribute.lock.spi.support.AcquireResultBuilder; import org.springframework.core.annotation.Order; import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.Lock; /** * * 本地热点锁LocalHandler * * 获取锁时，会先获取本地的锁，然后尝试获取后面的锁 * 如果后面的锁获取成功，则返回 * 如果后面的锁获取失败，则需要解锁 * * 释放锁时，会先释放后面的锁，然后尝试释放当前的锁，不要抛出错误即可 * * * * @author weipeng2k 2021年12月14日 下午18:43:29 */ @Order(10) public class LocalHotSpotLockHandler implements LockHandler, ErrorAware { private final LocalHotSpotLockRepo localHotSpotLockRepo; public LocalHotSpotLockHandler(LocalHotSpotLockRepo localHotSpotLockRepo) { this.localHotSpotLockRepo = localHotSpotLockRepo; } @Override public AcquireResult acquire(AcquireContext acquireContext, AcquireChain acquireChain) throws InterruptedException { AcquireResult acquireResult; Lock lock = localHotSpotLockRepo.getLock(acquireContext.getResourceName()); if (lock != null) { // 先获取本地锁 if (lock.tryLock(acquireContext.getRemainingNanoTime(), TimeUnit.NANOSECONDS)) { acquireResult = acquireChain.invoke(acquireContext); // 没有获取到后面的锁，则进行解锁 if (!acquireResult.isSuccess()) { unlockQuietly(lock); } } else { AcquireResultBuilder acquireResultBuilder = new AcquireResultBuilder(false); acquireResult = acquireResultBuilder.failureType(AcquireResult.FailureType.TIME_OUT) .build(); } } else { acquireResult = acquireChain.invoke(acquireContext); } return acquireResult; } @Override public void release(ReleaseContext releaseContext, ReleaseChain releaseChain) { releaseChain.invoke(releaseContext); Lock lock = localHotSpotLockRepo.getLock(releaseContext.getResourceName()); if (lock != null) { unlockQuietly(lock); } } private void unlockQuietly(Lock lock) { try { lock.unlock(); } catch (Exception ex) { // Ignore. } } }        可以看到，本地热点锁都存储在LocalHotSpotLockRepo中，由使用者进行设置。通过DistributeLock获取锁时，框架会先从LocalHotSpotLockRepo中查找本地锁，如果没有找到，则执行后续的LockHandler，反之，会尝试获取本地锁。需要注意的是，成功获取到本地锁后，如果接下来没有获取到分布式锁，就需要释放当前的本地锁，避免阻塞其他线程获取分布式锁的行为。        对于释放锁而言，需要在releaseChain.invoke(releaseContext);语句之后释放本地锁，也就是在分布式锁（的存储服务）被释放后，再释放本地锁。如果释放顺序反过来，提前释放了本地锁，会使得被（释放本地锁而）唤醒的线程立刻向存储服务发起无效请求。        上述功能以插件的形式提供给使用者，只需要依赖如下坐标就可以激活使用： io.github.weipeng2k distribute-lock-local-hotspot-plugin 该插件会在应用的Spring容器中注入LocalHotSpotLockRepo，通过调用它的createLock(String resourceName)方法完成本地锁的创建。        接下来通过两个测试用例：distribute-lock-redis-testsuite和distribute-lock-redis-local-hotspot-testsuite来展示本地热点锁的优化效果。考察的指标是通过执行Redis提供的info commandstats来查看SET命令执行的数量来进行判定的，因为获取分布式锁就是依靠SET命令。        两个测试用例都会运行3个实例，分两个批次执行，获取的分布式锁名称都是lock_key。每个实例都会以4个并发获取分布式锁，尝试获取400次，数据对比如下表所示： 用例 执行前SET命令数量 执行后SET命令数量 获取锁成功数量 获取锁失败数量 对Redis的SET请求数量 Redis锁测试集 183168 204413 1099 101 21245 Redis锁测试集（包含本地热点锁插件） 204413 210518 1147 53 6105        如上表所示，可以看到本地热点锁插件能够显著的降低热点锁对存储服务的请求，有70%的无效请求被该插件所阻挡。随着对Redis请求量的下降，分布式锁获取成功率也随之上升。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-event-impl.html":{"url":"book/distribute-lock-event-impl.html","title":"热风-推模式的分布式锁","keywords":"","body":"推模式的分布式锁        前文提到JUC中的单机锁是一种推模式的锁，对于单机锁而言，每个获取锁的线程都会以节点（Node）的形式进入到队列中，该队列的具体内容可以参考《Java并发编程的艺术》第5章的2.2小节，这里只需要注意对锁的获取行为会进行排队。节点（对应的线程）获取了锁，执行完同步逻辑后释放锁，释放锁的操作会通知处于等待状态的后继节点，后继节点（对应的线程）被通知唤醒后会再次尝试获取锁。 什么是推模式？        （同步）队列中的节点就是锁的资源状态，它包含了获取锁的执行信息（实例以及线程）。推模式锁的核心在于队列操作和节点事件通知，对于推模式分布式锁也一样，任意实例获取锁的行为都会以节点的形式记录在队列中，同时节点的变化会通知到实例，这就需要存储服务具备（面向队列的）原子新增和删除能力，并且在此基础上提供发布/订阅功能。在推模式分布式锁中，实例和存储服务的结构如下图所示：        如上图所示，推模式分布式锁在获取锁时会使用enQueue将获取锁的线程以节点的形式加入到同步队列中，通过addListener来监听节点事件。释放锁时，会使用deQueue将节点从队列中删除，通过notifyEvent发布节点事件。存储服务提供的上述功能以及描述，如下表所示： 名称 参数说明 功能 描述 enQueue(queue, Node) queue，锁对应的队列Node，需要入队的节点 原子入队 将获取锁的线程（以及实例信息）转换为节点Node，顺序保存到队列queue尾部，该过程需要保证原子性 deQueue(queue, Node) queue，锁对应的队列Node，需要出队的节点 原子出队 将队列queue中指定的节点Node出队，该过程需要保证原子性 addListener(queue, Node, Listener) queue，锁对应的队列Node，需要监听变更的节点Listener，监听器 新增监听 指定队列queue中的任意节点Node进行监听，当被监听的节点Node发生变化时，存储服务（相当于Broker）会回调监听者Listener，执行其预设的逻辑 notifyEvent(queue, Node) queue，锁对应的队列Node，发生事件的节点 通知事件 当节点Node发生变化，比如：修改或者删除，会发送事件。存储服务会将事件通知到节点Node相应的监听者Listener        每一个分布式锁，在存储服务上都会有一个队列与之对应。任意实例中的线程尝试获取锁，都会转换为节点加入到锁对应的队列中，那怎样才能定义成功获取到锁了呢？因为enQueue能够确保原子化的顺序入队，所以只要（当前线程对应的）节点为队列中的首节点，就表示该节点对应的实例（中的线程）获取到了锁。获取推模式分布式锁的流程如下图所示：        如上图所示，获取锁时的输入描述可以参考《拉模式的分布式锁-什么是拉模式》一节，这里不再赘述。可以看到推模式的分布式锁获取流程与（《Java并发编程的艺术》中的第5章中）独占式超时获取同步状态的流程是相似的，当然也是复杂的。        获取推模式分布式锁时，首先会将执行信息（包括：线程和实例）生成节点，然后通过enQueue将节点加入到锁对应的队列。生成节点的主要目的是能够让获取锁的竞争者进行排队，同时当轮到它们（被通知）出队时，能够按图索骥唤醒它们所对应的实例（或线程）。如果当前节点不是首节点，代表该节点对应的线程没有获取到锁，从而需要通过addListener来监听它的前驱节点，因为前驱节点的变更（包括：获取锁超时、失败或者成功后的删除）都会以事件的形式通知到注册的监听逻辑Listener，通知会唤醒处于等待的节点，而节点对应的线程会执行注册的监听逻辑。        获取锁失败的节点会进入等待状态，而被唤醒后执行的监听逻辑主要是判断节点自身在队列中的位置，如果当前节点是首节点那就表示成功获取到了锁，否则将会做超时判断，如果没有超时，会再次进入等待状态。如果获取锁超时（或被中断），则会将当前节点通过deQueue进行出队，并将节点删除事件使用notifyEvent通知到节点的Listener。        每一个尝试获取锁的线程（以及实例）都会以节点形式穿在锁对应的队列上，除首节点外的任意节点都在监听其前驱节点的变化。释放锁时，会将对应的节点从队列中删除，并通知后继节点，这种击鼓传花的方式我们已经在单机锁释放过程中看到过，释放推模式分布式锁的流程如下图所示：        如上图所示，当前线程（以及实例）对应的节点为首节点时，就可以释放锁。释放锁时主要包含两个操作：节点出队和通知节点删除事件，前者会将当前节点从存储服务的队列中删除，后者会将删除事件通知到该节点的监听器。随着节点删除事件的发布，后继节点会被唤醒，而后继节点对应的线程将会尝试再次获取锁。 ZooKeeper如何实现推模式分布式锁        ZooKeeper（以下简称：ZK）是由雅虎研究院发起的一个项目，后被捐献给Apache，旨在为分布式系统提供可靠的协作处理功能。ZK作为一个树型的数据库，除了支持原子化的节点操作，还具备监听节点变更和变更事件通知的能力，因此它非常适合作为推模式分布式锁的存储服务。        推模式分布式锁对存储服务的诉求主要包括：原子出入队、新增监听和通知事件，共4个操作，ZK并没有直接提供这些API，那么它是如何满足推模式分布式锁对于存储服务的诉求呢？在介绍如何之前，先来快速过一下ZK。 ZooKeeper简介        ZK是一个C/S架构的系统，用户可以使用多种客户端来进行操作，客户端不仅限于应用使用的Jar包，也包括ZK提供的脚本等。这些客户端通过与ZK服务端之间的长链接进行通信，服务端一般以集群的形式提供服务，而处于集群中的ZK节点有不同的身份类型，包括：Leader、Follower和Observer，而其中只有Leader能够执行写操作，所以ZK对外的服务是能够保证顺序一致性的。ZK客户端与服务端集群的结构如下图所示：        如上图所示，多个不同种类的客户端（包括：ZK自带的zkCli脚本和依赖ZK客户端的应用）会和ZK集群中的实例建立连接，该连接被称为会话（session）。会话通过心跳（也就是客户端和服务端之间连接上定时的Ping-Pong）来保持，当客户端进程停止或心跳中断时，会话就会终止。集群中的ZK实例通过选举产生出一个Leader，数据变更会从Leader向另外两种身份类型的实例同步，客户端会与集群中的任意实例建立链接，如果客户端向一个Follower实例发送写请求，该请求会被委派给Leader。 zkCli脚本通过执行类似shell命令的方式来访问服务端，依赖ZK客户端的应用则需要基于SDK编程，虽然形式不同，但本质是一样的。ZK的选举以及数据更新是通过ZAB协议实现的，这里不做展开。        许多知名项目，比如：Apache HBase、Apache Solr以及Apache Dubbo等都使用ZK来存储元数据（或配置）。由于配置项会在运行时更改，所以ZK支持监听配置项（即节点）的变更，应用可以通过使用ZK客户端来监听某个节点，当节点发生变化时，ZK会以事件的形式通知应用。这些配置项在ZK内部都会以节点（即ZNode）的形式存在，而节点之间会以树的形式来组织，这棵树就如同Linux文件系统中的路径一样，其根节点为/，它的存储结构示意如下图所示：        如上图所示，这是一个层高为4的树，而对任意节点的访问需要给出全路径，比如：/军舰/052/052D，新增节点也相同，比如：/军舰/055/055A。节点除了包含路径以外，还可以保存值，同时节点有多种类型，节点（部分）类型、描述以及客户端命令示例如下表所示： 类型 描述 生命周期 zkCli命令示例 持久化（PERSISTENT） 通过指定全路径可以创建该类型节点 持久，节点新增后（非删除）会一直存在，纵使创建节点的会话终止，节点也不会删除 新增：create /军舰/055/055A value，表示创建指定的路径节点，且节点的值为value查看：get /军舰/055/055A，表示获取指定路径节点的值 持久化有序（PERSISTENT_SEQUENTIAL） 通过指定路径模式来创建该类型节点，节点全路径为路径模式+自增ID 同持久化类型 新增：create -s /军舰/052/052D/052D，按照路径模式创建有序节点，实际创建的节点全路径可能是：/军舰/052/052D/052D0000000000 临时（EPHEMERAL） 同持久化类型 会话，节点新增后，如果创建节点的会话终止，节点会自动删除 新增：create -e /军舰/055/055A，-e选项表示节点类型为临时 临时有序（EPHEMERAL_SEQUENTIAL） 同持久化有序类型 同临时类型 新增：create -se /军舰/052/052D/052D        如上表所示，从创建节点的角度看，ZK的节点类型主要就2类，一类是指定全路径进行创建，另一类是则是指定全路径的前缀模式。将生命周期这一维度与其相交，就会有4种类型，而实现推模式分布式锁，就需要使用到临时有序类型的节点。        访问ZK的官网可以下载ZK服务器，解压缩后，可以在bin目录下找到zkServer和zkCli脚本。前者是ZK服务器脚本，在类Unix系统下，可以通过zkServer.sh start 在本地启动一个单机版的ZK实例。后者是ZK客户端脚本，如果需要操作本地ZK，可以通过zkCli.sh -server localhost 2181连接到ZK实例上。 需要在conf目录中准备一个zoo.cfg配置，可以简单的拷贝该目录中的示例配置zoo_sample.cfg。        接下来通过启动两个zkCli客户端（分别命名为客户端A和B）来演示一下实现推模式分布式锁会用到的ZK操作，演示过程主要包括客户端各自创建节点，客户端A监听客户端B创建的节点变更。假定已经存在节点/member-123，在该节点下，客户端A和B分别创建三个前缀为/member-123/lock的临时有序节点，如下图所示（左边为客户端A）：        如上图所示，通过执行create -es /member-123/lock，能够在 /member-123节点下创建前缀为lock的临时有序子节点。通过ls /member-123命令，可以列出该节点下所有子节点，能够看到 /member-123拥有6个子节点，节点名称为lock与自增ID的拼接。并发创建节点的请求能够被有序且安全的创建。        接下来客户端A监听客户端B创建的 /member-123/lock0000000003节点，监听的方式可以使用get -w $node_path命令，该命令能够获取节点的内容，并在节点变更时收到通知。然后客户端B通过执行delete命令删除了对应的节点，该过程如下图所示：        如上图所示，当客户端B删除了 /member-123/lock0000000003节点后，客户端A收到了ZK实例的事件通知，该通知表示所监听的节点被删除了。因为创建的子节点均为临时有序类型，所以当客户端退出，会话终止后，由会话创建的（临时类型）节点都会被删除。        最后，客户端A监听客户端B创建的另外两个节点，然后将客户端B退出，再观察通知情况，该过程如下图所示：        如上图所示，随着客户端B的退出，客户端A收到了两个节点的删除事件通知。从上述演示可以看到，ZK支持节点的创建、访问、列表、监听和通知，而这些特性可以被用来实现推模式分布式锁。 如何实现队列操作        ZK兑现原子出入队功能的方式是支持enQueue和deQueue操作，实际就是需要提供线程安全的分布式队列服务。不同客户端并发获取锁的请求都需要在这个分布式队列中排队，使用ZK该如何实现呢？答案是使用临时有序类型节点来构建（同步）队列，以前文中的演示为例，其结构如下图所示：        如上图所示，可以看到在根节点下有多个（位于第2层的）子节点，这些节点可以被看作是不同的分布式锁，而它们的名称可以是业务类型+业务主键的形式，比如：对主键为123的会员上锁，这个节点（或者锁）的全路径就是 /member-123。如果客户端要获取相应的锁，就需要在锁节点下创建临时有序类型的子节点，图中的客户端A至E均尝试获取会员123的锁，这些获取锁的请求会在 /member-123节点下创建（第3层的）多个子节点。由于是顺序创建，所以这些节点可以被视作一个线程安全的队列，其中编号最小的子节点为头节点，新获取锁的请求会以大编号节点的形式由尾部添加。        当进行enQueue操作时，以获取会员123的锁为例，使用create -es /member-123/lock命令创建一个临时有序节点即可，由于新增节点ID会全局自增，所以创建的节点自然就会在队尾。创建节点命令会返回节点的全路径，当进行deQueue操作时，可以使用delete命令删除节点。        不同客户端并发获取锁的请求会在这个队列中排队，该队列的底层实现更像一个数组，因为数组的下标会全局自增，而节点之间没有相互指向的引用。那怎么判断排队中的节点获取到锁了呢？可以认为如果一个节点没有前驱节点，即为首节点时，代表它获取到了锁。由于ZK没有提供获取首节点的API，所以只能变相的通过获取全部子节点，然后判断自身在子节点数组中的下标是否最小来完成。 如何实现节点监听通知        ZK要兑现新增监听和通知事件功能，就需要支持监听节点以及变更通知，在之前的演示中，可以看到ZK是能够支持监听节点变更，并在节点发生变更时通知监听者的。当通过enQueue新入队一个节点时，如果该节点不是首节点，则需要监听它的前驱节点。在任意时刻去看队列里各节点之间的监听关系，会发现它们是链式的，以前文中的演示为例，如下图所示：        如上图所示，客户端进程A至E共创建了6个节点，每个节点在入队后，该节点在队列中的位置（即下标）也就确定了，这时还需要获取member-123节点下的所有子节点，根据下标找到前驱节点进行监听。        当获取到锁的节点（即首节点）执行完同步操作，就可以释放锁，释放锁会将该节点删除，而删除操作会以事件的形式通知到后继节点注册的监听逻辑。监听逻辑就是获取锁的逻辑，该逻辑会先获取锁节点下的全部子节点，如果当前节点为首节点，则获取锁成功，否则将会对前驱节点进行再次监听。为什么需要再次监听呢？节点入队时不是已经设置了吗？原因在于节点的删除不只是由于锁的释放，也有可能是客户端进程崩溃或重启所致。        在分布式环境中，客户端进程可能随时会重启，也可能会由于各种原因而突然崩溃，当客户端进程终止时，需要其创建的节点能被自动删除，否则同步队列中就会出现僵尸节点，使得通知链路断掉，导致锁的可用性无法保证。ZK的临时有序节点，就能很好的解决这个问题，因为一旦客户端进程退出，它和ZK之间的会话就会终止，而它创建的（临时）节点就会被ZK自动删除。考虑到节点的删除不一定发生在队首，就需要支持再次监听的逻辑，该过程如下图所示：        如上图所示，当客户端进程C退出后，它与ZK之间的会话就会随之终止，而它创建的lock003和lock004节点会被自动删除。lock004节点的删除事件会通知到客户端进程D，客户端进程D执行监听逻辑时会将监听对象由lock004改为lock002。 Curator分布式锁        使用ZK来构建分布式锁，肯定不能选择使用zkCli脚本，而是需要依赖ZK客户端进行编程。ZK原生客户端使用起来不是很方便，而网飞开源（并捐赠给Apache）的Curator项目很好的提升了使用体验，该项目不仅支持流式API来简化使用，还提供了诸如：选举、分布式锁和服务注册与发现等多种功能组件（Recipes子项目），对部分分布式问题场景做到了开箱即用。        通过依赖curator-recipes坐标，可以将其分布式锁组件引入到项目中，依赖如下： org.apache.curator curator-recipes 本书使用的Curator版本为：5.2.0。        在使用Curator分布式锁之前，需要先构造CuratorFramework，该接口是Curator框架的入口，创建代码如下所示： CuratorFramework curatorFramework = CuratorFrameworkFactory.builder() .connectString(connectString) .connectionTimeoutMs(connectionTimeoutMs) .sessionTimeoutMs(sessionTimeoutMs) .retryPolicy(new ExponentialBackoffRetry(baseSleepTimeMs, maxRetries)) .build(); curatorFramework.start();        CuratorFramework需要调用start方法完成启动后方可使用。通过CuratorFrameworkFactory创建CuratorFramework时，需要设置若干参数，上述代码中的参数以及描述如下表所示： 参数名称 描述 connectString ZK服务端地址，包括：IP和端口 connectionTimeoutMs 连接超时时间 sessionTimeoutMs 会话超时时间 baseSleepTimeMs 失败重试策略重试间隔时间 maxRetries 失败重试策略最大重试次数        Curator分布式锁提供了多种实现，包括互斥的分布式锁InterProcessMutex，以及分布式读写锁InterProcessReadWriteLock等。以InterProcessMutex为例，示例代码如下所示： InterProcessMutex lock = new InterProcessMutex(curatorFramework, \"/member-123\"); lock.acquire(5, TimeUnit.MINUTES); try { // 执行同步逻辑 } finally { lock.release(); }        可以看到Curator分布式锁对操作ZK的细节做了很好的封装，它不仅有良好的使用体验，还隐藏了推模式分布式锁复杂的逻辑。 ZooKeeper分布式锁实现        推模式分布式锁的实现要比拉模式复杂，出于可靠性与难易度的考虑，可以将Curator分布式锁适配到LockRemoteResource接口。因为InterProcessMutex已经提供了与锁操作的相关方法，所以适配过程非常简单，适配实现为ZooKeeperLockRemoteResource。以获取锁为例，代码如下所示： public AcquireResult tryAcquire(String resourceName, String resourceValue, long waitTime, TimeUnit timeUnit) throws InterruptedException { InterProcessMutex lock = lockRepo.computeIfAbsent(resourceName, rn -> new InterProcessMutex(curatorFramework, \"/\" + rn)); AcquireResultBuilder acquireResultBuilder; try { boolean ret = lock.acquire(waitTime, timeUnit); acquireResultBuilder = new AcquireResultBuilder(ret); if (!ret) { acquireResultBuilder.failureType(AcquireResult.FailureType.TIME_OUT); } return acquireResultBuilder.build(); } catch (Exception ex) { throw new RuntimeException(\"acquire zk lock got exception.\", ex); } }        可以看到获取锁时，首先从lockRepo中获取锁资源resourceName对应的InterProcessMutex，然后调用它获取锁，并将调用结果适配为AcquireResult返回。需要注意的是，每个创建出来的InterProcessMutex都会被认为是一个独立的锁实例（纵使它的路径是相同的），如果在每次调用tryAcquire方法时都创建InterProcessMutex，结果就是各用各锁，起不到并发控制的作用，锁的正确性也无法保证，因此需要将锁资源与创建出来的InterProcessMutex缓存起来使用。 类型为ConcurrentHashMap的lockRepo缓存了resourceName与InterProcessMutex。        释放锁的代码也很简单，这里不再赘述，如果感兴趣可以查看分布式锁项目中的distribute-lock-zookeeper-support模块。 ZooKeeper分布式锁存在的问题        ZK是一款典型的CP型存储，能够提供高可用以及顺序一致性的保证，因此基于它实现的分布式锁也会具备良好的可用性和正确性，但并不代表它实现的分布式锁就没有弱点，在性能和正确性上，ZK分布式锁就存在一些问题。        首先是性能问题，主要有两点：一是I/O交互多，二是ZK自身的读写能力一般。以客户端一次获取锁的过程为例：需要新增节点、获取子节点列表以及新增监听等多次对ZK的调用，而这些调用不是并行的，是存在顺序依赖的。与Redis分布式锁的一次SET命令相比，ZK分布式锁交互次数变得更多，开销也比较大。当ZK处理新增节点请求时，需要将数据变更同步到ZK集群中的Follower节点才能返回，虽然同步过程有优化，只需要等待超过半数的Follower同步成功即可，但这种为了确保一致性的同步机制，还是在性能上却有所损失。        其次是正确性问题，也就是还会存在多个实例能够同时获取到锁的情况。ZK能够在分布式环境中保证一致性，而分布式锁正确性的本质其实也就是多个实例对于资源状态需要有一致的视图，从这点来说分布式锁的正确性和存储服务的一致性是正相关的。即然ZK能够保证一致性，为什么ZK分布式锁还会出现正确性问题呢？原因就在于ZK会话存活的实现机制。        ZK分布式锁依靠临时有序节点来避免由于客户端实例宕机导致的可用性问题，因为一旦客户端进程崩溃，它和ZK之间的会话就会终止，进而它创建的节点就会被自动删除。临时节点的存在与否和会话是相关的，而ZK检测会话是否存活的方式是通过（定时）心跳来实现的，如果客户端与ZK实例之间的心跳出现（了一段时间的）中断，ZK会认为客户端可能出现了问题，从而将它们之间的会话终止。 通过心跳来判断存活是分布式环境中常用的策略，但心跳中断的原因不一定是对端崩溃，也有可能是对端负载过高、进程暂停或网络延迟所致，因此心跳没有问题表示对端一定存活，心跳出现问题则表示对端可能终止。        以Java应用为例，如果客户端获取到锁，而在执行同步逻辑时由于负载过高（网络请求堆积）引起心跳中断，则会可能导致ZK分布式锁对于正确性的保证失效，该过程如下图所示：        如上图所示，客户端A成功获取到锁，然后开始执行锁保护的同步逻辑。此时客户端B尝试获取锁，该过程会创建节点B，由于不是首节点，所以获取锁失败，进入等待状态。客户端A执行同步逻辑时（由于GC暂停或同步逻辑出现高消耗操作导致）负载飙高，它和ZK之间的心跳处理不及时，导致会话终止。客户端A与ZK之间的会话终止使得节点A被自动删除，由于节点B监听节点A的变化，会收到节点A的删除通知，而该通知会唤醒客户端B，使之重新尝试获取锁。        客户端B尝试获取锁，此时节点B已经是首节点，因此客户端B能够成功获取到锁并开始执行同步逻辑。假如此时客户端A从负载高的桎梏中恢复过来，开始继续执行同步逻辑，那原本被锁保护的同步逻辑就被并发执行了，导致锁的正确性被违反。当然可以通过修改心跳配置来使得客户端A与ZK之间的会话不会很快终止，由此一定程度上避免出现该问题，但该问题的理论模型依旧存在。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-with-redis.html":{"url":"book/distribute-lock-with-redis.html","title":"热风-使用Redis实现分布式锁","keywords":"","body":"使用Redis实现分布式锁        本文是在Redis官方文档：https://redis.io/topics/distlock基础上进行翻译和完善。        分布式锁在很多环境中是一种非常有用的原语级别的并发控制设备，它能够为多进程提供一种基于共享资源的排他式工作方式。有许多的库和文章都在介绍如何使用Redis来实现分布式锁，但是每个库都用了不同的方式实现，并且实现方式或多或少都有些小问题。        这篇文档目的是介绍如何能够更好的使用Redis来实现分布式锁，并且提出了一个算法，叫做：Redlock，它能够实现我们认为更加安全可靠的分布式锁。同时也希望社区能够分析它，给出一些反馈。 正确的实现一个单节点Redis的分布式锁        设计分布式锁，至少要考虑的三个点： 安全属性（正确性）：排他，任意时间一个客户端获取到锁； 活性A：不存在死锁（可用性），任何时刻都能够尝试去获取锁，总是获取到锁的客户端挂了； 活性B：失败容忍性（可用性），如果大多数的redis节点存在，就能提供锁服务。 为什么基于FailOver的实现存在问题        为了能够明白我们希望提升什么，我们分析一下现有基于Redis的分布式锁。        使用Redis来制作一个分布式锁的最简单方式是通过在一个Redis实例中创建一个KEY，一般这个KEY会设置一个TTL，使用了Redis数据过期的特性，而这将避免出现死锁。当客户端需要释放锁时，只需要删除这个KEY即可。        表面上看起来它能够正常工作，实际也是如此，但是这种方式存在一个问题：它是单点架构。如果Redis实例挂掉怎么办？这将会违反可用性的要求，我们可以通过Redis的主从模式集群来提升可用性，但很不幸，这并不奏效。因为Redis的主从复制是异步的，而当主Redis挂掉后，从Redis的数据KEY存在延迟（或少量丢失），这使得分布式锁的正确性受到影响。        这种方式会有明显的竞争问题： ClientA从主Redis获取到锁 主Redis崩溃，但KEY还在向从Redis同步中 从Redis晋升为主 ClientB尝试获取锁，并获取到        该过程为违反了分布式锁的正确性。如果在这种不常出现的特定情况下，可以允许短时出现多个客户端在同一时刻占据一把锁的情况发生，那可以选择使用这种基于主从复制的Redis集群来提升可用性。一般在数据库一层或多或少都存在唯一约束的保护，所以在出现问题时，需要数据库承担一定的并发控制工作，这会在短时造成回滚（回滚数量多少视锁的竞争程度而定），需要使用者能够做好评估。否则我们建议按照文档中提到的解决方案来实现分布式锁。 单节点Redis分布式锁正确的实现方式        在介绍克服单点问题的方案之前，先来看一下在单个实例下，如何正确的实现一个分布式锁，事实上，这是一个可行的解决方案，而这种基于单个Redis节点的分布式锁实现方式将会演绎出多个Redis节点的分布式锁实现方式。        为了获取锁，客户端一般会使用如下命令： SET resource_name my_random_value NX PX 30000        该命令通过NX选项，确保只有在KEY，也就是resource_name不存在的情况下才能设置，同时PX选项表示该KEY将会在30000毫秒，也就是30秒后过期，而my_random_value需要在同一时刻做到所有的客户端唯一。        为什么要随机值，且需要保证在同一时刻所有客户端唯一，目的是能够保证安全（通过CAS方式）的释放锁，在Redis命令中，可以通过一段Lua脚本来实现，脚本如下： if redis.call(\"get\", KEYS[1]) == ARGV[1] then return redis.call(\"del\", KEYS[1]) else return 0 end        如果直接通过简单的DEL命令删除KEY来释放锁，会出现其他获取到锁的客户端删除锁，导致分布式锁的正确性被破坏。通过Lua脚本的方式，保证只有客户端传递的值与当前Redis存储的KEY值相等时，才允许删除当前KEY，通过这种方式实现了CompareAndDelete，从而确保不会出现KEY被误删的情况。 如果使用阿里云的RDB缓存服务，可以不使用上述脚本，因为它提供了cad扩展命令。        这个随机字符串应该怎样怎样获得呢？可以通过使用/dev/urandom产生的随机数据流，但也可以用其他更简单的方式来完成这个工作。比如：通过使用机器信息加上Unix毫秒时间戳的拼接来生成一个字符串，虽然它不一定可靠，但是对于这个场景是足够了。        我们设定的TTL时间可以被称为锁有效时间。它不仅是自动释放的时间，也是其他客户端能够获取到锁之前执行操作的最长时间。现在位置我们有了一个不错的获取和释放锁的方式。当前是一个单点Redis，如果它一直很可靠，该系统也能够正常运作。接下来我们会将这个概念延伸到分布式系统，通过多个Redis节点组成的分布式锁来提升可用性。 Redlock算法        该算法的分布式版本我们假设有N个Redis主节点，这些节点都是相互独立的，不使用主从复制集群或其他隐含的服务发现系统。我们已经讨论过如何在单节点Redis的环境下进行正确的锁获取和释放操作。我们依旧采用这种方式在单个Redis节点上获取和释放锁，在接下来的算法示例中，假设N为5，这是一个合理的值，所以我们需要在不同的计算机或虚拟机上部署5个Redis示例，理论上它们不会在同一时刻挂掉。        为了获取锁，客户端会按照如下的行为进行操作。 获取当前的时间（毫秒）； 接下来针对N个Redis实例使用相同的KEY和随机值顺序获取锁。对于IO超时时间可以设置一个足够短的时间，比如：如果过期时间是10秒，则超时时间可以设置在5到50毫秒的范围，目的是能够在Redis实例挂掉的时候能够快速反应，使得能够与下一个Redis尽快完成交互； 如果客户端的耗时，从第一步到N个实例都调用完成，如果不超过锁的过期时间，并且在获取到锁的Redis实例数量大于等于3个，则认为该客户端获取到了锁； 如果锁获取到了，那么这个分布式锁的占用时间可以认为是初始的过期时间，比如：10秒，减去获取锁的耗时； 如果客户端没有获取到锁，比如：获取到锁的实例数量是少数，获取在步骤3中的耗时已经超过了锁的过期时间，客户端将会选择释放所有节点的锁，纵使客户端它没有获取到对应节点的锁，这出于简单性的考虑。 该算法是异步的吗？        该算法假设在整个分布式系统中不存在同步时钟，但是依旧认为所有的计算节点拥有相近的时间，并且以基本一致的频率跳动。这个假设很像真实的计算机世界，每台计算机都有一个本地始终，我们各自依赖它，所幸它们之间的时间差很小，小到可以忽略。        在这点上我们可以在算法规则上作出一些调整，可以认为获取到锁的实例拥有锁的时间是在超时时间减去获取锁耗时的基础上，再减去若干毫秒用于补偿上述的时间差。 失败重试        当客户端无法获取到锁时，它应该尝试随机等待一下，然后再次尝试获取，由于是获取到多数的Redis实例，所以如果以相同频率再次获取，可能多个客户端会进入没有人能赢的尴尬局面。所以越快醒来的客户端，就更有机会获取到锁，同时客户端应该尝试使用多路复用将SET命令同时发往N个实例。        值得注意的是，如果客户端获取到少数的Redis实例锁，在获取分布式锁失败的前提下，释放获得的锁也是很重要的。因为它会使得对分布式锁的获取可以立刻继续，而不用等待到过期，但如果发生网络问题或者客户端与Redis实例断开链接，那就要承担这个影响。 释放锁        释放锁非常简单，只需要对所有Redis实例进行释放即可，不需要考虑当前客户端是否获取到锁。 正确性保证        这个算法的正确性有保证吗？我们尝试从多个不同场景来理解它。        在开始之前，我们假设客户端已经获取到了多数Redis实例的锁，这时，所有的多数Redis实例上都会具有一个KEY且设定了相同的TTL，也就是过期时间。但是这些KEY是在不同时间设置到实例上的，所以这些KEY也会在不同的时间过期。假设第一个设置的KEY在T1（发送命令给Redis实例前），最后一个设置的KEY在T2（Redis实例返回响应之后），我们可以肯定第一个KEY过期的时长至少在MIN_VALIDITY = TTL - (T2 - T1) - 时钟差。我们在当前这个时刻设置了KEY，能够确保其他实例上的KEY一定比这个时长更晚之后才会过期。        在多数实例的KEY被设置后，其他的客户端将无法获取到锁，因为其他客户端无法完成设置多数实例的KEY，毕竟之前的KEY在多数的Redis实例上存在，这是满足锁的排他性的。        我们需要保证多个客户端不能同时获取到锁，也就是维护锁的排他性语义，这是确保正确性的基础。如果客户端获取到多数Redis实例的锁，花费的时间大于TTL，则考虑获取锁失败，并释放锁。因此我们仅需要考虑客户端在TTL之内获取到锁的情况。在这种情况下，MIN_VALIDITY内，其他客户端不能够获取到锁，在这个时间内，锁是安全的。 可靠性保证        系统的可用性主要基于以下三点： 通过过期时间来保证锁的自动释放，最终锁一定能够能够被获取； 事实上对于大多数释放锁的情况都是要么获取或者没有获取到锁，客户端主动的释放锁，而等待超时到自动释放的情况非常少； 客户端等待重试获取锁的时间要远比获取多数锁所花费的时间长，这样使得多个客户端出现无效竞争的几率很低。        但是在网络分区或者出现问题时，将会付出需要等待KEY过期时间才能继续获取锁的代价，这种使用KEY的过期时间来做到避免死锁的方式，其可靠性和网络环境的可靠性基本一致。 性能与故障恢复        许多将Redis用作锁服务器的用户在获取和释放锁的延迟以及每秒可以执行的获取/释放操作数量方面都需要高性能。为了满足这一要求，与N个Redis节点的通信应该采用多路复用（即使用非阻塞IO的方式进行通信，将请求全部发出，然后等待返回）。        如果我们关注一个可以恢复的系统结构，那么还需要关注持久化。        可以看到的问题就是如果没有配置Redis的持久化。一个客户端获取到了多数（3/5）Redis的锁资源，但其中一个拥有锁资源的Redis实例被重启，此时另一个客户端发起获取锁的操作，就有概率形成多数，这使得锁的正确性没有得到保证。        如果我们启用AOF持久性，情况会有很大的改善。例如，我们可以通过发送SHUTDOWN并重新启动服务器来升级服务器。由于Redis过期是在语义上实现的，因此服务器关闭时几乎时间仍然流逝，这不会产生任何问题。只要是正常的启停，一切都很好，但如果实例断电怎么办？如果默认情况下，Redis配置为每秒钟在磁盘上fsync，重新启动后我们的部分KEY可能会丢失。当然我们为了要保证能够面对任何情况的实例重启行为，可以在持久化配置中开启fsync=always，但这会极大的损伤性能，因为这和使用传统关系数据库来实现分布式锁差别不大了。        然而，事情比乍一看要好。只要实例在崩溃后重新启动，它就保留了算法安全性，因为它不再参与任何当前获取锁的活动，其只保留了旧有的锁信息。因此实例重新启动时的当前锁的集合都是通过锁定正在重启的Redis实例以外的实例获得的。        为了使上述的内容更加可靠，我可以将一个Redis实例的重启时间适当延长，长到超过我们设置的TTL，这样当它恢复后重新加入集群，所有在该实例上锁的KEY都已经过期，使之不回产生副作用。使用重启延迟的方式可以使得在没有足够Redis持久化保障的前提下，能够提供当前解法以足够的可靠性保障。但如果一个集群中的多数Redis在同一时刻都挂掉，这将会使整个系统陷入不可用的状态。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-how-to-do-it.html":{"url":"book/distribute-lock-how-to-do-it.html","title":"热风-如何实现分布式锁","keywords":"","body":"如何实现分布式锁        本文是Martin Kleppmann论证Redlock难以确保分布式锁正确性的文章，文章原文在这里。        Redlock算法看起来是一个不错且高可用的分布式锁实现，但是通过仔细的推敲，现实远比理想复杂，它并不是一个可靠的分布式锁实现，文中详细的阐述其问题所在，并对于分布式锁给予了自己的见解。整体文章偏学术化，严谨以及精确的表述让人读起来很惬意。Redlock算法的描述可以看我翻译的文档。        以下是笔者对这篇文章的（摘录）翻译和理解。如果是笔者的理解将会以来开头，以引言的形式来展示，比如： 这个是作者的理解。。。        描述内容示例。。。        作为我书研究的一部分，我在Redis网站上发现了一种名为Redlock的算法。该算法声称在Redis上实现容错分布式锁，页面要求进入分布式系统的人提供反馈。该算法本能地在我脑海中敲响了一些警钟，所以我花了一些时间思考并写下这些笔记。        由于Redlock已经有超过10个独立实现，并且我们不知道谁已经在依赖此算法，我认为值得公开分享我的笔记。我不会讨论Redis的其他方面，其中一些已经在其他地方受到了批评。        在我讨论Redlock的细节之前，请允许我说，我非常喜欢Redis，我过去曾在生产中成功使用它。我认为这非常适合您想在服务器之间共享一些瞬时、非精确、快速变化的数据，如果您出于任何原因偶尔丢失这些数据，这没什么大不了的。例如：可以将启用在请求计数器上，用于记录每个IP访问的次数，或者记录用户使用的IP列表，用来检测登录异常。但是 Redis被越来越多用在数据管理领域，而该领域数据强一致性和可靠性要求而著称，这显然不是Redis擅长的。分布式锁就是属于这个领域的内容，我接来下详细展开。 使用分布式锁来干什么？        使用分布式锁的目的是为了能够在进行相同工作的多个节点中确保只有一个节点在同一时刻处理工作。这个工作可能是向一个共享存储写入数据或进行一些计算，也有可能是调用外部的API。在更高的层面，在分布式环境中使用锁，一般会有两个原因：要么是为了效率，要么是为了正确性。为了能够区分你是哪一种，你需要问自己，如果锁失败，你的场景会出现什么问题呢？        效率：使用锁可以避免不必要的两次相同工作（例如一些昂贵的计算）。如果锁失败，两个节点最终完成相同的工作，结果是成本略有增加（您最终向AWS支付的费用比平时多5美分）或小的不便（例如：用户最终收到两封相同的电子邮件）。        正确性：使用锁可以防止进程并发的处理任务，从而导致互相影响，最终把系统搞乱。如果锁失败，两个并行的节点会在同一行数据上工作，结果要么是文件被弄乱，数据丢失或混乱，好比医药管理员给病人发错了药，甚至会引起严重的问题。        上述两个使用分布式锁的原因都是合情合理，但是你需要清楚你的原因究竟是哪一个。我想指出如果仅仅是因为解决效率问题，那完全没有必要选择成本昂贵的Redlock，运行5个Redis服务器来提供分布式锁服务。你最好使用一个单Redis实例来解决你的问题，如果好的话可以使用一个支持主从的Redis集群。        如果使用单个Redis实例，当Redis实例出现掉电或其他故障，可能会使部分锁失效。但是如果出于效率问题使用分布式锁，短时的失效是可以容忍的，而且Redis实例并不会经常崩溃。对于使用单节点Redis分布式锁的系统，使用者会认为这个锁不能够提供完全可靠的分布式锁服务，这样会给使用者一个暗示。        另一方面，Redlock算法及其5个副本和多数票，乍一看，它似乎适合解决分布式锁用于应对正确性的场景。我将在下面的章节中论证，它并不适合解决正确性的问题。在本文的其余部分，我们将假设您的锁很关注正确性，如果两个不同的节点同时认为它们持有相同的锁，这是一个严重的错误。 使用锁来保护资源的访问        我们暂时把Redlock放到一边，先讨论分布式锁的一般使用方式（独立于使用的某些算法）。重要的是要记住，分布式系统中的锁不像多线程应用程序中的互斥锁，这是一只更复杂的野兽，因为不同的节点和网络都可能以各种方式出现故障。        举个例子，假设你有一个应用，它会修改共享存储（如：HDFS或S3）上的文件。客户端首先获取到锁，然后读取文件，作出一些更改，然后将修改后的文件写会存储，最后释放锁。这个锁能够确保两个客户端不会同时在一个文件上进行操作，如果这么做了，可能会丢失一些更新。上述代码如下所示： function writeData(filename, data) { var lock = lockService.acquireLock(filename); if (!lock) { throw 'Failed to acquire lock'; } try { var file = storage.readFile(filename); var updated = updateContents(file, data); storage.writeFile(filename, updated); } finally { lock.release(); } }        非常不幸，纵使你有一个完美的锁服务，这块代码依旧存在问题。下面的图示会演示你如何遇到被损坏的数据：        在这个例子中，客户端获取锁后，持有锁很长时间，比如：GC导致的暂停。分布式锁具有的超时，一般这会是一个好主意（毕竟如果一个客户端获取了锁，随后它崩溃了，这回导致锁一直不被释放）。然而在这种情况下，如果GC暂停时间超过锁的过期时间，并且客户端没有意识到它已经过期，如果此时锁已经被另一个客户端获取并完成了处理，那么当前客户端的继续执行就会造成问题。        这个Bug不是理论上的，HBase就曾发现有这个问题[3,4]。一般情况下GC的停顿很短，但是stop-the-world形式的GC有可能会造成长达数分钟的停顿，这足以长到锁超时。虽然以并发收集而著名Hotspot JVM的CMS回收策略，也不能做到回收与应用代码同时进行，它也需要stop-the-world。        你也无法通过在写会存储之前做持有锁的检查来解决这个问题。需要注意的是GC可以在任何时候暂停正在运行的线程，包括对你来说最可能发生的点（在检查和写入操作之间）。        如果你认为可以选择使用没有运行时（或者说GC停顿）的语言来避免面对这个问题，那你需要知道还有很多情况让你的进程暂停。也许你的进程试图读取一个尚未加载到内存中的地址，结果在加载它的时候出现了页失效，从而需要等待从磁盘中加载。也许你的磁盘是使用的亚马逊的EBS服务，因此在亚马逊拥堵的网络上，读取变量无意中变成了同步网络请求。也许还有许多其他进程争夺CPU，而您在调度器树中遇到了一个黑色节点。也许有人不小心把SIGSTOP发送到了该进程。总之，运行的进程被暂停是无法避免的。        如果你还是不相信进程会被暂停，那么可以想象对存储文件的写请求出现了延迟。在网络上的传输的包出现了延迟，比如：重传导致。在Github出现的一次网络故障中，在网络中传输的包被延迟了大约90秒[9]。这意味着应用实例在发送写请求时出现了延迟，但是锁已经过期释放，从而导致可能出现多个实例对存储的并行写。即使在管理良好的网络中，这种事情也可能发生。您根本无法对时间安排做出任何假设，这就是为什么无论您使用什么分布式锁，上述代码基本上都是不安全的原因。 使用令牌来提升锁的正确性保障        解决这个问题实际上也挺简单，只需要在向存储服务写时使用一个令牌即可，或者说乐观锁。在这个场景中，令牌可以是一个简单自增数字（通过锁服务提供），当获取锁时，如果获取成功，锁服务会返回一个令牌。该过程如下图所示：        Client1获取到了锁并同时得到了令牌33，但是在执行操作的时候，由于暂停而导致锁实质已经过期。Client2在Client1暂停期间（由于锁超时释放而）获取到了锁，同时获得了更新的令牌34，然后执行操作，将数据完成写入，也包括了令牌34。Client1随后恢复过来，然后开始尝试向存储写数据，同时包括之前获取到了令牌33。由于存储服务记得之前处理过一个令牌为34的请求，所以将会拒绝Client1的这次请求。        请注意，这要求存储服务能够支持令牌的检测，在发生令牌倒退时，拒绝写入请求。一旦你知道了诀窍，这并不特别难。如果锁服务生成严格单调增加的令牌，这样能够充分的确保分布式锁的正确性。例如：如果你使用ZooKeeper作为分布式锁的底层，那可以使用zxid或znode版本号作为令牌，并且它们能很好的工作。        回过头来看一下Redlock算法，它没有使用令牌来保证在出现进程暂停或者网络延迟时不出现问题。虽然这个算法看起来很不错，但是用起来不是那么安全，因为一旦出现了进程暂停或者网络延迟，客户端之间的数据竞争就会有出现的概率。        我没有完善Redlock算法，以期望其能够支持自增的令牌。这是有些难度的，如果使用随机值不会产生效果，而再一个Redis节点上完成自增计数可靠性又不高，因为这个节点可能失效，而在多个节点进行计算会导致需要进行同步，从而引入分布式共识算法来保证自增。 用时间来解决共识问题        Redlock没有使用令牌，这也表明在对锁的正确性有依赖的场景不适合使用Redlock。但是这里有更多的问题值得讨论。理论上讲，一个更具有实践性质的算法应该是类似基于失败检测的异步网络模型。简单的说，这意味着该算法对于时间（或者延迟）没有任何假设：进程被暂停任意时长，网络出现了任意时长的延迟，系统时钟之前出现了偏差，这些都不会影响到算法的正确性。鉴于我们之前讨论的问题，这是对于假设一个非常合理的要求。 其实Redlock的问题所在，就是对时间的假设，带来可用性的同时，降低了正确性（或一致性）。        使用时钟以及超时的分布式锁算法（比如：Redlock），其目的就是为了避免出现由于实例的故障导致死锁。但是超时往往不会那么精确，只是一次请求超时并不意味着对端机器（Redis实例）挂掉，因为这可能时网络上突然出现的抖动，或者本地时钟出现问题。使用超时作为故障检测的手段，只能获取一个大概的结果，也就是猜测对端可能出现了问题（如果可以的话，分布式算法完全没有时钟，但共识就不可能[10]。获取锁就像比较和设置操作，这需要达成共识[11]）。        需要注意Redis使用了gettimeofday函数，而非monotonic clock来判定是否过期。根据gettimeofday的说明文档表明，它返回的时间可能会出现跳转——也就是说，它可能会突然向前跳几分钟，甚至跳回时间（例如：如果时钟因与NTP服务器差异太大而由NTP介入，或者如果时钟由管理员手动调整）。因此，如果系统时钟正在做奇怪的事情，很容易发生Redis中KEY的过期速度比预期要快得多或慢得多的问题。 monotonic clock是以系统启动时间开始计算的，它不受时钟调整的影响，是一个严格单调递增的时钟序列。        对于异步网络模型中的分布式算法来说，这不是一个大问题：这些算法通常确保其正确性，而不做出任何关于时间的假设[12]。只有可用性要求依赖于超时或其他故障检测器。简单的说，这意味着即使系统中的计时到处都是（进程暂停、网络延迟、时钟向前和向后跳跃）问题，就算算法的性能糟透了，但算法永远不会做出错误的决定。        然而Redlock不是这样的，其正确性取决于它对于时间的多个假设：它假设所有Redis节点上的相同KEY在过期时间上大致相同；网络延迟相对于过期时间短很多；进程暂停比过期时间短得多。 使用糟糕的延迟让Redlock失效        让我们看一些例子来证明Redlock对时间假设的依赖。假设系统有五个Redis节点（A、B、C、D和E）和两个客户端（1和2）。如果其中一个Redis节点上的时钟向前跳动会发生什么？ 客户端1获取了节点A，B，C，由于网络问题，D和E不可达 在节点C上的时钟出现前跳，导致C上的锁过期 客户端2获取了节点C，D，E，由于网络问题，A和B不可达 客户端1和2，在这一时刻，都获得了锁        如果节点C在将锁持久化之前崩溃了，并重新启动，也可能会发生上述问题。在Redlock算法文档中，建议节点重启时进行延迟，以期望锁能够过期，但是这种重启延迟还是讲问题抛给了时间，如果处理不当，还是会出现问题。当然，如果你认为时钟的跳跃是难以发生的，因为在数据中心里已经配置好了NTP，并保证其服务质量。基于这个假设，我们看一下如果进程暂停，是否也会导致失败： 客户端1在A，B，C，D，E节点上获取锁 Redis实例都返回获取成功，但响应在客户端1网口开始排队时，客户端1发生fullGC，出现了暂停 锁在所有的Redis实例上过期 客户端2获取A，B，C，D，E节点上的锁 客户端1的GC完成，并收到延迟后的响应，根据响应判断客户端1获取到了锁 客户端1和2都认为获取到了锁        虽然Redis时用C实现的，它不存在GC，但是对当前问题没有任何帮助，因为客户端可能是使用了有GC特性的语言编写的。如果想要在这种情况下保证正确性，就一定要在客户端2获取到锁后，客户端1不会执行相关操作，而解法就可以使用前面提到的令牌方式。网络延迟（TCP窗口排队）也可以产生与进程暂停相同的问题。 Redlock基于同步的假设        Redlock需要在同步系统模型的假设下才能正常工作，所谓同步系统模型有以下特性： 有界网络延迟（您可以保证数据包总是在有保证的最大延迟内到达） 有界进程暂停（换句话说就是强实时约束，就像汽车上的安全气囊一样及时） 有界时钟错误（祈祷不会从有问题的NTP服务端获取时间）        需要注意的是同步系统模型并不是意味着节点的系统时钟完全同步，只是意味着你假设了一个网络延迟、进程暂停以及时钟偏差的上限[12]。Redlock算法假设这些延迟和偏差要远小于过期时间（TTL），如果这个时间假设长于过期时间，该算法就会失效。        在网络环境很好的数据中心内，对于时间的假设基本上都能够满足，这就是部分同步网络[12]。但是这样就够了吗？只要时间的假设被推翻，Redlock算法就不能够保证其正确性，会出现一个客户端获取了一个过期（但被另一个客户端认为其持有的）锁。如果你需要锁具有最大程度的正确性，那么基本上这个假设就是不足够的，你需要的确定性的正确性。在同步网络中可以运行的模式或者假设在部分同步网络中不适用已经有了很多例子[7,8]。提醒自己不要忘了Github的90秒的包延迟问题[9]，同时Redlock很大可能无法通过Jepsen测试。        另一方面，使用了为部分同步网络（或者带有故障检测的异步网络）而设计的共识算法对解决这个问题是有帮助的。Raft、Viewstamped Replication、Zab和Paxos都属于这一类算法。这种算法必须放弃所有对于时间的假设。很容易假设网络、进程和时钟比实际更可靠，但在分布式系统这种不可靠的环境中，你必须非常谨慎地对待你的假设。 结论        我认为Redlock算法是一个糟糕的选择，因为它不伦不类，完全不必要的重量级解决方案以及昂贵的实施代价，但是花了这么大的代价却无法兑现足够的正确性保障。尤其是该算法作出了关于时间和系统时钟有危险性的假设，而这种假设一旦不被满足，将会对其正确性带来损失。此外，它还或缺一个令牌生成的装置，用来最终确保算法不会收到网络延迟和进程暂停的影响。        如果你只是关注锁的效率，我建议你可以直接使用单节点的Redis来实现 分布式锁，并且在文档中说明，该锁能够提供近似正确的分布式锁服务，可能存在偶尔的失败，而不是使用一个至少需要5个Redis节点的Redlock。        如果你关注锁的正确性，那就不要使用Redlock，使用一个适合的共识系统，例如：zookeeper，通过一些组件，比如：Curator的支持（甚至它已经实现了一个分布式锁）来实现分布式锁。还需要确保底层存储都支持令牌。 发消息怎么办，清理缓存怎么办，这些都是存储服务，让其底层支持令牌，不切实际。如果存储都支持令牌了，那么要锁干什么，其实看来这个令牌也没什么价值，只是把问题转嫁给了后续流程。        就像我开始说的，Redis如果用在正确的场景中，是一个非常出色的工具。上述所有的观点都没有降低Redis本身的成绩。Salvatore在Redis这个项目上工作了很多年，它的成功是理所因当的，但是任何工具都有它的局限性，识别它们以及用好它们更加重要。        如果你更深入的了解这些问题，我在自己的书中第8和第9章节做了更深入的阐述。如果想更深入的了解zookeeper，我推荐Junqueira and Reed的书[3]。对于分布式系统如果需要更好的了解，我推荐Cachin, Guerraoui 和 Rodrigues的书[13]。        谢谢 Kyle Kingsbury, Camille Fournier, Flavio Junqueira和Salvatore Sanfilippo的校审。        更新自2016年2月9日：Salvatore，Redlock的作者，也是Redis的作者，他发表了一篇文章来反驳我的观点。他有一些论点是不错的，但是我还是坚持我的结论。如果有时间我会更加详细的阐述这个问题，但是请你自己能够有主意，当然你可以参考我的引文，大多数都是严谨的学术文章，它们更加可信。 Martin Kleppmann在观点的阐明上做的很不错，抓住了锁的释放在Redlock中不是由锁的持有者来做的，而可能是通过超时做到的。抓住Redlock基于时间的假设一通猛打。 然后抬出了学术化的参考引用，力求对Salvatore形成绝杀，同时也给自己的书做了一些广告。 诚然认可Martin Kleppmann的观点，其本质就是Redlock在P的前提下，通过强化了A，从而导致C受到了影响，只要抓住P的问题可能出现，就一定能够打掉对方的观点。但是文中对于Redis超时实现方案的批评实属超出了分布式锁的范畴，显得没有必要。 参考引用 [1] Cary G Gray and David R Cheriton: “Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency,” at 12th ACM Symposium on Operating Systems Principles (SOSP), December 1989. doi:10.1145/74850.74870 [2] Mike Burrows: “The Chubby lock service for loosely-coupled distributed systems,” at 7th USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006. [3] Flavio P Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordination. O’Reilly Media, November 2013. ISBN: 978-1-4493-6130-3 [4] Enis Söztutar: “HBase and HDFS: Understanding filesystem usage in HBase,” at HBaseCon, June 2013. [5] Todd Lipcon: “Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1,” blog.cloudera.com, 24 February 2011. [6] Martin Thompson: “Java Garbage Collection Distilled,” mechanical-sympathy.blogspot.co.uk, 16 July 2013. [7] Peter Bailis and Kyle Kingsbury: “The Network is Reliable,” ACM Queue, volume 12, number 7, July 2014. doi:10.1145/2639988.2639988 [8] Mark Imbriaco: “Downtime last Saturday,” github.com, 26 December 2012. [9] Tushar Deepak Chandra and Sam Toueg: “Unreliable Failure Detectors for Reliable Distributed Systems,” Journal of the ACM, volume 43, number 2, pages 225–267, March 1996. doi:10.1145/226643.226647 [10] Michael J Fischer, Nancy Lynch, and Michael S Paterson: “Impossibility of Distributed Consensus with One Faulty Process,” Journal of the ACM, volume 32, number 2, pages 374–382, April 1985. doi:10.1145/3149.214121 [11] Maurice P Herlihy: “Wait-Free Synchronization,” ACM Transactions on Programming Languages and Systems, volume 13, number 1, pages 124–149, January 1991.doi:10.1145/114005.102808 [12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “Consensus in the Presence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–323, April 1988. doi:10.1145/42282.42283 [13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues: Introduction to Reliable and Secure Distributed Programming, Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7, doi:10.1007/978-3-642-15260-3 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-is-redlock-safe.html":{"url":"book/distribute-lock-is-redlock-safe.html","title":"热风-Redlock能保证锁的正确性吗？","keywords":"","body":"Redlock能保证锁的正确性吗？        本文是Salvatore Sanfilippo反驳Martin Kleppmann关于论证Redlock难以确保分布式锁正确性的文章，文章原文在这里。        Martin Kleppmann关于论证Redlock难以确保分布式锁正确性的文章可以看我翻译的文档。        作者的整篇回复有些乱，没有或者回避了Martin观点的核心，或者说Martin也没有直白的说出来。如果读了Martin的文章，实际上可以看出，任意对时间的假设，都会导致锁超时（释放）的一刻，两个客户端同时有进行操作的可能，这点在理论模型上是能够说的通的，毕竟释放锁的不是锁的持有者，而是锁自己。作者说到所有的分布式锁都有这个问题，我会在之后说明，这个之后通篇没有看到。        如果是基于Zookeeper的分布式锁，可以通过心跳的方式，让出问题的客户端由于心跳失败而导致锁过期，这种释放锁的判定方式会显得更加的自然，虽然复杂度有了提升，但是这和由超时直接释放锁有很大不同。个人认为，作者的回复是苍白无力的，Martin也没有继续纠缠，总之，正确与否交给用户自己决定吧。        以下是笔者对这篇文章的（摘录）翻译和理解。如果是笔者的理解将会以来开头，以引言的形式来展示，比如： 这个是作者的理解。。。        描述内容示例。。。        Martin Kleppmann，一个分布式系统研究者，昨天发布了一片关于Redlock的分析文章，你可以在这里找到它。        Redlock是我设计基于Redis的一种客户端分布式锁算法，它使用一组具备了特定能力的数据存储（也就是Redis），用于构建一个具备多主容错，正确性有保障和自动释放（也就是高可靠）的分布式锁服务。你也可以通过MySQL来实现Redlock算法。        该算法的目标是为了给使用单节点Redis或者主从复制Redis实现分布式锁的人以更稳定可靠和确保正确性的方案，当然它会带来一些少许的复杂性，但是性能是很好的。        自从我发布了Redlock算法，许多人将其实现，这里不乏多种语言，当然它也被用在很多地方。        Martin对Redlock的分析表明，这个算法难以确保正确性。非常感谢Martin发表了它的文章，以及整个分析过程，Redlock原始的说明在这里：http://redis.io/topics/distlock。但是我对Martin分析的结果并不认同，好消息是分布式系统不向其他的编程领域，它可以通过数学的方式加以表达和论证，因此一组算法特性能够在假设成立的场景下确保其可靠性。在这篇文章中，我将会分析Martin的文章，最终我们能够明白，Redlock算法究竟是否能够确保正确性。 Martin认为Redlock无法确保正确性的原因        在Martin的分析文档中，主要有两个观点： 具有自动释放功能的分布式锁（相互排斥的锁属性仅在获取锁后的固定时间内有效）需要一种方式（或策略）来避免客户端在过期后使用锁时出现问题，这违反了访问共享资源时的互斥性，也就是锁的正确性无法保障。Martin认为Redlock没有这样的机制。 Martin认为不管问题1是否存在，只要它（redlock）在可能产生分区以及不可靠的分布式环境中运行就无法确保其正确性。 其实只有一个观点，就是Redlock基于时间的假设是很容易在分布式环境中被颠覆的，而一旦假设被推翻，则正确性也就不复存在了。作者的这两个观点，第一个避重就轻，只是说出了问题如何能够让他work round，第二个有些模棱两可。        我将分别对这两个观点进行陈述，从第一个开始。 分布式锁，自动释放以及令牌        一个分布式锁如果没有自动过期机制，那么锁的持有者就有几率永久的持有锁，这回导致它是无用的。如果一个持有锁的客户端挂掉，并在一段时间内没有恢复，那么死锁就会产生，而由分布式锁保护的资源将无法被外界访问。一旦出现这种活性问题，这将是无法接受的，没有一个正常的分布式锁会不具备自动过期机制的。        具有实践性的锁，会拥有一个TTL，也就是过期时间。在过期时间到达后，锁的主要属性，也就是排他性会消失，另一个客户端将会获取到锁。如果两个客户端在不同的时间获取锁，但是第一个客户端由于GC或者其他调度问题导致暂停，而第二个客户端此时（由于锁超时）获取到了锁，它们在这一刻都能够访问到共享资源，这会导致发生什么呢？        Martin认为如果分布式锁服务提供令牌机制就可以解决这个问题，也就是在获取锁时，能够得到一个（全局上）单调递增的令牌。Martin对于令牌的使用方式如下：两个客户端由于锁问题，同时访问了共享资源，但是可以将令牌设置在写事务（乐观锁）中，而只有更大的令牌的更新才能够获得通过。        Martin的描述：修复这个问题其实很简单：你只需要在写存储时利用一个令牌即可。在这个场景下，一个单调递增的令牌即可，由锁服务提供，每当获取到锁时，会将令牌同时返回给客户端。        注意这回要求存储服务充当了检查令牌的角色，并且拒绝所有令牌回跳的写请求。        我认为Martin的这个观点存在以下若干问题： 虽然分布式锁可能已经过期，但是大多数情况下你还是需要它来保证排他性语义。分布式锁是在对共享资源没有很好控制的情况下一种非常有效的装置。在Martin的分析中，他假设就算锁出现过期释放，也需要有其他方式来避免竞争。我认为这是一种非常奇怪的推理方式，为什么要解决数据竞争就一定要用分布式锁，就一定要分布式锁来解决数据竞争问题。接下来我会介绍Redlock在这种人为假设场景下也能工作良好的原因； 如果你的数据存储支持令牌，且对令牌的回跳拒绝写入，那么你的数据存储就是一种线性存储。如果基于这种存储，你只需要生成自增ID即可，而Redlock就会确保这个自增ID服务能够在获得锁时返回一个自增ID。但是接下来，我会展示这是没有必要的； 可是第二点并不是一个合理的选择，在面向共享存储（比如：关系数据库等）时，它们大多数不是线性的存储，那该怎么办呢？每个Redlock实例都会生成随机的令牌，它们彼此不会冲突，如果有一个在一时刻唯一的令牌，该怎么办呢？可是使用检查并设置的方式来解决。当开始操作共享存储时，可以将令牌作为一个状态进行存储，同时操作流程遵循读取-修改-写入的方式，当然写入时就以令牌作为比对的条件； 在某些场景下，可以说，顺序的令牌是非常有用的。但是需要注意的是，由于Martin提到的GC暂停，虽然令牌被获取到了，可是由于暂停原因，导致后续操作共享资源时并不一定会遵从获取令牌（或者说锁）的顺序； 大多数情况下，锁用于访问以非事务性方式更新的资源。例如：有时我们使用分布式锁来移动文件或者与其他外部API交互等等。 Martin的解法主要有两个问题：1）既然都有了令牌，那还要锁做什么；（2）所有的存储或者所有对状态更改的装置都需要支持令牌，很不现实。 对于回复4，这个观点不难理解，很正常，作者的回复有些不知所云。 对于回复5，对于非事务性的访问，或者说锁用于保护非事务性的资源，这点我持反对的观点，至少它是为了保证隔离性和原子性，是一种事务的体现。        我想再重复一下我的观点，假设一定要有种方式来处理当锁失败的情况，这种观点是多么奇怪。事实上，如果你的系统面临数据竞争，你大可不必使用分布式锁，或者至少你不需要一个有强约束力的分布式锁，而你需要的锁可能会有时失效，但大多数时候能够以高性能的方式提供服务。但是如果你同意Martin的关于令牌的观点，那么使用锁提供了唯一VALUE也是可以做到一样的效果。 接下来看一下系统模型        上述批评，也就是没有为每个锁提供单调递增的计数器，这种批评在自动释放的分布式锁中基本上是常见的。然而Martin对Redlock有一些特别的批评，并且真正分析了算法，然后尝试推翻它。        Redlock假设一个半同步的系统模型，其中不同的进程可以或多或少以相同的“速率”计算时间。不同的进程不需要由于绝对时间的边界偏差而做任何事情。它们只需要能够在计数5秒的情况下，可以允许10%的偏差，例如：一个进程计数了4.5秒，而另一个是5.5秒，这就够了。        Martin还指出，Redlock要求网络消息的最大延迟，据我所知，这是不正确的（我稍后会解释他的推理有什么问题）。        所以我们先从不同的进程不能以相同速率计时开始说起。Martin认为在一个系统中出现时钟跳跃有两个原因： 系统管理员手工调整时钟； NTPD根据收到的更新通知来更新本地时钟。        上述两个问题是可以解决的，对于问题1，只需要不做类似操作即可，对于问题2，可以使用不会进行时间跳跃的NTPD后台服务即可，它会通过在更大时间范围内进行更改时间。        但是我认为Martin关于Redis或者Redlock关于时间的实现需要切换到monotonic时间API上来，这个论点是正确的，因为它会或多或少的让上述问题得以解决。这个改造在过去被提到过多次，它需要在Redis的实现中引入一些复杂度，但确实是个好的提议，我会在未来的几周来实现它。切换到monotonic时间API上目的是能够更好的运行在系统上，因为它会带来避免受到时间服务器以及人为操作的影响，而如果单纯的出于解决这个问题，就算使用gettimeofday方法也是可以完成的，因为我们可以通过计算相对时间来达到目的。 实际上这个问题是针对Redis的。        请注意，过去曾尝试过实现分布式系统，即使假设存在绑定绝对时间错误（通过使用GPS单元）的情况。Redlock不需要类似的保障，只需要不同进程在计时时，能够将10秒计数为9.5或11.2（示例中最多+/- 2秒）的能力。        因此Redlock是能够确保正确性的吗？这个答案取决于上面的假设。如果我们使用了monotonic时间API来完善这个场景，如果没有意外的人为操作，那会怎么样呢？一个进程能够在最大容忍错误下进行可靠的计时吗？我想一定可以，而且相比较一个进程能够在磁盘日志上可靠的写入更加确定。 网络延迟        Martin人为Redlock的问题不仅仅是在依赖于计时的假设，他说道：可是Redis不仅如此，它依赖了太多对于时间的假设：它假设所有的redis节点能够以大致相同的过期时间来保存KEY，假设网络延迟要小于节点之间过期时间的差距，当然进程暂停时间也会小于这个差距。        我们将上面的观点做一下分解： Redis节点之间存放的过期时间大致相同 网络延迟要小于节点之间过期时间的差距 进程暂停时间要小于节点之间过期时间的差距        Martin所说关于时间跳跃的问题，我假设我们已经通过使用monotonic时间API都已经解决了。        关于问题1，这不是一个问题，我们假设在节点之间能够以大致相同频率进行计时，除非有其他观点反驳它。        关于问题2，有些复杂，Martin说：当然，如果你认为时间跳跃不现实，因为你认为你配置好了一个不错的NTP服务，当然这点我同意，他接着说： 客户端1在A，B，C，D，E节点上获取锁 Redis实例都返回获取成功，但响应在客户端1网口开始排队时，客户端1发生fullGC，出现了暂停 锁在所有的Redis实例上过期 客户端2获取A，B，C，D，E节点上的锁 客户端1的GC完成，并收到延迟后的响应，根据响应判断客户端1获取到了锁 客户端1和2都认为获取到了锁        如果你看了Redlock的说明，这篇文档我已经几个月没有更新它了，你能看到对于Redlock的描述： 获取当前的时间（毫秒）； 接下来针对N个Redis实例使用相同的Key和随机值顺序获取锁。对于IO超时时间可以设置一个足够短的时间，比如：如果过期时间是10秒，则超时时间可以设置在5到50毫秒的范围，目的是能够在Redis实例挂掉的时候能够快速反应，使得能够与下一个Redis尽快完成交互； 如果客户端的耗时，从第一步到N个实例都调用完成，如果不超过锁的过期时间，并且在获取到锁的Redis实例数量大于等于3个，则认为该客户端获取到了锁； 如果锁获取到了，那么这个分布式锁的占用时间可以认为是初始的过期时间，比如：10秒，减去获取锁的耗时； 如果客户端没有获取到锁，比如：获取到锁的实例数量是少数，获取在步骤3中的耗时已经超过了锁的过期时间，客户端将会选择释放所有节点的锁，纵使客户端它没有获取到对应节点的锁，这出于简单性的考虑。        注意第1步和第3步，无论网络中发生怎么样的延迟，只要在这两步之间，延迟一定可以被检测出来，所以延迟如果要发生的话，一定在第3步之后。也就是获取到锁后，在第3步之后出现了锁的过期，也就是Martin最早提出的，当客户端持有一个过期锁而去操作共享资源时的问题。我再次强调，所有的分布式锁实现都会有这个问题，而使用令牌去解决显得更加不切实际。 Martin不应该给出令牌的解决方式，只用说问题即可，导致作者一直追着令牌说，其实这个问题不重要，重要的是锁的语义，锁不是由持有者释放的，而是由超时释放的。        注意步骤1和3之间发生的事情，你可以添加任意你需要的网络延迟，锁总是可以被很好的处理，如果延迟过长，则锁就会失效，Redlock能够对网络延迟有很好的免疫力。它在设计时就考虑了这些问题，我无法想象它会出现竞争问题。 这点作者是没注意还是水平低，1和3之后的任意网络操作都有可能延迟，而只要有足够的延迟时长，就会造成这个问题。感觉作者已经上头了。        然而，Martin的博客文章也得到了多名分布式系统专家的审查，所以我不确定我是在这里遗漏了什么，还是只是Redlock的工作方式同时被许多人忽视了。我很乐意收到一些关于这一点的澄清。问题3和问题2的应对方式类似。 关于网络延迟的题外话        一个简短的说明。如果使用一个具备自动释放的分布式锁服务，客户端尝试获取锁，服务端判定可以，但是由于客户端进程出现了暂停，导致收到服务端获取锁成功的响应有些延迟，而此时锁已经过期。但是你可以做很多事情来避免进程陷入长时间暂停，但你无法控制网络的延迟，所以在获取锁之前以及之后最好做一下时间记录，通过时间差来判定是否能够安全的进行下一步操作。 其实这么做意义不大，因为完全可以在你根据时间差判定没有问题后再进行GC暂停，所有的问题在真正调用存储服务前都可看似正常，但是在调用的一刻，出现暂停，那就会有问题。 是否使用Fsync？        Martin谈到了Redlock使用延迟重启实例的方式。该方式要求能够或多或少地等待指定时间。重复同样的事情是没有用的。        但是这个方式实际上是可选的。你可以通过配置Redis节点的fsync特性来保证每次处理外部的请求都会进行持久化，如果配置了，那么锁信息就一定在磁盘上存在，这也是其他强约束系统能够提供的相同能力。Redlock非常有趣的是，你可以通过通过延迟启动来选择任何磁盘对于锁服务的参与。这意味着可以使用几个Redis实例每秒处理数十万个锁，这是其他系统无法做到的。 GPS设备与本地计算机时钟        回到系统模型，使Redlock系统模型实用的一件事是，您可以假设一个进程永远不会被系统时钟分区。请注意，这与其他使用GPS单元的半同步模型不同，因为在这种情况下可能会发生两个不明显的分区： GPS与GPS网络隔开，因此无法获得修复； 进程和GPS无法交换消息，或者交换的消息出现延迟。        上述问题可能会造成可用性或正确性问题，取决于系统是如何编排的（只有在出现设计错误时才会发生正确性问题，例如：GPS异步更新系统时间，以便在GPS不工作时，绝对时间错误可能会超过最大时延）。        Redlock的系统模型没有依赖复杂的设备或者额外的硬件，它仅仅依赖计算机时钟，甚至一个非常便宜的时钟就可以满足，因为温度或者其他原因都可以影响到时钟计时。 结论        我认为Martin对于使用monotonic时钟API的观点是可以接受的，Redis和Redlock应该避免使用系统时钟，这点提了个醒。但是我不认可关于Redlock其他正确性的相关结论。        如果能够收到其他专家的反馈，或者使用Jepsen（或类似）工具的测试数据，那就太好了。 作者应该自己测一下，MD还需要别人给数据，有点搞。        非常感谢我的朋友帮我review这篇文章。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/distribute-lock-another-way.html":{"url":"book/distribute-lock-another-way.html","title":"热风-再看分布式锁","keywords":"","body":"再看分布式锁        分布式锁保证获取锁的实例（或线程）能够看清锁的资源状态，并依赖存储服务提供具备正确性和可用性的锁服务。由于分布式环境存在网络分区且不可靠，所以分布式锁无法将正确性和可用性同时推向极致，因为它们之间存在矛盾。不论是为了正确性而选择推模式，还是为了性能而选择拉模式，比选择更重要的是需要确保同步逻辑具备面向失败的设计，拥有一定的自愈能力。        随着应用分拆以及微服务的不断落地，在分布式环境下遇到并发问题的概率越来越高，而开发者应对这类问题的解法却很单一，即使用分布式锁。通过引入分布式锁，将一个容易出问题的并发场景串行化，使之降低思考难度，从而便于理解和实现，但这么做是真的好吗？答案是不一定的，通过深入分析问题场景，也许不使用分布式锁就能够更好的解决该问题。 比选择推与拉更重要的是什么？        分布式锁根据等待获取锁的实例被唤醒方式的不同而分为拉模式和推模式，由于二者对存储服务的诉求不同，锁的（获取与释放）流程不同，所以它们在性能、正确性、可用性和成本上也不尽相同。一般来说，拉模式的性能较好且成本不高，适合面向用户操作的链路，也就是业务系统的核心（流量）链路，因为该链路对性能有要求，更短的RT，更高的TPS，往往是该链路所追求的。推模式在正确性上有优势，比较适合一些后台关键场景，这些场景可能会存在一些复杂计算或者高耗时的操作，因为推模式不需要机械的设置锁的占据时间，所以遇到同步逻辑耗时长短不一的情况也能从容应对。        将如何选择推与拉放到一边，先再看一下分布式锁的使用流程，如下图所示：        如上图所示，在使用分布式锁之前会先进行前置查询检验，此时会调用获取多方数据进行逻辑判断，如果符合要求，再进行加锁。锁的粒度一般是以具体的业务实体来设计的，比如：会员、商品或者订单粒度。被分布式锁保护的同步逻辑不一定只是位于一个系统中，因为并发冲突往往不是在相同的业务场景（或系统）中产生的。多个不相干的业务场景中时常会产生并发冲突，比如：签到场景会更新用户积分，而用户确认收货也会更新用户积分，当系统进行自动确认收货时，用户又刚好做了签到操作，此时对用户的积分更新就会产生并发冲突，需要依赖用户维度的分布式锁保证多方逻辑的正确执行。成功获取锁后，就会执行同步逻辑，而同步逻辑中一般会先进行查询检验，不会直接进行操作更新。        分布式锁虽然被用来处理并发问题，但实际生产环境中的并发度并没有想象的那么高，就算在排名前列的互联网公司也不例外。因为虽然访问量巨大，但是如果按照用户维度进行流量分组，会发现用户级别的并发度并不高，因此分布式锁承担的职责更多是对正确性的保证。虽然用户级别的并发度不高，但出现了问题却影响严重，轻则损害用户体验，重则导致资损，开发人员会疲于应对。        那解决方案就是在推和拉中做出正确选择吗？答案是否定的，解决之道不是选择，而是在于如何编写同步逻辑。只有保证同步逻辑是健壮、可信和容错的，才能在锁失效或同步逻辑出现问题时表现的镇定自若，而提升同步逻辑可靠性的方式有许多种，这里举几个例子，如下表所示： 名称 描述 目的 数据约束 数据结构（比如：关系数据库的表）需要根据自身业务要求建立适当的唯一约束 避免出现异常数据，保证最低限度的正确性 访问日志 对于锁的获取与释放，以及同步逻辑中的关键步骤需要打印结构化日志。日志中需要包含关键的业务信息，比如：相关业务主键，关键业务数据等 便于掌握分布式锁的工作情况，以及同步逻辑的执行情况，包括：调用数量、耗时以及正确率等 监控报警 对结构化日志进行监控，发现错误或异常后会进行报警，比如：通过短信通知应用负责人 防止遗漏问题，能够在问题发生的第一时间得到通知，进行处理 后台处理 对同步逻辑中的错误进行捕获，并将其记录到数据库中，通过后台可以查看出错记录，也可以触发重试 系统化处理问题，避免出现高风险的人工数据订正 自动补偿 同步逻辑执行出现错误后，会将错误记录下来，随后进行定时重试，而重试逻辑会进行相应的检查以及补偿操作 对于问题能够做到系统自愈，减少人工干预 结构化日志，即非自由格式日志，由事先定义好的数据格式来输出日志，目的是使日志的后续处理、分析或查询变得方便高效。        如上表所示，表格中由上至下这些方式分别着重于可观测性、健壮性和可恢复性，同时这些特性对技术的要求以及成本会随之变高。因此，在不同的场景中，也需要根据重要程度来运用这些方式，比如：对于核心业务场景，肯定需要不计成本，做到最好，而对于一般场景，至少需要确保做到可观测和有监控。可观测以及有监控之所以是必要的，是因为系统出现问题往往不是偶然，而是由于长时间业务需求不断上线，系统缺少维护劣化所致。面对这种必然，有效的指标观测以及报警，会使得问题被提早暴露出来并得以修复加固。 解锁胜于用锁        在分布式环境中遇到并发问题时，选择使用分布式锁能够很快且直接的解决问题，但是随着锁的引入，会对性能造成长久的影响。除了降低一定的系统性能，还需要系统的维护者在未来不仅要关注同步逻辑的执行情况，还要对依赖的锁服务状况进行持续监控，在任何业务活动来临时，都要提前给足容量。        正因为分布式锁的引入会带来一些问题，增加一些维护成本，所以在识别出具体场景中存在并发问题后，需要思考是否存在不使用分布式锁就能解决并发问题的实现方案，而这种解锁的方案更值得推荐。由于解锁是建立在具体场景上的，所以没有直接可以套用的模式和成熟的方法论，需要开发者能够仔细分析场景问题，不是只聚焦在一个点上，而是从数据的产生到消费全链路的思考和推演，只有这样才可能找到好的解法，下面举两个解锁的例子。        场景一：多个使用方都会调用会员的属性更新接口，当并发冲突发生时，会出现由于数据覆盖而产生脏数据。该场景是分布式环境下，基础服务经常面临的问题。如果使用分布式锁来解决，可以在接口实现上增加会员粒度的分布式锁，这样多方并行的更新请求就会在会员维度上排队，从而避免产生脏数据的问题。        那假设不使用锁呢？可以通过在会员属性的存储结构中增加数据版本来解决。使用方在调用会员属性更新接口前，一般都会查询出会员相关属性，然后传入目标值调用接口进行更新，而数据版本的解决思路就是要求更新参数需要携带之前获取的会员数据版本。假设会员微服务使用关系数据库作为存储实现，该过程如下图所示：        如上图所示，使用方查询出会员属性，然后将其数据版本与更新参数一并发往会员微服务。会员微服务在处理更新请求时，使用where条件和数据版本，能够保证只有符合数据版本要求的更新请求才会被处理，这样使用过时数据版本的更新请求会被忽略，同时数据版本也会在更新成功后自增。使用方得到更新结果后，可以选择返回错误或者重试。通过增加数据版本，使得会员属性更新在没有引入分布式锁的情况下做到了线程安全。        场景二：异步接受商品变更消息，将商品信息查询出来并同步到其他系统，当消费端并发收到消息时，有概率会出现旧数据覆盖新数据的情况。当消费端并行处理消息时，先收到消息的线程查出了旧有商品数据，而后收到消息的线程可能已经完成了同步处理，此时先收到消息的线程会将旧数据再次同步，从而导致产生了脏数据。这种后发先至的问题也经常在分布式环境下遇到，而该问题往往在测试环境中难以复现，并且在生产环境中也只是随着流量变高而偶现，是一种具备隐蔽性的问题。如果使用分布式锁来解决，可以在消费端处理逻辑中增加商品粒度的分布式锁，这样数据同步就会在商品维度顺序执行，从而不会出现脏数据的问题了。        那假设不使用锁呢？可以将商品变更消息的类型改为顺序消息，消息路由策略可以选为按照商品ID取模。这样同一个商品变更消息的处理会被控制在相同的消费线程中，使得该处理逻辑从宏观上看是并行的，但从（具体商品）微观上看又是串行的。通过引入顺序消息，不使用分布式锁，也可以解决该场景并发冲突的问题。 顺序消息是消息中间件提供的一种严格按照顺序进行发布和消费的消息类型，比如：RocketMQ和Kafka就具备这种特性。        由上述解锁的例子可以看出，对业务场景问题进行仔细分析，通过一定的技术方案调整或实施，是可以做到不使用分布式锁就能解决并发问题的。当然，并不是每个存在并发问题的场景都有无锁的解法，只是开发者不要放弃对无锁的追求，同时解锁的经验也需要开发者在工作中多加思考和练习。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/change-to-temurin.html":{"url":"book/change-to-temurin.html","title":"热风-更新到Eclipse Temurin","keywords":"","body":"更新到Eclipse Temurin        最近想更新到OpenJDK 17，发现Homebrew上的AdoptOpenJDK最高只到16，是没有更新吗？了解了一下，原来是AdoptOpenJDK被废弃了，那该怎么办呢？ 之前JDK16发布时，朋友圈有一堆同学转发欢呼，而JDK17发布时，却安静了许多。我却挺高兴，毕竟是LTS版本啊，我不禁想问：你们怎么不欢呼了？        这就要提到Eclipse基金会旗下的ADOPTIUM了，它是一个发布二进制安装包的项目，而OpenJDK的发行版也被其囊括在内。AdoptOpenJDK从长远考虑，加入到ADOPTIUM，成为Eclipse Temurin。它被设定为用于苛刻的生产环境，换句话说AdoptOpenJDK改名了，叫做Eclipse Temurin。        接下来，还是基于Homebrew，我们来看看怎样切换到Eclipse Temurin。 已使用AdoptOpenJDK        需要卸载AdoptOpenJDK，卸载的方式是通过brew remove --cask $name命令来进行卸载，其中$name是之前安装的AdoptOpenJDK。        以adoptopenjdk8为例，执行命令：brew remove --cask adoptopenjdk8。        当移除完所有adoptopenjdk${version}，就可以对AdoptOpenJDK进行untap了。        执行命令：brew untap AdoptOpenJDK/openjdk，和AdoptOpenJDK说拜拜。 还在使用OracleJDK        OldTimer，需要向前走了。首先删除/Library/Java/JavaVirtualMachines/目录下的旧有JDK，顺便清除下面目录中的内容： /Library/Internet Plug-Ins/JavaAppletPlugin.plugin /Library/PreferencePanes/JavaControlPanel.prefPane ~/Library/Application Support/Oracle/Java        这样基本清理了旧有JDK的内容，开始准备安装OpenJDK。 安装Temurin        还是以二进制包的形式进行安装，所以还是不可避免的使用cask，先tap上cask-versions，使之能够找到所有的casks。        运行命令：brew tap homebrew/cask-versions。        然后进行安装，比如要安装OpenJDK8。        运行命令：brew install --cask temurin8        笔者安装了OpenJDK8、11、17三个版本，分别需要运行： brew install -- cask temurin8 brew install -- cask temurin11 brew install -- cask temurin // 默认是17        安装完成后，在/Library/Java/JavaVirtualMachines/目录下会出现三个目录： 切换JDK的版本        在~/.bash_profile中添加以下脚本： export OPENJDK_JAVA_8_HOME=\"/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home\" export OPENJDK_JAVA_11_HOME=\"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\" export OPENJDK_JAVA_17_HOME=\"/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home\" alias openjdk8='export JAVA_HOME=$OPENJDK_JAVA_8_HOME' alias openjdk11='export JAVA_HOME=$OPENJDK_JAVA_11_HOME' alias openjdk17='export JAVA_HOME=$OPENJDK_JAVA_17_HOME' export JAVA_HOME=$OPENJDK_JAVA_11_HOME export PATH=\"/usr/local/bin:/usr/local/sbin:$PATH”        然后运行：source ~/.bash_profile使之生效，通过运行openjdk11，可以将当前JDK切换为OpenJDK11。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/using-jetbrains-toolbox.html":{"url":"book/using-jetbrains-toolbox.html","title":"热风-使用JetBrains Toolbox来管理IDE","keywords":"","body":"使用JetBrains Toolbox来管理IDE        今天（🎄圣诞@2021）续费了JetBrains的全家桶，下载了JetBrains Toolbox用了一下。它能够管理你的JetBrains工具，包括：工具的升级、配置以及项目的目录管理。        访问如下链接，可以下载Toolbox。        安装好后，就可以打开Toolbox，可以看到旧有通过安装器直接安装的IDE，比如：IDEA，如下图所示：        推荐删除它，并改用从Toolbox安装的APP。通过Toolbox安装的程序，可以在：/Users/${Your Name}/Library/Application Support/JetBrains/Toolbox/apps中找到。        点击安装后的APP设置按钮，可以对APP进行设置，如下图所示：        还可以安装历史版本。这里先尝试对IDEA进行配置，因为主要使用Java，所以内存各方面配置给的高一些。        将堆设置到10G，编辑对应的文本即可，适当的增大CodeCache配置，在看某些大项目时体验会好些。        Toolbox也是支持JetBrains账户的，推荐登录一下，设置一下APP的自动更新。Toolbox会开机自动启动，这个可以在设置中取消，我不太喜欢自动启动，关闭了它。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/8-books-to-be-better-dev.html":{"url":"book/8-books-to-be-better-dev.html","title":"热风-成为更好程序员的8本书","keywords":"","body":"成为更好程序员的8本书        本文翻译自DZone的文章，文章推荐了成为一个更好程序的8本书。书单看起来比较偏向Java程序员，也许是因为Java更加Pragmatic，所以在这门语言下累积了太多的软知识，而这些宏伟的软知识又需要借助Java来向开发者们宣告自己的意志。        以下是笔者对这篇文章的（摘录）翻译和理解。如果是笔者的理解将会以来开头，以引言的形式来展示，比如： 这个是作者的理解。。。        描述内容示例。。。        DZone是一个不断发展的作者社区，这些作者们具备广泛的技能，同时他们也愿意将自己的知识分享给社区里的其他成员。我们请求了DZone核心社区的成员分享那些帮助提升他们技能的书籍，因此我们收到了一份令人吃惊的书单，它们不仅很有用，而且很值得去读。        程序设计、架构和设计模式都被包含在这份书单中，你可以看完这篇文章，并把它们加入到你的书架上。希望这些书能够让你成为一个更好的程序员。 《Java Concurrency in Practice》        多任务处理越来越多的占据了我们的生活，我们在手机上操作APP，经常一个APP的工作还没有结束，就立马切换到另一个APP。并发在快速和高效的程序里扮演着重要角色，这本书是在2006年出版的，它基于Java 5。虽然Java这些年更新了许多大版本，但是这本书仍然是学习并发编程的最好资料。        这本书讲述了Doug Lea并发包中的大部分内容，以使用者的角度，它能够让你熟练的运用并发工具来解决日常的问题。 如果你想探究并发的本质，从理论上让自己不惑，同时也能够在和其他开发者交谈过程中有所谈资，那么推荐你看一下Doug Lea的这本书，该书已经是绝版。 这本书体现了Doug Lea的学术水平，他会以形式化的方式来教会开发者如何触及到理论，让开发者有机会接近并发的本质。当然如果你对并发包中的实现感兴趣，也推荐读一下Java并发编程的艺术。 《Clean Code》        能写代码是不足以成为一个有技能的程序员，能够读代码和写出可读性高的代码跟怎样写代码一样重要。这本书教会我们如何识别出代码中的“坏味道”，然后通过持续的重构，使之变为好代码。无论你是一个新手还是一个有经验的老手，我们都推荐你读一读它。 这本书是Uncle Bob的力作，推荐！ 《Refactoring》        不断演进的需求会使得程序不断的更新，在用户功能上变得更好，而在代码世界里，重构也经常出现。一个小的改动，也会导致级连（关联）错误或不可预期的行为。这本书包含了关于重构的方方面面，并演示通过重构来获得一个更加简单和易变的代码库，同时避免出现一些重构中的常见错误，并且能够更好的理解敏捷开发。 这本书是Martin的力作，推荐！ 《The Pragmatic Programmer》        处理事情从现实出发，而不是理论，这是种务实的表现。这本书经常被推荐给新入行的程序员或者在校生，数十年来，在它的帮助下，一个个经验丰富的开发者成长起来。它涉及到现实问题，但不失趣味的文风和类比，讨论了编程、测试和调试等范畴，是一本能够始终在案前陪伴你的书。 这本书的中文版是马维达，中文名是程序员修炼之道。刚参加工作那会，每晚躺在床上，都会读上一小段，依稀还记得调试那节讲的曳光弹。 这本书很薄，可以很快读完，从开篇的石头汤开始，整本书需要你多读几遍。相对于这本书，我推荐代码大全。 这本书连注脚都推荐你读一下。 《Introduction to Algorithms》        算法是计算机技术图谱中重要的一个组成部分，从类似谷歌和亚马逊的科技公司，到一个简单的排序功能，都是用算法来解决问题。进入算法世界并磨练你的逻辑能力，这本书涵盖了多种类型的算法，包括了动态和贪心算法。通过练习和解决问题来学习，将会提升你的逻辑能力，尤其是在注重算法的今天。 买了这本书，但没怎么读，大部头的书。貌似Java程序员都不太熟悉算法，if和for走天下，如果一个if不行，就再来几个。 《Programming in Scala Fifth Edition》        Scala能够同时支持面向对象和函数式编程，这些年变得越来越流行了。该书已经出了第五版，而第五版相比于之前的版本，是完全的重写。在新原则和详细介绍的帮助下，这本书能够帮助你掌握好Scala。 买了这本书，但完全没读。我评估了下，掌握一门运行在JVM上的语言，不如与Java共进退，尤其是Java8已经面世的今天。 如果是语言和函数思维方面的书，推荐下面这本书，不要错过里面的任何章节。 读起来真的很畅快，能够很好的用函数化思维来重构你的认知，同时让你能够写出更好的函数化程序。注意这本书的封面，它已经出了第二版，涵盖了模块化特性，值得期待。 《Design Pattern》        设计模式能够提升代码的可用性，并允许开发者实现和重用复杂的组件来减少已有代码的体量。这本书包含了创建、结构和行为模式，而设计模式也是个大公司面试时常问的。深入的掌握多种设计模式，能够帮助不同语言的开发者利用好功能。 买了这本书，有些年头了，很薄的一个册子，而且基于C++。 由于作者的名头，这本书的影响力很大，但读起来不轻松。如果你想学的轻松一些，推荐下面这本书。 如果不介意大部头，可以读一下Java与模式，这本书更加全面。 但如果想更进一步提升理论，掌握本质，先听这句话：这道理难以被求取，但如果你顺应这些道理，那益处将会自然浮现。你可以将道理换成模式，是不是有些感觉了。设计模式就是这样，你深入去思考每一个模式，不会有什么感觉，但是一旦你在合适的地方使用了它，那好处就会非常自然的出现了。说了这么多，推荐下面这本书。 克里斯托弗将会向你阐述建筑中的模式，让你感受什么是自然和谐的设计。 《Microservices Patterns: With Examples in Java》 微服务现在到处都是，许多组织都在进行从巨石应用向微服务应用的迁移。这本书深入的探讨了微服务架构，并使用了基于Java的例子来让开发者更加容易的理解和实现微服务。在这本书的帮助下，去创建生产应用吧。 这本书关注了很久，也早都买了，但是没有读。涉及微服务领域的技术被探讨的太多，但是如何构建，有哪些模式，却很少，这本书显得非常特殊。对于我而言，会专门挑时间，仔细读一下。 这里没有其他的书可以推荐。微服务作为一个充满争议的技术走到开发者之间，懂与不懂的人都在争论着，对于懂得人，希望看到讲述模式的书籍，对于不懂的人，总喜欢用自己旧有的知识去度量新生事物。这里我想说：用带有欣赏的眼光去看待它，理解它和使用它。 最后，感谢核心成员Bartek Żyliński、Tyler Hawkins、Allan Kelly和Boris Zaikin对推荐书单的支持。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-summary.html":{"url":"book/juc-summary.html","title":"热风-JUC总结","keywords":"","body":"JUC总结 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-stamped-lock-doc.html":{"url":"book/juc-stamped-lock-doc.html","title":"热风-JUC中的StampedLock文档","keywords":"","body":"JUC中的StampedLock文档        StampedLock是Java8新增的并发控制装置，它虽然是一个锁，但是没有实现Lock接口，在读多写少的场景下，它能够提供比ReentrantReadWriteLock更好的吞吐量。它的文档挺多，目前没有看到很好的翻译，这里我就做一下这个工作。        以下是笔者对这篇文章的（摘录）翻译和理解。如果是笔者的理解将会以来开头，以引言的形式来展示，比如： 这个是作者的理解。。。        描述内容示例。。。        一个基于功能的锁，它使用三种模式来控制读写访问。StampedLock的状态由版本和模式组成。获取锁的方法会返回一个邮戳，该邮戳用来表示锁的状态，同时在后续对锁的访问控制做出更改时，需要依赖该邮戳，而这些访问控制以try开头的方法，如果接受邮戳后返回一个0，则表示控制访问失败。释放和转换锁的方法都需要依赖传入邮戳，如果邮戳与锁的状态不一致，操作会失败返回。 邮戳相当于一个印章，是对当前状态的一个摘要，如果邮戳发生变化，与系统新生成的邮戳不一致，这代表系统发生了变化。        三种模式分别是：        写模式。调用writeLock方法获取写锁，可能会由于排他访问而阻塞，该方法会返回一个邮戳，同时也就获取到了写锁，当同步逻辑操作完成，使用unlockWrite进行解锁时，需要用到它。具备获取超时的tryWriteLock方法也有提供，当锁的状态为写模式，没有读锁可以被获取，同时所有乐观读锁的验证都会返回失败。 writeLock与writeLockInterruptibly都是获取写锁，同时后者会响应中断。 乐观读锁的验证针对的是乐观读锁的操作步骤，获取乐观读锁后，在实际使用数据时，需要进行验证，这种两步走的操作方式基于读多于写的前提，提供更高的并发访问能力        读模式。调用readLock方法获取读锁，可能会由于非排他访问而阻塞，该方法会返回一个邮戳，可以用来调用unlockRead进行解锁。具备获取超时的tryReadLock方法也有提供。        乐观读模式。当锁没有处于写模式时，方法tryOptimisticRead会返回一个非0的邮戳，方法validate可以用来验证邮戳，如果获取邮戳后到验证前，锁没有进入过写模式，验证方法会返回true。该模式可以被看作是读模式的一个极度弱化的版本，只要有写操作，它就会被打破。乐观读模式在简短只读的代码片段上表现更好，因为它能减少竞争并提升吞吐量。但是乐观读用起来比较的凌乱，乐观读部分的代码只能将需要访问的字段读取存放到本地变量，然后再通过validate方法验证完成后，方可以使用。在乐观读模式下读取的数据可能非常不一致，需要使用者对数据非常清楚，并且通过反复调用validate方法进行验证。例如：当读取到对象或者数组时，在访问其字段、元素或者方法时，就需要使用这些步骤。 通过获取乐观读锁后，需要将数据保存到本地变量，然后在使用数据前，需要进行validate，只有validate通过，方能继续使用，也就是说这段时间数据的确没有写访问。        StampedLock也提供了有条件的模式转换方法。例如，tryConvertToWriteLock方法可以完成模式的升级，也就是转换到写模式，需要锁处于以下条件：        （1）已经处于写模式；        （2）在读模式中，但是当前没有其他的读取线程；        （3）在乐观读模式中，并且（写）锁可以被获取。        这些Convert类型的方法被设计用来减少用户手写重试转换的代码。 不仅有tryConvertToWriteLock的升级方法，也有转换到读或者乐观读的降级方法。        StampedLock是个被设计用来构筑线程安全组件的内部工具。想使用好StampedLock，首先需要了解想用锁保护的数据、对象和方法实现，它是不支持重入的，所以在同步逻辑中如果调用当前类的其他方法，开发者需要确认该方法的实现没有获取锁的行为。读模式的正确性是依赖于同步逻辑没有副作用，乐观读模式下，如果没有进行验证，就不要调用无法容忍潜在不一致性的方法。邮戳的表达是有限的，并且没有进行加密，使用者可以进行邮戳的猜测和尝试。邮戳会在一年后回收，如果获取了邮戳，长时间不使用或者验证，那么很可能在时间到达后，无法进行使用和正确验证。StampedLock实现了序列化接口，但是它反序列化时，锁的状态会被初始化，因此想用它来实现远程分布式锁是不可能的。        StampedLock像Semaphore，而不像其他的Lock实现，它对（线程）所有者没有所有权的概念，同时它可以由一个线程获取后，由另一个线程释放或者转换。        StampedLock的调度策略没有总是喜欢读或者写，反之亦然。所有try开头的方法都会尽最大努力来完成，而并不一定符合任何调度或者公平策略。任何调用try开头方法进行锁的获取或者转换的方法，如果返回的邮戳为0，这个返回并不会代表锁的任何信息，而后续的调用尝试可能会成功。        因为StampedLock可以实现多种类型的锁，所以该类没有直接实现Lock或者ReadWriteLock接口。出于方便而言，StampedLock提供了一些视图方法，比如：asReadLock、asWriteLock以及asReadWriteLock，提供给使用者以简化的方式。        下面演示了一个平面点的抽象Point类，通过演示该示例可以了解该类的使用方式以及一些约定。 import java.util.concurrent.locks.StampedLock; /** * @author weipeng2k 2022年02月13日 下午22:39:49 */ public class Point { private final StampedLock sl = new StampedLock(); private double x, y; // 排他的锁定方法 void move(double deltaX, double deltaY) { long stamp = sl.writeLock(); try { x += deltaX; y += deltaY; } finally { sl.unlockWrite(stamp); } } // 只读方法 // 如果乐观读失效，将会晋升到读模式 double distanceFromOrigin() { long stamp = sl.tryOptimisticRead(); try { for (; ; stamp = sl.readLock()) { if (stamp == 0L) { continue; } // possibly racy reads double currentX = x; double currentY = y; if (!sl.validate(stamp)) { continue; } return Math.hypot(currentX, currentY); } } finally { if (StampedLock.isReadLockStamp(stamp)) { sl.unlockRead(stamp); } } } // 从乐观读升级到写 void moveIfAtOrigin(double newX, double newY) { long stamp = sl.tryOptimisticRead(); try { for (; ; stamp = sl.writeLock()) { if (stamp == 0L) { continue; } // possibly racy reads double currentX = x; double currentY = y; if (!sl.validate(stamp)) { continue; } if (currentX != 0.0 || currentY != 0.0) { break; } stamp = sl.tryConvertToWriteLock(stamp); if (stamp == 0L) { continue; } // exclusive access x = newX; y = newY; return; } } finally { if (StampedLock.isWriteLockStamp(stamp)) { sl.unlockWrite(stamp); } } } // 从读升级到写 void moveIfAtOrigin2(double newX, double newY) { long stamp = sl.readLock(); try { while (x == 0.0 && y == 0.0) { long ws = sl.tryConvertToWriteLock(stamp); if (ws != 0L) { stamp = ws; x = newX; y = newY; break; } else { sl.unlockRead(stamp); stamp = sl.writeLock(); } } } finally { sl.unlock(stamp); } } }        算法笔记：        该类的设计采用了顺序锁以及有序读写锁，前者被应用在Linux的内核中，可以参考Lameter的论文：http://www.lameter.com/gelato2005.pdf，以及其他地方，Boehm的文章：http://www.hpl.hp.com/techreports/2012/HPL-2012-68.html，后者可以参考Shirako的文章：http://dl.acm.org/citation.cfm?id=2312015。        从概念上讲，锁的状态的特定位（long类型的第8位）为真时，表示存在写锁，而该位为假时，表示不存在写锁。对于读的状态在验证乐观读时会被忽略，因为StampedLock使用了一个较小且有限的数（long类型的前7位，也就是127个）来保存读状态，或者是存在的读数量，当超过该界限(RBITS，也就是127）的读继续出现时，会使用一个int类型的readerOverflow来保存。        StampedLock使用了一个CLH队列的变体来处理等待获取锁的请求，CLH队列在AbstractQueuedSynchronizer中使用过，对于等待的读或者写请求，都会以节点的形式进入到队列中。StampedLock中对于读请求会汇聚到一起，以节点中的cowait字段来连接。队列结构由于其功能诉求，不需要节点的编号，每个新进入的节点只需要连接到它的前驱上即可。特别的，该类使用了阶段公平且反冲突的原则：如果当前读锁被获取，且同步队列中有一个写请求排队，此时新进入的读请求将会排队。（这个规则可以从acquireRead的复杂实现中看出端倪，如果没有这个规则，锁将会变得非常不公平）。释放读锁的方法不会唤醒等待的cowaiter，它会由唤醒节点对应的线程来完成。        这些规则适用于实际排队的线程。所有tryLock形式的方法，会投机的获取锁，而不去遵循这些规则，看起来会增加冲突。随机的自旋被用来减少（日益昂贵的）上下文切换，同时避免多个线程之间的内存抖动。StampedLock会在队列的头部进行自旋尝试，如果一个被唤醒的线程再次获取锁失败了，那么很可能另一个线程通过tryLock的形式刚好获取到了锁，这种冲突竞争的形式会通过自旋升级来应对，获取锁失败后，将自旋的上限提升到MAX_HEAD_SPINS，避免当前线程在面对持续可能出现的冲突时处于下风。        几乎所有这些机制都会在acquireWrite和acquireRead方法中展示出来，其中这些操作和重试都依赖于一致的本地缓存变量。 代码中对于状态没有直接的使用，而是尝试载入到本地变量后，再用一致的逻辑加以判断或者使用，仅在CAS设置时会与状态变量进行交互。        如之前Boehm的论文提到的，序列（或者说邮戳）的验证（也就是validate方法）需要比volatile提供更加严格的顺序保障。为了确保验证，StampedLock在validate方法中使用了acquireFence。人们不期望看到CAS操作被重排序，因为这里存在控制流的依赖或者说先后顺序，但这些在理论上是可能存在的，因此StampedLock通过在CAS操作后添加storeStoreFence来进行保障。        内存布局将锁状态和队列指针放在一起（通常在同一缓存行上），对于读来说是非常适合的。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-seqlock.html":{"url":"book/juc-seqlock.html","title":"热风-《Linux系统高效同步》SeqLock摘译","keywords":"","body":"《Linux系统高效同步》SeqLock摘译        《Effective Synchronization on Linux/NUMA Systems》论文中提到了SeqLock，这个与StampedLock有一定相关性，内容位于论文的第六节，做一下翻译和总结。        SeqLock是最具有扩展性的锁形式，如果面对大量的读请求，它会非常有用，读请求根本不需要向内存中发起写操作。读请求的发起者只需要关键部分（或者说同步逻辑）前后比较计数器，如果计数器没有更改，那就代表这个过程没有写发生，而之前关键部分的操作就是有效的，如果计数器发生了改变，那就废弃掉刚才的结果，重来一遍。        SeqLock包含一个自旋锁，该锁用来保障写请求之间的排他性。写请求如果获取到了锁，会增加计数器的值，该值会是一个奇数。读请求获取锁时，如果发现计数器是一个奇数的值，则代表写正在发生。当写请求操作完成，释放锁时，会再次增加计数器的值，使之成为一个偶数。锁的获取需要两个原子化的信号量操作，一个是用来获取自旋锁，另一个是计数器。当写请求完成，释放锁时，需要增加计数器，同时更改自旋锁的状态（以便于等待的线程能够获得通知进行锁的获取操作）。这意味着SeqLock的性能不如普通的自旋锁或者读写锁。        读请求可以通过循环执行关键部分来推迟这个过程，对于只有少量写时，是比较好的。在没有竞争的情况下，对于锁而言，知识两个内存读取操作和两个内存屏障。如果读请求在执行时，一个写请求也在同步进行，那么对于读而言，计数器必然不会相等，只需要重复执行关键部分即可。        SeqLock主要用在Linux中的时间获取操作上。对于安腾处理器，在读取时间信息时，会避免任何写请求，这使得时钟访问变得高度可扩展。所有的处理器能够维护它们各自的Cache Line。 这里就体现了乐观读的概念，如果计数器验证通过，一切都会发生在当前处理器的Cache Line上，确保了高性能，避免了同步的发生，而这些需要基于读远远大于写的前提下。        SeqLock的问题在于除了读取，读请求在关键部分中不能做其他事情，因为这个部分时需要重复执行的。读请求的关键部分会和写请求的执行内容产生竞争，因此对关键部分所涉及的数据结构、指针或分配内存都可能存在问题。 SeqLock所保护的数据需要使用者非常清楚。        就性能而言，这是一种理想的锁，因为读请求不需要一个排他的Cache Line。理想情况下，写请求会获取一个排他的Cache Line，其中包含了自旋锁和计数器，然后更新它们。 上述内容与StampedLock中的写状态设计有相关性，利用一个位来表示是否有写，方便了读请求的判断。同时StampedLock又不限于此，写状态能够在奇偶变幻中体现出版本的概念，使得ABA问题不会出现。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-stampedlock-summary.html":{"url":"book/juc-stampedlock-summary.html","title":"热风-StampedLock简介","keywords":"","body":"StampedLock简介        StampedLock是Java8引入的一款锁，在功能层面与ReentrantReadWriteLock（以下简称：RRWLock）相似，它被定义为内部工具类，用来创建其他的线程安全组件。Stamp意为邮戳，而使用StampedLock获取锁时会返回一个邮戳，这个邮戳是对锁状态的一个快照摘要，当后续操作锁（比如释放锁）时，就需要传入先前获取的邮戳，而StampedLock会验证邮戳与当前锁状态，假设二者能够匹配，则表示操作是线程安全的，否则就可能存在并发安全问题。可以看到，邮戳的出现，使得StampedLock的使用方式不同于原有Lock，以获取写锁为例，使用方式如下所示： StampedLock stampedLock = new StampedLock(); long stamp = stampedLock.writeLock(); try { // 写操作的同步逻辑 } finallY { stampedLock.unlockWrite(stamp); }        如上述代码所示，调用writeLock()方法会获取写锁，如果此时写锁或读锁已被获取，则该方法会阻塞，最终会返回一个邮戳stamp，当执行完写操作的同步逻辑后，在释放锁时需要传入先前获取的邮戳。由于使用方式与Lock不同，所以StampedLock没有直接实现Lock或者ReadWriteLock接口，但是出于简化使用考虑，还是提供了一些视图方法，比如：asReadLock()、asWriteLock()以及asReadWriteLock()，让开发者以熟悉的方式来操作而不用去了解邮戳的概念。        StampedLock有三种模式用来获取锁，分别是：读模式、写模式和乐观读模式，前两者与RRWLock中的读和写功能相似，而乐观读模式很特别，它不会阻塞写锁的获取，可以被看作是读模式的一个弱化版本。如果线程A获取了乐观读锁，在此过程中线程B获取了写锁，线程A是可以感知到锁的变化，并需要进行重试来避免使用到过期的数据。乐观读模式在读多写少的情况下表现的很好，因为它能减少竞争并提升吞吐量，但使用起来比较麻烦，获取乐观读锁后会得到邮戳，同步逻辑将需要访问的字段读取到本地变量，然后再通过validate(long stamp)方法验证通过后，方可以使用，如果验证失败，需要进行重试。        StampedLock并没有基于同步器AbstractQueuedSynchronizer来实现，而是选择了自定义类似CLH队列的变体，基于其全新的状态和队列设计，优化了读锁和写锁的访问，相比RRWLock，在性能方面有了很大提升。在读多写少的场景下，RRWLock往往会造成写线程的“饥饿”，而StampedLock的队列设计采用了一种相对公平的排队策略，使得该问题得以缓解，同时在队列首节点引入随机的自旋，有效的减少了（日益昂贵的）上下文切换。RRWLock仅支持写锁降级为读锁，StampedLock则能够做到读锁和写锁之间的相互转换，但它不支持重入和Condition，并且有特定的编程方式，如果使用不当，会导致死锁或其他诡异问题。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-stampedlock-inf-and-case.html":{"url":"book/juc-stampedlock-inf-and-case.html","title":"热风-StampedLock的接口与示例","keywords":"","body":"StampedLock的接口与示例        StampedLock提供了很多方法，乍一看有些乱，但它们可以分为以下五类：获取与释放读锁、获取与释放写锁、获取状态与验证邮戳、获取锁视图和转换锁模式。        获取与释放读锁主要包括以下方法，如下表所示： 方法名称 描述 long readLock() 获取读锁并返回long类型的邮戳，如果当前存在写锁，那么该方法会阻塞，直到获取到读锁，邮戳可以用来解锁以及转换当前的锁模式。该方法不响应中断，但与Lock接口类似，该类提供了readLockInterruptibly()方法 long tryReadLock(long time, TimeUnit unit) 在给定的超时时间内，尝试获取读锁并返回long类型的邮戳，如果未获取到读锁，返回0 long tryOptimisticRead() 获取乐观读锁并返回long类型的邮戳，该方法不会产生阻塞，也不会阻塞其他线程获取锁，如果当前写锁已经被获取，则会返回0。返回的邮戳可以使用validate(long stamp)方法进行校验 void unlockRead(long stamp) 根据指定的邮戳释放读锁        写锁的操作和读锁类似，而获取锁视图如前文所介绍，可以通过调用as开头的方法来获取对应的锁视图，比如：调用asReadWriteLock()方法可以获得一个读写锁的视图引用。如果StampedLock提供了获取锁的视图方法，是不是可以不用掌握该类复杂的API，直接用适配的方式来使用它就可以了？答案是否定的，以StampedLock提供的乐观读模式为例，需要配合验证邮戳的的boolean validate(long stamped)方法才能工作，而该方法会验证获取乐观读锁后是否有其他线程获取了写锁，如果验证通过，则表示写锁没有被获取，本地数据是有效的，而该过程在读多写少的场景下会带来性能的大幅提升，这点是通过锁视图无法做到的。除了验证邮戳的方法，还支持获取锁的状态，比如：boolean isReadLocked()方法用来判断写锁是否已经被获取。        除了上述方法，StampedLock还支持三种模式的相互转换，比如：在获取了读锁后，如果要升级为写锁，可以使用之前获取读锁的邮戳，调用long tryConvertToWriteLock(long stamp)将读锁升级为写锁，其他的转换方式可以查阅包含tryConvertTo的方法，这里不再赘述。 代码示例        StampedLock使用不善会导致死锁和一些诡异问题，因此有一套推荐的编程模式。接下来，通过一个缓存示例说明StampedLock的使用方式，示例代码如下所示： public class SLCache implements Cache { private final Map map = new HashMap<>(); private final StampedLock stampedLock = new StampedLock(); @Override public V get(K k) { long stamp = stampedLock.tryOptimisticRead(); try { for (; ; stamp = stampedLock.readLock()) { if (stamp == 0L) { continue; } V v = map.get(k); if (!stampedLock.validate(stamp)) { continue; } return v; } } finally { if (StampedLock.isReadLockStamp(stamp)) { stampedLock.unlockRead(stamp); } } } @Override public V put(K k, V v) { long stamp = stampedLock.writeLock(); try { return map.put(k, v); } finally { stampedLock.unlockWrite(stamp); } } @Override public V putIfAbsent(K k, V v) { long stamp = stampedLock.tryOptimisticRead(); try { for (; ; stamp = stampedLock.writeLock()) { if (stamp == 0L) { continue; } V prev = map.get(k); if (!stampedLock.validate(stamp)) { continue; } // 验证通过，且存在值 if (prev != null) { return prev; } stamp = stampedLock.tryConvertToWriteLock(stamp); if (stamp == 0L) { continue; } prev = map.get(k); if (prev == null) { map.put(k, v); } return prev; } } finally { if (StampedLock.isWriteLockStamp(stamp)) { stampedLock.unlockWrite(stamp); } } } @Override public void clear() { long stamp = stampedLock.writeLock(); try { map.clear(); } finally { stampedLock.unlockWrite(stamp); } } }        如上述代码所示，写操作put(K k, V v)方法，通过调用writeLock()方法获取写锁以防止其他线程并发修改HashMap中的值，同时返回的邮戳需要保存到本地变量，在更新完HashMap后，再调用unlockWrite(long stamp)进行解锁。StampedLock对写锁和读锁的操作与Lock接口类似，只需要注意邮戳的处理即可，而主要不同在于乐观读锁的使用，它需要遵循的编程模式，使用伪码如下所示： // 获取乐观读锁 long stamp = stampedLock.tryOptimisticRead(); // 将锁保护的数据读入到本地变量 copyDataToLocalVariable(); // 验证邮戳 if(!lock.validate(stamp)){ // 验证失败，升级为读锁，此时可能会阻塞 stamp = stampedLock.readLock(); try { // 刷新本地变量 refreshLocalVariableData(); } finally { lock.unlockRead(stamp); } } // 使用本地变量执行业务操作 doBizUseLocalVariable();        如上述伪码所示，乐观读锁的获取仅仅返回了代表那一刻锁状态的邮戳，开销很低。获取到乐观读锁后，需要拷贝数据到本地变量，如果之后的邮戳验证失败，就需要获取读锁并刷新之前本地变量对应的数据。乐观读锁的编程模式会让人感到有些琐碎，尤其是需要获取多个本地变量的时候，在刷新逻辑中稍有遗漏，就有可能导致使用到过期数据而产生问题。为了避免出现遗漏，在读操作get(K k)方法中，可以看到通过使用for循环将copyDataToLocalVariable()以及refreshLocalVariableDate()两段逻辑合并来减少重复代码的做法。        写操作putIfAbsent(K k, V v)方法演示了乐观读锁升级为写锁的用法，与get(K k)方法中升级到读锁类似，从非阻塞轻量级的“乐观读锁”升级到具备阻塞能力“重量级”的写锁，可以调用tryConvertToWriteLock(long stamp)方法来完成。 如果升级转换失败，返回的邮戳为0，则会调用writeLock()再次获取写锁。        可以看到StampedLock使用复杂度主要是由乐观读锁带来的，即然编程难度增加，那它的性能提升有多大呢？下面我们就通过测试来对比一下。 微基准测试        开发者对于一些代码实现差异或不同类库使用存在性能疑虑时，往往会编写测试代码，采用重复多次计数的方式来进行度量解决。随着JVM不断的演进，以及本地缓存行命中率的影响，使得重复多少次才能够得到一个可信的测试结果变得让人困惑，这时候有经验的同学就会引入预热，比如：在测试执行前先循环上万次。没错！这样做确实可以获得一个偏向正确的测试结果，但是Java提供了更好的解决方案，即JMH (the Java Microbenchmark Harness)，它能够照看好JVM的预热、代码优化，让测试过程变得简单，测试结果显得专业。        JMH的使用较为简单，首先在项目中新增jmh-core以及jmh-generator-annprocess的依赖，坐标如下所示： org.openjdk.jmh jmh-core 1.34 org.openjdk.jmh jmh-generator-annprocess 1.34        创建测试类StampedLockJMHTest，代码如下所示： import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.annotations.Scope; import org.openjdk.jmh.annotations.Setup; import org.openjdk.jmh.annotations.State; /** * @author weipeng2k 2022年02月19日 下午22:18:06 */ @State(Scope.Benchmark) public class StampedLockJMHTest { private Cache rwlCache = new RWLCache<>(); private Cache slCache = new SLCache<>(); @Setup public void fill() { rwlCache.put(\"A\", \"B\"); slCache.put(\"A\", \"B\"); } @Benchmark public void readWriteLock() { rwlCache.get(\"A\"); } @Benchmark public void stampedLock() { slCache.get(\"A\"); } }        如上述测试代码所示，fill()方法标注了@Setup注解，表示在微基准测试运行前将会调用它初始化两个缓存。另外两个方法，readWriteLock()和stampedLock()，标注了@Benchmark的注解，声明对应的方法为微基准测试方法，JMH会在编译期生成基准测试的代码，并运行它。StampedLockJMHTest还需要入口类来启动它，代码如下所示： import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder; /** * @author weipeng2k 2022年02月19日 下午22:19:25 */ public class StampedLockJMHRunner { public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(\"StampedLockJMH\") .warmupIterations(3) .measurementIterations(3) .forks(3) .threads(10) .build(); new Runner(opt).run(); } }        如上述代码所示，StampedLockJMHRunner不仅是一个入口，它还完成了JMH测试的配置工作。默认场景下，JMH会找寻标注了@Benchmark类型的方法，但很有可能会跑到一些你不期望运行的测试，毕竟微基准测试跑起来比较耗时，这样就需要通过include和exclude两个方法来完成包含以及排除的语义。warmupIterations(3)的意思是预热做3轮，measurementIterations(3)代表正式计量测试做3轮，而每次都是先执行完预热再执行正式计量，内容都是调用标注了@Benchmark的代码。forks(3)指的是做3轮测试，因为一次测试无法有效的代表结果，所以通过3轮测试较为全面的测试。threads(10)指的是运行微基准测试的线程数，这里使用10个线程。        运行StampedLockJMHRunner（测试环境：i9-8950HK 32GB），经过一段时间，测试结果如下： Benchmark Mode Cnt Score Error Units StampedLockJMHTest.readWriteLock thrpt 9 5166194.657 ± 235170.493 ops/s StampedLockJMHTest.stampedLock thrpt 9 828896328.843 ± 22702892.910 ops/s        可以看到，相较于读写锁实现的缓存，基于StampedLock实现的缓存在get操作上是前者的160多倍，达到了每秒8亿次以上。 Mode类型为thrpt，也就是Throughput吞吐量，代表着每秒完成的次数。Error表示误差的范围。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/juc-stampedlock-implement-analysis.html":{"url":"book/juc-stampedlock-implement-analysis.html","title":"热风-StampedLock的实现分析","keywords":"","body":"StampedLock的实现分析        接下来分析StampedLock的实现，主要包括：状态与（同步队列运行时）结构设计、写锁的获取与释放以及读锁的获取与释放。 状态与结构设计        StampedLock没有选择使用AQS来实现，原因是它对状态和同步队列的实现有不同需求。首先看状态，StampedLock需要状态能够体现出版本的概念，随着写锁的获取与释放，状态会不断的自增，而自增的状态能够反映出锁的历史状况。如果状态能够像数据库表中的主键一样提供唯一约束的能力，那么该状态就可以作为快照返回给获取锁的使用者，而这个快照，也就是邮戳，可以在后续同当前状态进行比对，以此来判断数据是否发生了更新。相比之下，AQS的状态反映的是任意时刻锁的占用情况，不具备版本的概念，因此StampedLock的状态需要全新设计。接着是同步队列的实现，AQS将同步队列中的节点出入队以及运作方式做了高度的抽象，这种使用模版方法模式的好处在于扩展成本较低，但是面对新场景却力不从心。虽然在程序运行在多处理器环境下，但并发冲突却不是常态，以获取锁为例，适当的自旋重试要优于一旦无法获取锁就立刻进入阻塞状态，AQS的实现与后者相似，它会导致过多的上下文切换，而选择前者的StampedLock就需要重新实现同步队列。        StampedLock使用了名为state的long类型成员变量来维护锁状态，其中低7位表示读，第8位表示写，其他高位表示版本，划分方式如下图所示：        如上图所示，state实际仅使用低8位用于存储当前锁的状态，第8位如果是真（也就是1）表示存在写，反之不存在写。写锁具有排他性，使用一个位来表示，理论是足够的，但读锁具有共享性，需要使用一个数来保存状态。多个线程获取到读锁时，会增加低7位的数，而释放读锁时，也会相应的减少它，但二进制的7位最大仅能描述127，所以一旦超过范围，StampedLock会使用一个名为readerOverflow的int类型成员变量来保存“溢出”的读状态。        StampedLock仅用一位来表示写状态，就不能像AQS实现的读写锁那样，用一个16位的数来描述写被获取了多少次，从这里就可以看出该锁不支持重入的原因。如果写锁被获取，第8位会被置为1，但写锁的释放不是简单的将第8位取反，而是将state加上WBITS，这样操作，不仅可以将第8位置为0，还可以产生进位，影响到高56位，让高56位如同一个自增的版本一样，每次写锁的获取与释放，都会使得（数据）版本变得不同。如果不同线程看到的锁版本是一致的，那么它们本地所保存受到锁保护的数据也应该是相同的，这也就是乐观读锁运行的基础。 StampedLock实际使用包含了写操作位的高57位作为锁的版本。        StampedLock定义了若干long类型的常量，比如：描述写操作位为真的WBITS，state能够描述的最大读状态RBITS等，这些常量与状态进行表达式运算可以实现一些锁相关的语义，比如：判断锁是否存在读等。主要语义与表达式描述如下表所示： 语义 表达式 描述 是否存在读 state & RBITS > 0 为真代表存在读 是否存在写 state & WBITS != 0 为真代表存在写 验证邮戳（或版本） (stamp & SBITS) == (state & SBITS) stamp为指定的邮戳，验证邮戳会比对stamp和state的高57位 是否没有读写 state & ABITS == 0 为真代表没有读写        状态能够表示锁的读写情况，而等待获取锁的读写请求如何排队，就需要同步队列来解决。StampedLock自定义了同步队列，该队列由自定义节点（WNode）组成，该节点比AQS中的节点复杂一些，其成员变量与描述如下表所示： 字段名称 字段类型 描述 prev WNode 节点的前驱 next WNode 节点的后继 cowait WNode 读链表，实际是栈 thread Thread 等待获取锁的线程 status int 节点的状态0，默认1，取消-1，等待 mode int 节点的类型0，读1，写        WNode通过prev和next组成双向链表，而cowait会将等待获取锁的读请求以栈的形式进行分组排队。接下来用一个例子来说明队列是如何运作的，假设有5个线程（名称分别为：A、B、C、D和E）顺序获取StampedLock，其中线程A和E获取写锁，而其他3个线程获取读锁，这些线程都只是获取锁而不释放锁，因此只有线程A可以获取到写锁，其他线程都会被阻塞。当线程E尝试获取写锁后，同步队列的节点组成如下图所示：        如上图所示，StampedLock通过whead和wtail分别指向同步队列的头尾节点。节点A是首节点，它是由第一个未获取到锁而被阻塞的线程所创建的，也就是线程B，该节点的类型是写，状态为等待。节点E是尾节点，线程E由于未获取到写锁，从而创建该节点并加入到队列中，节点类型为写，而状态为默认。节点状态的修改，是由后继节点在获取锁的过程中完成的，因为没有第6个线程获取锁，所以节点E的状态是默认，而非等待，获取锁的过程会在后续章节中详细介绍。        可以看到，同步队列的主队列是横向的节点A、B和E，而在节点B出现了纵向的子队列，原因是StampedLock将被阻塞的连续读请求进行了分组排队。节点B先进入同步队列，随后读类型的节点C会挂在前者cowait引用下，形成节点B至C的一条纵向队列。线程D由于未能获得读锁，也会创建节点并加入到了同步队列，此时尾节点是节点B，StampedLock选择将节点D入栈，形成顺序为节点B、D和C的栈。为什么纵向队列使用栈来实现，而不是链表呢？原因在于读类型的节点新增只需要由读类型的尾节点发起即可，这样做既省时间又省空间，因为不需要遍历到链表的尾部，更不需要保有一个链表尾节点的引用。 如果有被阻塞的离散读请求，中间再混有若干写请求，则会产生多个纵向子队列（栈），此时保有链表尾节点引用的实现方式就显得不切合实际了。        如果线程E是获取读锁，那么栈中节点的顺序是节点B、E、D和C。如果有第6个线程获取锁，不论是它是获取读锁还是写锁，都会排在节点E之后，同时节点E的状态会被设置为-1，即等待状态。 写锁的获取与释放        获取写锁的流程主要包括两部分，尝试获取写锁并自旋入队和队列中自旋获取写锁。如果打开StampedLock源码，会发现获取写锁的逻辑看起来十分复杂，实现包含了大量的循环以及分支判断，而主要逻辑并不是在一个分支中就完成的，而是由多次循环逐步达成。获取写锁的主要流程如下图所示：        如上图所示，左右两侧分别对应流程的两部分。线程在获取写锁时，首先会尝试自旋获取，而获取的操作就是在没有读写状态的情况下设置写状态，如果设置成功会立刻返回，否则将会创建节点加入到同步队列。接下来，在同步队列中，如果该节点的前驱节点处于等待状态，则会阻塞当前线程。在队列中成功获取写锁的条件是前驱节点是头节点，并成功设置写状态，而被阻塞的线程会被前驱节点的释放操作所唤醒，这点与AQS的同步队列工作机制相似。        成功获取写锁后会得到当前状态的快照，即邮戳，在释放写锁时，需要传入该邮戳，释放写锁的主要流程如下图所示：        如上图所示，（外部）输入的邮戳与状态理论上应该相同，因为写锁具有排他性，从写锁的获取到释放，状态不会发生改变，所以之前返回的邮戳和当前状态应该相等。释放写锁会唤醒后继节点对应的线程，被唤醒的线程会继续执行先前获取锁的逻辑，在队列中自旋获取写锁。 读锁的获取与释放        获取读锁的流程与写锁类似，但实现要复杂的多，主要原因在于cowait读栈的存在，新加入队列的读类型节点会根据尾节点的类型来执行不同的操作。获取读锁的主要流程如下图所示：        如上图（左侧）所示，在节点node加入到同步队列时，会判断当前尾节点的类型，如果是读类型，就选择入栈尾节点的cowait，否则将会被设置为同步队列的尾节点。进入到cowait读栈的节点会成为栈顶节点的附属，当栈顶节点被唤醒时，它们也会随之被唤醒。进入到横向主队列的节点会尝试自旋获取读锁，当其前驱节点为头节点时，如果锁的状态仅存在读，则进行读状态的设置，设置读状态成功代表获取到了写锁。读状态的设置需要判断现有的读状态是否超出state读状态的上限，如果超过就需要自增readerOverflow。        StampedLock中的state与readerOverflow合力维护了读状态，因此读锁的释放相比写锁要复杂一些，写锁一旦释放就可以唤醒后继节点，而释放读锁不能立刻唤醒后继节点，需要等到读状态减为0时才能执行。释放读锁的主要流程如下图所示：        如上图所示，需要先判断当前状态与传入的邮戳，二者的版本（也就是高57位）是否相同，如果相同则会对读状态进行自减操作。当读状态为0时，释放读锁会唤醒后继节点对应的线程，被唤醒的线程会继续执行先前获取读锁的逻辑。        StampedLock为了避免状态以及同步队列头尾指针出现数据不一致的情况，在实现锁的获取与释放时，都会提前将其拷贝到本地变量。实现涉及到大量的for循环和if判断，使得读懂它需要花些时间，建议读者结合流程图阅读一下源码，感受一下作者（Doug Lea）缜密的逻辑。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run.html":{"url":"book/program-how-to-run.html","title":"热风-程序是怎样跑起来的","keywords":"","body":"程序是怎样跑起来的        《程序是怎样跑起来的》是日经的矢泽久雄先生的书，这本图解计算机组成原理的小册子，看起来很单薄，也就是200多页，可它蕴含的能量那是大大的。        很早的时候，读了一小部分，然后由于事情耽搁了，忘的可比记得快啊。现在有时间仔细读一下，读完这本书后记录一些札记，并做些总结。由于在iPad上手写了一版笔记，这里就直接贴出来，不再打字了，如果可以，还是会做一下文字版的总结，感觉还是挺有意义的。 如果我在大二上计算机组成原理这门课的时候，能够遇到这本书，那该多好呀！ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-01.html":{"url":"book/program-how-to-run-01.html","title":"热风-对程序员来说CPU是什么","keywords":"","body":"对程序员来说CPU是什么        CPU是由寄存器、控制器、运算器和时钟构成的。寄存器需要重点关注，它是计算机功能的体现，通过指令将数据放置（通过电信号）到对应的寄存器针脚上，再施加计算指令，就可以与运算器配合完成计算。        程序载入内存，由程序计数器读入指定的指令，并解释执行。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-02.html":{"url":"book/program-how-to-run-02.html","title":"热风-数据是用二进制数表示的","keywords":"","body":"数据是用二进制数表示的        集成电路通过电压的高低来表示1或0，不同的针脚上不同的电压，就组成了01100101这样的二进制表示方式。无论是图片还是文本，底层都是由二进制来表示的，只是装载它们的程序将其解析成为它们各自的样子。        对于整数而言，二进制与十进制一样，二进制转为十进制需要带位权的多项式加法，十进制转二进制需要不断的除2运算。负数是由补码描述的，这些都建立在严格的数学理论基础上，也就是说计算机基础实际是建立在布尔代数的基石上的。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-03.html":{"url":"book/program-how-to-run-03.html","title":"热风-计算机进行小数运算时出错的原因","keywords":"","body":"计算机进行小数运算时出错的原因        用于描述二进制小数的位权（指数）是负数，二进制小数转十进制小数依旧是多项式相加，而十进制小数转成二进制，需要循环乘2后取整数。就像十进制1/3一样，十进制转成二进制小数也会遇到无法表示的问题，所以表述都存在误差，更何况计算呢？        计算机表述二进制小数依靠IEEE标准，使用了符号、指数和尾数三个区段来表示，而其中尾数需要使用到正则运算，而指数需要依靠EXCESS系统。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-04.html":{"url":"book/program-how-to-run-04.html","title":"热风-熟练使用有棱有角的内存","keywords":"","body":"熟练使用有棱有角的内存        内存IC需要通电才能保持状态，而控制它的方式是使用地址信号、数据信号和控制信号。通过一组电压给出地址信号，在配合控制信号进行输入，就可以在数据信号的引脚上呈现出存储在对应地址的数据。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-05.html":{"url":"book/program-how-to-run-05.html","title":"热风-内存和磁盘的亲密关系","keywords":"","body":"内存和磁盘的亲密关系        程序保存在存储设备中，内存和磁盘都可以被视为存储设备，只是CPU能够直接访问内存。内存和磁盘会有交互，比如：虚拟内存，会将磁盘上的一部分空间当作内存使用，采用分页或者分段的方式完成其与内存之间的数据交换。        磁盘是以扇区的方式提供使用，只是切分的粒度更大，一次读取和写入的数据单位也就更大。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-06.html":{"url":"book/program-how-to-run-06.html","title":"热风-亲自尝试压缩数据","keywords":"","body":"亲自尝试压缩数据        数据会放在格子里，如果放置的很零散，那利用率就很低，通过压缩的手段，让它们能够密实一些，是一个好办法。        常见的RLE编码，以及哈夫曼树的方式进行压缩数据，都是一种压缩策略。哈夫曼树的构造是自底向上的，从而可以构造出一个节点不重复的树，利用节点编码来描述被压缩的数据文件。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-07.html":{"url":"book/program-how-to-run-07.html","title":"热风-程序是在何种环境中运行的","keywords":"","body":"程序是在何种环境中运行的        程序会在运行环境中运行，而运行环境包括了软件（操作系统）和硬件（CPU），运行环境的变化都会导致程序无法运行。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-08.html":{"url":"book/program-how-to-run-08.html","title":"热风-从源文件到可执行文件","keywords":"","body":"从源文件到可执行文件        CPU只能运行机器语言编写的本地代码，而一般编程语言需要依靠对应语言的编译器，转换成为本地代码，这个过程就是编译。不同的编程语言具备的特性会有所差别，但其主要工作在于各自语言的编译器的设计和开发。        编译源代码只是一个笼统的说法，其实它包括了编译和链接，前者会将源代码转换成为二进制的目标文件，后者会根据编译环境（硬件和软件）将已有的二进制目标文件同之前生成的文件进行融合，该过程称为链接，只有链接后的程序方能在环境中跑起来。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-09.html":{"url":"book/program-how-to-run-09.html","title":"热风-操作系统和应用的关系","keywords":"","body":"操作系统和应用的关系        操作系统也是一步一步发展而来的，它从开始解决通用问题，到现在的包装硬件能力完成应用程序与硬件的解耦，逐步的成长为一个复杂的程序集。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-10.html":{"url":"book/program-how-to-run-10.html","title":"热风-通过汇编语言了解程序的实际构成","keywords":"","body":"通过汇编语言了解程序的实际构成        汇编语言是由指令和操作数组成的语言，它面向硬件，编写汇编代码能够感受到直接和硬件对话的感觉，毕竟你操作的就是寄存器和内存。虽然语言显得很枯燥，但是通过一些模式，还是能够演绎出许多特性，朴实的表面下是强大的能力。        尤其是函数调用，通过栈以及若干寄存器的配合，就能够实现出来，同时将函数调用这一概念暴露给高级语言，非常不简单。 当然汇编语言也有函数调用，这里只是强调实现方式。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-how-to-run-11.html":{"url":"book/program-how-to-run-11.html","title":"热风-硬件控制方法","keywords":"","body":"硬件控制方法        端口是计算机对外部设备的区分定义，通过IN和OUT指令能够让计算机访问外部设备，当然有了操作系统的帮助，应用程序不会同硬件直接打交道，直接与操作系统的API对接即可。        外部设备请求CPU通过中断请求来提醒CPU进行介入。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program.html":{"url":"book/program-program-program.html","title":"热风-程序！程序！程序！","keywords":"","body":"程序！程序！程序！        本文讲述的是程序和计算机的故事，涉及到什么是程序，以及运行程序的软硬件环境，可以理解为是一篇漫谈计算机程序的文章。由于文章内容比较长，所以笔者将其拆分成了5篇文章，由于是拆分的文章，所以或多或少有些前后依赖。分别是：程序的视角、硬件的执念、编译的力量、数据的表达和协同的奥义。        程序的视角以一个C语言的例子为开头，介绍程序的运行环境，程序的运行环境包含了软硬件环境，软体现在操作系统，硬则是指令集。        硬件的执念，以机器语言来解释执行开发者编写的程序，这里会使用到部分汇编知识。        编译的力量重点介绍编译的过程，一门编程语言最重要的就是它的编译器，它完成了概念到实现的转化，什么是编译、汇编和链接。        数据的表达则是讲述数据是以何种形式存储与计算机的，为什么选择二进制，整数和小数如何以二进制的形式存储在芯片中的。        协同的奥义，介绍处理器的基本构成，以及内存和磁盘如何存储数据，它们是如何协同工作的。        加油！加油！克里斯托弗。 参考文献        1. 《程序是怎样跑起来的》是日经的矢泽久雄先生的书，这本图解计算机组成原理的小册子，看起来很单薄，也就是200多页，可它蕴含的能量那是大大的。        2. Crash Course Computer Science的视频课程。        3. 《程序是怎样跑起来的》作者笔记，该书的学习笔记。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program-01.html":{"url":"book/program-program-program-01.html","title":"热风-程序的视角","keywords":"","body":"程序！程序！程序！—— 程序的视角        我们使用计算机来完成工作，其实用的就是计算机应用程序。进行文字编辑的时候，我们会打开Word，跟朋友聊天的时候，我们会使用微信，这些都是计算机应用程序，纵使它们运行在不同的终端上。        内行的人会说，这些终端上都运行着不同的操作系统，比如：windows或iOS，它们上面跑着不同的应用。那么一个应用程序能否在不同的操作系统上运行吗？计算机从诞生的一刻，就有操作系统吗？如果要回答这些问题，就需要我们更深入的学习一下了。 一个C的例子        计算机程序离不开编程语言，时至今日，有不下百种编程语言，比如：Java、Go以及Python，它们有着迥异的语言特性，但在控制方式上都会有顺序、条件和循环。因为从本质上讲，这些语言都是冯·诺伊曼语言，它们都被打上了C语言的烙印。        我们使用C语言编写一个简单的计算机程序，它会要求我们输入两个数，然后通过判断，输出较大的一个。首先我们看一下程序的源代码： #include int gt(int a, int b); int main() { int a, b; printf(\"输入第一个数：\"); scanf(\"%d\", &a); printf(\"输入第二个数：\"); scanf(\"%d\", &b); int x = gt(a, b); printf(\"较大的数是：%d\\n\", x); return 0; } int gt(int a, int b) { return a > b ? a : b; }        保存源代码为文件gt.c，通过cc -o gt gt.c可以将源文件编译成为可执行文件gt，可执行文件也就是程序，通过双击gt或者在终端运行./gt就可以运行程序。 % ./gt 输入第一个数：123 输入第二个数：321 较大的数是：321        通过编译生成的程序gt和我们安装的Word程序有什么不同吗？其实没有什么本质不同，它们都是可执行文件，运行在对应的运行环境中。 程序的运行环境        程序是运行在运行环境中的，运行环境不只是操作系统，而是操作系统和硬件的集合，这里的硬件可以狭义的理解为CPU，因为程序都是由CPU来解析执行的。CPU只能解释其自身能够理解的机器语言，如果遇到不认识的机器语言指令，就无法执行了，这点没有我们想象中的智慧。当然CPU不是只会一种语言，它会若干种，以Intel的i9-8950HK为例，它能够懂的语言可以使用如下方式展示出来，在终端输入命令sysctl machdep.cpu，可以看到如下（只是部分的）输出： machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C        这些语言就是指令集，指令集包含了指令，比如：AVX1.0指令集，ARM处理器就不懂，它是用来增强CPU浮点运算的，那么ARM不懂是不是意味着在浮点运算上输了x86一阵呢？其实不然，ARM处理器也会有自己提升浮点运算能力的指令，也会说自己能懂的指令（或语言）。        可以看到在运行环境中，CPU可以根据指令集的不同分为几个大类，比如：x86、ARM以及risc-v。编译器编译生成的可执行文件一定是基于某些指令集的，或者说隶属于某个大类的，比如：针对x86编译的程序，可以看出，面相某一类处理器的程序是无法在另一类处理器上运行的。        除了硬件，运行环境还包括软件，我们一般可以理解为操作系统。我们编写的程序都会显性或隐性的依赖操作系统的能力，显性依赖很好理解，在编写程序时依赖对应操作系统提供的SDK，比如：Win32API或者 .Net运行时，一旦依赖这些内容后，程序就会和具体的操作系统绑定。有同学会说，我就是一个简单的像gt一样的程序，只是依赖了标准库，这样编译出来的程序就和操作系统无关了吧？其实不然，这属于隐性依赖，虽然通过使用标准库，可以让我们不修改源代码在不同的操作系统上进行编译，但是它们都需要与具体操作系统中的二进制内容进行链接后才能生成可执行文件，而在某个操作系统下编译生成出了可执行文件后，该文件已经同具体的操作系统完成了链接，这种链接就是隐性依赖。        早期计算机硬件厂商很多，处于百花齐放的阶段，不同的硬件（CPU或输入输出设备），提供了不同的指令。那个年代的操作系统还很薄弱，具体的程序会选择使用硬件提供的指令，导致程序好虽好，但是既挑硬件又挑操作系统，程序编写起来很痛苦，移植很困难。程序与软硬件关系的变迁如下图所示：        如上图所示，X软件在早期是同时依赖软件（操作系统）和硬件（指令集）的，如果软件要有广泛的使用面，那就需要做许多版本。操作系统的快速发展，收拢了访问硬件的入口，使得软件只需要针对操作系统环境进行开发即可。从变迁过程可以看出，操作系统的出现使得程序能够与硬件逐步解耦，这样的依赖关系也会让人觉得有层次感，这也说明了好的架构是逐步演进出来的，但是时至今日，我们的设计中再出现依赖混乱的结构就不应该了。 程序与操作系统        程序依靠运行环境得以运行，而运行环境中的操作系统是一个承上启下的存在，向下抽象硬件，避免程序毫无依靠的野战，向上提供一致考究的API供开发者使用，也就是所谓的赋能。那么操作系统是如同宇宙大爆炸一样突然出现的吗？肯定不是，它也是逐步发展而来的，操作系统的发展过程如下图所示：        如上图所示，在没有操作系统的时代，所有的程序都要从头写到尾，如何启动和运行都需要自己负责，这时肯定就有人希望把这些通用的能力给沉淀下来，所以早期的监控程序就是操作系统的雏形。罗马不是一天建成的，操作系统也是如此，更重要的是，操作系统不是一个独立的程序，而是一个程序集合体。        程序运行在操作系统之上，由操作系统来管理硬件，访问硬件的形式变成了调用操作提供的API，这种间接控制硬件的方式，使得程序的移植方便了许多。以前文中，程序对操作系统的隐性依赖为例，程序源代码和操作系统的关系如下图所示：        如上图所示，程序的源代码（或产出物）依赖标准函数库，看起来程序没有直接依赖操作系统，但是实际上标准函数库就像接口一样，不同的操作系统会加以实现。操作系统实现的标准函数库底层还是会使用系统调用，因此程序的每次调用，最终都是系统调用，所以说操作系统决定了应用程序的能力上限。        操作系统之于程序的重要性已经不言而喻，但我们的硬件是直接与操作系统关联吗？看起来像是，毕竟我们拿到新电脑后第一件事就是安装操作系统。其实在操作系统启动前，首先会运行一个BIOS（Basic Input/Output System），这个系统一般存储在主机板的ROM中，有引导程序的能力。开机后，BIOS会确认硬件是否工作正常，比如：常见的内存自检等环节，当众多检查通过后，会启动引导程序。引导程序会读取磁盘的起始位置，按照要求加载操作系统到内存中，然后结束，此时控制权自然就落到了操作系统手中。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program-02.html":{"url":"book/program-program-program-02.html","title":"热风-硬件的执念","keywords":"","body":"程序！程序！程序！—— 硬件的执念        计算机只能够执行机器代码，也就是由0和1组成的编码，编码不是无规律的0和1，而是是由指令和数据构成的。        对于之前编译生成的gt程序，我们可以观察一下它对应的二进制编码是什么样的。使用vi工具，输入：vi -b gt载入编辑器，然后键入:%!xxd，可以看到二进制内容按照字节的格式化输出，如下图所示：        输出的二进制内容没有源码那么和善，因为它不是面向人类的，指令和数据混在一起，但早期人们写程序时，就是这样的开发环境，人手一个对照表，然后按照格式一点点的撸。后来有同学会认为这样开发的效率太低了，需要不断的查表切换，所以干脆就归纳出一套具有语义的标签，这些标签能够涵盖计算机的操作指令（包括操作计算机所关联的存储设备），这样程序写起来就方便一些了，而这些标签可以被称为助记符，使用助记符的程序设计语言就是汇编语言。 生成汇编语言        汇编语言是低级语言，它和机器语言属于同一个级别，只是有了助记符，能够让人容易理解，但是还是需要程序员像机器一样思考，通过搬运数据，指令运算来获得结果。C语言源代码可以由编译器生成出汇编代码，运行 cc -S -masm=intel gt.c > gt.s，可以获得对应的汇编代码，转换后的汇编代码如下所示： .section __TEXT,__text,regular,pure_instructions .build_version macos, 12, 0 sdk_version 12, 3 .intel_syntax noprefix .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: push rbp .cfi_def_cfa_offset 16 .cfi_offset rbp, -16 mov rbp, rsp .cfi_def_cfa_register rbp sub rsp, 16 mov dword ptr [rbp - 4], 0 lea rdi, [rip + L_.str] mov al, 0 call _printf lea rdi, [rip + L_.str.1] lea rsi, [rbp - 8] mov al, 0 call _scanf lea rdi, [rip + L_.str.2] mov al, 0 call _printf lea rdi, [rip + L_.str.1] lea rsi, [rbp - 12] mov al, 0 call _scanf mov edi, dword ptr [rbp - 8] mov esi, dword ptr [rbp - 12] call _gt mov esi, eax lea rdi, [rip + L_.str.3] mov al, 0 call _printf xor eax, eax add rsp, 16 pop rbp ret .cfi_endproc ## -- End function .globl _gt ## -- Begin function gt .p2align 4, 0x90 _gt: ## @gt .cfi_startproc ## %bb.0: push rbp .cfi_def_cfa_offset 16 .cfi_offset rbp, -16 mov rbp, rsp .cfi_def_cfa_register rbp mov dword ptr [rbp - 4], edi mov dword ptr [rbp - 8], esi mov eax, dword ptr [rbp - 4] cmp eax, dword ptr [rbp - 8] jle LBB1_2 ## %bb.1: mov eax, dword ptr [rbp - 4] mov dword ptr [rbp - 12], eax ## 4-byte Spill jmp LBB1_3 LBB1_2: mov eax, dword ptr [rbp - 8] mov dword ptr [rbp - 12], eax ## 4-byte Spill LBB1_3: mov eax, dword ptr [rbp - 12] ## 4-byte Reload pop rbp ret .cfi_endproc ## -- End function .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"\\350\\276\\223\\345\\205\\245\\347\\254\\254\\344\\270\\200\\344\\270\\252\\346\\225\\260\\357\\274\\232\" L_.str.1: ## @.str.1 .asciz \"%d\" L_.str.2: ## @.str.2 .asciz \"\\350\\276\\223\\345\\205\\245\\347\\254\\254\\344\\272\\214\\344\\270\\252\\346\\225\\260\\357\\274\\232\" L_.str.3: ## @.str.3 .asciz \"\\350\\276\\203\\345\\244\\247\\347\\232\\204\\346\\225\\260\\346\\230\\257\\357\\274\\232%d\\n\" .subsections_via_symbols        机器语言构成的指令代码，一般称为本地代码，本地代码和汇编代码是等价的，对应关系如下图所示：        如上图所示，从汇编代码生成本地代码的过程称为汇编，反之，叫做反汇编。 什么是汇编        汇编是由助记符和操作数组成的代码，助记符一般是指令，比如该指令：mov dword ptr [rbp - 4], edi，mov这关键字就是助记符，也是指令，代表着移动，它的使用模式是：mov $target, $source。dword是修饰符，代表着double word，也就是两个词，实际就是四个字节，而$target和$source所指代的部分就没那么容易理解了。        这部分内容就是寄存器的名称，现在可以理解CPU是由控制器（CU）、运算器（ALU）和寄存器（Register）组成，而汇编语言只对寄存器做了抽象，虽说调用add指令会隐性的驱动运算器进行运算，但是实际上在汇编代码中能出现的CPU部件就是寄存器了。寄存器的种类有很多，不同的CPU架构也会有所不同，按照功能分，常用x86寄存器如下表：        运算器会和对应功能的寄存器进行配合来完成计算，这就好比将数据（以电信号的形式）放置到指定位置（也就是寄存器的引脚）上后，方能驱动运算器进行运算，这个过程就好比武器中的步枪射击一样。        你需要将不同的数据（部件）拼好，然后再使用指令，也就是扣动扳机，来触发计算。        本文的目的不是讲述汇编语言，所以不会就汇编语言来展开，我们通过一个gt代码与汇编的对比来看一下汇编语言是如何操作硬件的。在gt.c中，有gt函数，代码如下： int gt(int a, int b) { return a > b ? a : b; }        这段代码逻辑很简单，通过三目运算符，返回两个参数中，大的一个（当然如果参数相等，返回后一个参数）。类似的写法在不同的编程语言中都可以看到，大家已经见怪不怪了。接下来，看一下这个C语言函数对应的汇编代码，如下所示： _gt: ## @gt .cfi_startproc ## %bb.0: push rbp .cfi_def_cfa_offset 16 .cfi_offset rbp, -16 mov rbp, rsp .cfi_def_cfa_register rbp mov dword ptr [rbp - 4], edi mov dword ptr [rbp - 8], esi mov eax, dword ptr [rbp - 4] cmp eax, dword ptr [rbp - 8] jle LBB1_2 ## %bb.1: mov eax, dword ptr [rbp - 4] mov dword ptr [rbp - 12], eax ## 4-byte Spill jmp LBB1_3 LBB1_2: mov eax, dword ptr [rbp - 8] mov dword ptr [rbp - 12], eax ## 4-byte Spill LBB1_3: mov eax, dword ptr [rbp - 12] ## 4-byte Reload pop rbp ret .cfi_endproc ## -- End function        上述汇编代码中，#号后面为注释，汇编器不会关注它。以.开头的，比如：.cfi_startproc为伪指令，表示函数定义的开始。代码由上至下的执行，先看头尾两部分的指令，即： push rbp mov rbp, rsp ## 略 pop rbp ret        push指令表示将一个（rbp寄存器的）值入栈，pop指令表示出栈后将值赋给rbp寄存器。栈是一种LIFO（后进先出）的数据结构，一个CPU核心可以认为有一个栈，rsp即栈寄存器，它指向栈的顶部。上述指令相当于做了两件事情：第一件，将基址寄存器rbp的值保存到栈中，然后将栈寄存器rsp的值放入到基址寄存器rbp中，在执行完一系列操作后，会使用pop指令弹出先前保存的基址寄存器rbp值，将其设置到基址寄存器rbp。这个过程相当于保存rbp中的旧值，然后做完操作后再恢复，保证函数的执行对基址寄存器rbp是无副作用的。第二件，调用ret指令，隐性出栈一个值，并将值设置到程序计数器，也就使得函数执行完成后，执行链路能够返回到调用端。 上述指令以及函数调用的过程会在稍后章节进行详细介绍，现在如果看着晕晕的，也没关系，只需要记住rbp是基址寄存器，可以通过它进行地址运算，而rsp是栈寄存器，它始终指向栈的顶部。        在函数体内，会移动寄存器中的值到栈上，然后调用cmp指令，cmp指令会比对指令后面跟随的两个参数，然后将比对结果放置在标志寄存器中，比对的值是一个二进制数值。根据比对的结果数值，如果jle指令判断比对结果小于等于将会跳转到LBB1_2，接着对应区块的代码会将第二个（也就是较大的）参数移动到累加寄存器eax中，该寄存器被用来承担装载函数调用返回结果的职责。 存储在标志寄存器的值包括（且不限于）：是否大于，是否等于，是否小于，按照不同的位构成，方便后续的判断。        可以看到相对于C语言的几行代码，汇编代码需要一系列的指令操作，包括移动，比较和跳转，除此之外还需要用到不少寄存器，感觉复杂很多，但这其实就是计算机硬件理解程序（逻辑）的方式。 运行可执行文件        可执行文件，也就是程序，在未运行的时候，会存储在磁盘上，当运行时，需要载入到内存。从汇编代码可以看出，载入内存的程序，就像一个数组一样，当载入部分后，就可以将main函数入口的地址设置到程序计数器，随后就可以开始执行了。每执行一行指令，会增加对应的步进，重新设置程序计数器，然后CPU会根据程序计数器中的值，也就是地址信息，从内存中取出下一条指令，这样顺序执行就算做到了。程序、CPU和内存的运行时关系如下图所示：        如上图所示，内存中存有程序的指令和数据，当程序运行时，CPU会从内存中读入指令和数据，再将这些内容读入到寄存器中进行处理。这里不是说CPU就把内存当缓存用，而是需要将执行的指令加载到寄存器中方能运行。如果指令中有rbp基址寄存器参与运算，CPU也会按照地址去取内存中的数据。这种由寄存器、内存到磁盘的多级存储，是现代计算机的典型结构，越靠近CPU就越快，越远离CPU数据访问的单位（或者块头）就越大，CPU访问寄存器的速度可以快到1ns，而访问内存会下降到100ns。        程序在内存中一般分为四个部分，分别是：变量、函数、栈和堆。先来说说变量和函数，在编写的程序时，代码是将变量和函数混合在一起，这符合我们的直观感受，但是编译成可执行文件后，变量和函数会被分开，放置在不同的段（segment）中。编译器这么做，听起来觉得对可执行文件的约束有些强，但实际我们费劲脑汁编写的程序不就是由数据结构和算法（函数或方法）构成的吗？而它们不就是对应着变量和函数吗？放在一起的目的是为了在内存中更加的密实，更利于缓存的命中。 在之前的汇编代码中，第一行指令.section __TEXT,__text,regular,pure_instructions，就声明了几个段。        接下来看一下栈和堆，栈一般是由编译器生成的，在函数调用和本地变量分配时，都会使用到栈，而开发人员是无法直接感知到栈存在的。栈负责存储临时的数据，通过push和pop指令进行操作。在x86体系结构下，内存中的栈空间如下图所示：        如上图所示，内存好比一个数组，栈空间会在内存中选取一块大小合适且连续的空间作为存储，一般来说栈空间不需要很大，因为它和执行线程相关，本地变量的数量有限，同时函数在调用过程中，不断的入栈和出栈操作使得栈实际的消耗是相对有限的。如果用数组的视角看栈空间，下标小（也就是低位地址）是栈顶，而下标大的是栈底，如果栈是空的，那么数据从栈底开始放置，而栈顶会随着数据的存入和弹出而不断的变化。通过push指令，可以向栈中存入一个值，栈顶会向上移动，而esp（或rsp）寄存器始终指向栈顶数据，当调用pop指令时，会将esp指向的数据取出，并将esp指向下一个元素。        以下面代码为例，函数中局部变量都会分配在栈上。 int x = 1; int y = 2; int z = x + y;        上述代码，在栈中的结构如下图所示：        对应的汇编代码可以是： mov ebp, esp; mov dword ptr[ebp + 4], 1 mov dword ptr[ebp + 8], 2 mov eax, dword ptr[ebp + 8] add eax, dword ptr[ebp + 4] mov dword ptr[ebp + 12], eax        能够从对应的汇编代码看出，程序代码虽然可以写的飘逸，但是局部变量的分配已经在编译的时候就确定了，所以生成的汇编代码可以将我们操作任意本地变量的引用替换为地址，不论你写的多么难以理解，但你就是无法逃出如来佛的手掌心，因为你在声明变量和编写逻辑的时候，就已经将变量的类型、申请和释放告知给了编译器。 实际由于编译器的优化，汇编代码会使用寄存器来替换掉部分栈的操作，用于提升执行效率，所以对应的汇编代码只是概念上的（对等）。        另外，在main函数对应的汇编代码中，可以看到这行指令add rsp, 16，它表示将栈寄存器rsp的值增加16，也就是栈顶向下移动4个int（或者说16个字节），这样操作的结果就是不通过pop指令就完成了栈中元素的清理。        如果说栈是程序运行的内部要求，那堆就是开发者用来处理动态内存需求的工具。我们的程序运作时，会根据实际的情况来动态分配内存，不可能做到提前的预估，而对于这种动态内存空间需求，就需要使用堆。以C代码为例： char *str = (char *) malloc(15);        通过malloc函数，也就是内存分配（memory allocate)，可以开辟15个字节的内存空间用于存放数据，这个内存空间就在堆空间中。程序运行时，栈空间的大小是可知的，但是堆空间是根据程序的实际情况来计算分配的，这点对于Java程序来说，也是一致的。        程序在顺序执行中，遇到本地变量，就使用栈空间，那么不断的入栈，何时出栈呢？答案是函数调用，使用栈的原因就是它很适合完成函数调用，我们编写程序，实际就是在编写函数，从主函数main开始，不断的调用各种函数，有标准库函数，也有自定义函数。程序设计中的函数与数学中的函数有些类似，有输入和输出，函数体内部就是函数逻辑，函数逻辑依靠输入进行计算，最终将输出返回。以gt程序代码为例： #include int gt(int a, int b); int main() { int a, b; printf(\"输入第一个数：\"); scanf(\"%d\", &a); printf(\"输入第二个数：\"); scanf(\"%d\", &b); int x = gt(a, b); printf(\"较大的数是：%d\\n\", x); return 0; } int gt(int a, int b) { return a > b ? a : b; }        如上述代码所示，gt函数接受两个int类型参数，并返回较大的一个。我们观察一个场景，在调用gt函数前后，对于栈而言需要做什么。首先是参数，可以通过入栈的形式将两个变量传递给gt函数，但是调用gt函数完成后，还需要返回到调用端（也就是代码int x = gt(a, b);），这该如何完成呢？另外，如何能够让程序调用gt函数呢？这么看，实现一个普通的函数调用是相当困难的呀！可以看到，函数的调用涉及到参数的保存、控制流的变化（调用函数）、函数计算、结果返回、控制流的返回（调用函数返回）以及参数的清理，至少6件事情，这些事情肯定不是一个栈就能够做成的，它需要一些寄存器的帮助。        提到控制流的变化，就需要程序计数器的支持，而结果返回一般需要指定一个寄存器就好，常见会使用累加寄存器rax来做，当然栈寄存器rsp和基址寄存器rbp也是离不开的。首先考虑参数的保存，以示例中a和b两个参数为例，先将其入栈，考虑到函数调用后会返回，将int x = gt(a, b);，也就是函数调用后的语句地址也存入栈中，此时栈和寄存器的状态如下图所示：        接下来，控制流的变化，也就是函数调用，需要使用到程序计数器，直接将gt函数的地址设置到程序计数器即可，在时钟调度的驱动下，CPU会取出对应的指令进行执行，此刻函数就被调用了。该过程如下图所示：        可以看到对于函数的调用，除了需要将参数入栈，还必须将函数返回的语句地址入栈，同时更新程序计数器到函数地址，而后面这两步是需要一并完成的，因此汇编代码中有对应的call指令来替代。        在函数中运行，就如同执行正常逻辑一般，只需要从栈中取出对应的参数进行使用即可，只是在函数的入口和出口，存在特定的指令模式。以gt函数为例，部分指令如下： _gt: ## @gt push rbp mov rbp, rsp .cfi_def_cfa_register rbp mov dword ptr [rbp - 4], edi mov dword ptr [rbp - 8], esi // 略 mov eax, dword ptr [rbp - 12] pop rbp ret ## -- End function        在进入函数后，第一行指令push rbp是将当前基址寄存器的值入栈，然后将栈寄存器rsp的值设置到基址寄存器rbp中，这两条指令的目的是期望使用基址寄存器操作栈中的参数，同时将旧的基址寄存器中的值保存起来，以便后续能够恢复。在函数体中，可以看到通过类似基址寄存器[rbp - 4]的运算来取栈中的数据，[rbp - 4]表示从栈顶开始，往回4个字节的数据。        函数体末尾会通过pop rbp进行出栈，而出栈的值一定是先前在函数入口保存的基址寄存器的值，而将该（旧）值重新赋给基址寄存器，目的就是保证函数的无副作用。函数体经过执行，最终返回的值通过mov eax, dword ptr [rbp - 12]指令，将结果设置到累加寄存器eax中，调用函数后，直接从该寄存器中取值即可。 使用累加寄存器作为函数返回值的中介是C语言编译器的习惯，只需要确定使用一种合适位宽的寄存器作为函数返回值的中介即可，因为函数返回无非就是原生类型的数据或引用（指针）数据，数据位数其实都是固定的。        函数末尾除了出栈恢复基址寄存器，还需要出栈函数调用后的语句地址，并将该地址设置到程序计数器中，这样函数的执行就完成返回了。在汇编语言中，通过ret指令能够完成上述两个操作。这个特定的指令模式执行过程如下图所示：        函数返回后，如果参数不再使用，会通过修改栈寄存器rsp，逻辑上忽略先前入栈的参数，这样参数的清理也就完成了。函数调用的参与者有很多，但是明确了它们各自的职责后，会发现这个过程是很考究且有趣的，通过栈以及几个寄存器的配合，就可以使用计算机运行的基本原理包装出函数调用，而函数又是面向开发者的有力抽象，仔细想想，设计者真是很聪明啊！ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program-03.html":{"url":"book/program-program-program-03.html","title":"热风-编译的力量","keywords":"","body":"程序！程序！程序！—— 编译的力量        如果说操作系统是程序运行的基础，那么编译器就是源代码到程序的助产士，编译的过程如同分娩，编译器承担孵化应用程序过程中难以想象的复杂和困难，经历了一道道的工序，最终程序能够如同新生儿一样呱呱坠地，我们在赞美程序功能的同时，也能感受到编译的伟力。        从源代码到程序（或者说可执行文件）指的就是编译，但我们所说的编译实际上是一个比较笼统的概念，其实它是由三部分构成的，如下图所示：        如上图所示，编译器先将程序源代码翻译为汇编代码，再由汇编器将汇编代码转换为二进制的目标文件，最后目标文件由链接器将其与运行环境进行链接，生成出可执行文件。接下来用一个C语言例子演示一下这个过程，代码如下： #include int main() { printf(\"你好！\\n\"); return 0; }        首先将源代码保存为hello.c，然后使用命令cc -S -masm=intel hello.c > hello.s让编译器将将C源代码文件翻译为汇编代码，hello.s（部分）汇编代码如下： .section __TEXT,__text,regular,pure_instructions .build_version macos, 12, 0 sdk_version 12, 3 .intel_syntax noprefix .globl _main ## -- Begin function main .p2align 4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: push rbp .cfi_def_cfa_offset 16 .cfi_offset rbp, -16 mov rbp, rsp .cfi_def_cfa_register rbp sub rsp, 16 mov dword ptr [rbp - 4], 0 lea rdi, [rip + L_.str] mov al, 0 call _printf xor eax, eax add rsp, 16 pop rbp ret .cfi_endproc ## -- End function .section __TEXT,__cstring,cstring_literals L_.str: ## @.str .asciz \"\\344\\275\\240\\345\\245\\275\\357\\274\\201\\n\" .subsections_via_symbols        在前文中已经简单介绍过汇编语言，这种以指令和操作数组成的代码，直接与硬件打交道，看起来简单的语法能够演绎出丰富的特性。接下来，使用命令cc -c hello.s > hello.o，将汇编代码转换为目标文件，目标文件不同于可执行文件，它没有完成本地化，但是它已经被翻译成CPU能够解释执行的本地代码，是二进制形式的，这里就不做展示了。        目标文件还需要生成出可执行文件方能运行，此时链接器就能派上用场，链接器对当前所处的软件环境非常熟悉，知道系统的目标文件（或者库）所在位置。它会将源代码生成的目标文件中 “或缺” 的地方用系统的目标文件加以补充，一旦完成融合，可执行文件就生成了。使用命令cc hello.o -o hello，链接器会将目标文件生成为可执行文件。 链接器所熟悉的系统目标文件，比如：库文件对应的目标文件。        整个源代码编译（和链接）以及可执行文件（或程序）运行的过程，如下所示： % cc -S -masm=intel hello.c > hello.s % cc -c hello.s > hello.o % cc hello.o -o hello % ./hello 你好！        接下来，分别简单介绍一下编译和链接的过程。 编译的过程        编译的目的是将源代码转换为汇编代码，这个过程中编译器会对源代码进行语法、句法和语义解析，再根据系统运行环境，将源代码转换为汇编代码。不同语言的编译器能够支持各自语言的语义和语法解析，并且它们一般都会有一个对应表，这个对应表一边是自身编程语言的语法模式，而另一边就是汇编指令，编译器需要承担这个翻译的工作。编译器所处的位置，如下图所示：        如上图所示，编译器处于操作系统、硬件指令集和编程语言（概念）三者的中心，它需要和三者紧密的联系。编译器和语言相关，这点很自然，毕竟不同语言的语法结构都需要编译器能够理解、支持和翻译。编译器和操作系统相关，不同的操作系统对应用程序的布局、描述和支持都不一样，所以编译器和操作系统是相关的，它需要编译出运行在某种操作系统下的程序。编译器又和硬件指令集相关，硬件指令集狭义上就是CPU指令集，是x86还是arm，这是不一样的，毕竟寄存器和汇编指令都不一样的。        编译器将源代码转换成汇编代码的过程中充满了挑战，编译器不仅应对了这些挑战，还能够在这个过程中进行优化，比如：会尽可能多的使用寄存器，而不是使用（存储在内存中的）栈，原因就是寄存器的效率更高，毕竟离CPU更近一些，相对于访问内存的速度，访问寄存器快了几十倍。如果源代码中有一些用不着的变量声明，或者调用函数后忽略返回值，这些都会被编译器优化掉，可以看出编译器是编程语言设计者同普通开发者之间的桥梁。 链接的过程        汇编代码经过汇编器进行转换后，会生成目标文件，目标文件需要链接后才会生成能够运行在当前环境的可执行文件。程序想在运行环境中启动，需要得到软（操作系统）和硬（CPU指令集）的支持，目标文件已经与硬做好了适配，还需要同软打通，即目标文件需要同操作系统中已有的目标文件进行链接，才能使用操作系统提供的能力，这个过程如下图所示：        如上图所示，源代码生成了目标文件，链接器再根据源代码中依赖的线索（或提示）找到操作系统中对应的目标文件，从操作系统的目标文件中抽取出需要的部分同源代码生成的目标文件进行链接，最终生成可执行文件。        和汇编代码一样，可执行文件中的机器代码在运作时，需要访问内存，而访问内存就需要指定内存的地址，这样程序就会和硬件绑定了。为了避免硬件绑定，以及不同的程序不会产生冲突，生成的可执行文件都会使用虚拟的内存地址，而在可执行文件的头部，链接器会加上再配置信息。        可以设想我们平时开发的（服务端）应用，对于数据库连接池的配置肯定不会写死在代码中，而是放在一个配置里，代码中只会依赖其配置名，这个配置名就好比是再配置信息。操作系统会做好内存的管理，给到应用一个合适的配置，使得多个程序的运行不会相互冲突，同时让程序与硬件解耦。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program-04.html":{"url":"book/program-program-program-04.html","title":"热风-数据的表达","keywords":"","body":"程序！程序！程序！—— 数据的表达        大部分计算机使用者感慨于五彩缤纷的应用程序但不知道计算机内部却只认识0和1，这么单一的表达形式如何能够做出那么多伟大的应用程序呢？原因就是计算机对于数据的理解是建立在严格考究的规范上的。        比如：一段二进制内容1010101，它可能是EXCEL程序中展示的一个数值，也有可能是你正在查看照片中的一部分，还有可能是属于正在播放流媒体视频中的一帧。数据形式看似这么单一，但不同的数据类型有不同的规范和要求，而不同的数据类型使得计算机能够被用来构建丰富的应用程序。 为何选用二进制        硬件如何描述数据呢？这么说有些抽象，换句话说，（我们先考虑）如何能够在集成电路（Integrated Circuit）上表示一个数字？集成电路有非常多的引脚，就好像蜈蚣一样，这里看一下集成电路芯片的样子，如下图所示：        如上图所示，芯片长的像蜈蚣一样，那些金色的引脚看起来都一样，但实际每个都有所不同。如果你是芯片的设计师，让你用这个芯片表示一个数字，该如何做呢？感觉挺难，不过我们慢慢来。首先芯片离不开电，它需要通电，毕竟没有电这玩意儿完全无法工作。接下来，如果只考虑一排引脚，可以像十进制数字一样，每个引脚都能表示一个数，这样10个引脚就能表示一个10位的数字了，最大能够表示几十亿呢。那问题就来了，每个引脚如何表示一个数？既然用电，那就用电压来区分，0到9伏表示0到9，岂不美哉？没错，这样完全可以，你设计出了一款十进制的芯片，如下图所示：        如上图所示，这款十进制的芯片被制造出来了，但是实际工作中，电压有些波动，一个引脚实际电压是4.5伏，这表示它的值是4呢？还是5呢？这就要加固整个芯片的电源以及容错机制了，感觉血压随着成本一下就上去了。因此现实中的芯片并没有做的这么灵活，而是只用有电和没电两个状态，也就是说一个引脚，在一个时刻，只会处于两个状态中的一个而已。使用直流电压0伏和5伏两个状态，如果芯片在有直流电源接入的情况下，不同的引脚就会处于不同的状态，如下图所示：        这样下面5个引脚的电压如果从左到右去看就是，0伏、0伏、5伏、0伏和5伏，如果我们认为0伏是0，而5伏是1，那么芯片的引脚输出可以表示为00101。再进一步，如果用二进制来理解00101，它不就是代表着十进制的5么？从工业（或量产需要）以及实现难易度考虑，一个引脚只有两个状态，以及使用二进制来描述数字是综合成本较低的解决方案，由于二进制在计算时还具有十进制不具备的逻辑运算特性，诸如：取反、求交（与或）以及异或，使得建立在布尔代数基础上的二进制运算有了理论依据，这才使得计算机沿着二进制走下去有了底气。        从十进制到二进制，单位信息密度低了不少，两位十进制可以描述的数字，二进制需要更多的位元，那么有没有在密度和实现难易度上更好的进制解决方案呢？实际是有的，记得在汇编语言这门计算机专业课上，老师板书推导过，最合适的进制是自然常数e，也就是2.718…进制，四舍五入，即三进制。如何能够实现3进制呢？可以规定0到3伏为0，4到6伏为1，而7到9伏为2，这样一定程度增加了使用电压来实现不同状态的适应性，但现实世界中谁会这么做呢？其实还真有人这么做，红色苏联就在这条路上探索了一段时间，还真做出来了三进制计算机，但最终没有继续下去，也真是挺可惜的。 二进制数据的表达        既然二进制是计算机存储和计算的基础，也是芯片引脚上非黑即白的状态，那么作为操作计算机的人，我们更加熟悉十进制，并且编写程序时，数值也是以十进制的形式体现的。计算机能够理解二进制，并用二进制进行运算，但是人们输入的十进制内容需要转换为二进制，同样计算机处理完成后的二进制内容也需要再转换为十进制输出，这样就需要看一下这两种进制如何进行互转了。        我们先看一下整数，以二进制数010011为例，可以用位权多项式的方式，计算出十进制，计算过程如下所示：        如上图所示，二进制010011对应十进制的值为19，其中绿色的指数为位权，红色的0或者1是二进制各位对应的值。类似010011，这样的6位二进制数值所能描述的数值是有上限的，并且它需要有6个引脚来对外（或者对内输入）作为物理基础，如果计算的场景不一样，芯片的引脚数量就不一样，那会造成工业界的分裂，无法实施量产，因此，一般以8个引脚为一个概念意义上的单位，称之为字节。 能够一次处理4个字节，即有至少32个引脚的芯片称为32位芯片，而能够一次处理8个字节数据的就是64位芯片了。        以8个引脚来描述一个整数数值听起来挺不容易了，毕竟要堆那么多晶体管，但问题是数值不可能都是正数，还有负数，那么负数该如何表示呢？我们可以使用8个引脚中的最高位引脚，如果它为1则表示是负数，这么以来原来十进制19，8位二进制就是00010011，-19是不是就是10010011呢？其实不是的，负数是使用补码的形式来存储的，计算补码的方式分为两步，如下所示：        如上图所示，经过取反和加1两步之后，得到-19对应的二进制表示形式为11101101，不过补码的表示形式显得不是那么易懂，为什么不使用10010011表示 -19呢？其实是可以的，只不过考虑到后续二进制运算的实现，使用补码表示会有一些优势。比如：考虑19 + (-19)，如下图所示：        如上图所示，可以看到面对8位二进制加法，19 + (-19) = 0，在计算实现上会显得容易些，溢出到第9位的数会被舍弃。二进制整数转为十进制可以用多项式的方式进行相加，那么十进制数值如何转为二进制呢？答案是，循环除以2，计算的方式如下图所示：        如上图所示，将19不断的除以2，不能整除的余数会是1，如果能够整除则是0，自下而上就是二进制的值10011，为什么是自下而上呢？答案很简单，在下面的余数毕竟是除了更多的2得来的，在二进制数值中也就需要放在高位。        接着再看一下小数（或者浮点数），二进制形式表示小数还是需要用小数点用来划分整数和小数部分，十进制是这样，二进制也亦然。以二进制1011.0011为例，它代表十进制的小数是多少呢？之前的整数换算使用了多项式，其实小数换算也是一样的，如下图所示：        如上图所示，二进制的小数到十进制的转换方式同整数转换方式一致，就是多项式相加，只是位权在小数点之后为负数，二进制小数1011.0011对应的十进制小数为11.1875。十进制小数到二进制小数的转换方式就和整数转换方式不一样了，对于小数部分，它采用的方式是不断乘以2，然后取余数，以十进制1.625为例，整数部分是1，在二进制中也记为1，接下来看小数部分，也就是0.625，转换过程如下图所示：        如上图所示，二进制小数换算过程通过乘以2，取高于小数点的余数得到十进制0.625对应的二进制数值为0.101。经过上述介绍，我们对二进制与十进制在整数和小数上的互转有了认识，有兴趣的同学可以自己试一试。概念上能够完成数值的互转，还是要考虑一下二进制数值如何在芯片上完成读写，毕竟概念还是要落地的，前面提到了引脚呀、电压呀，那么我们把这个8位的芯片做一下延展，变成这个样子，如下图所示：        如上图所示，先看左上角的IC芯片，它拥有12个引脚，下面8个，上面4个。下面8个引脚很好理解，它们表示8位二进制数值的内容，可以向它们存入值或者读出值，存入和读出就需要感受8个引脚的电压，这就需要VCC（Volt Current Condenser）电路供电电压和GND（Ground）地线，其中VCC是5V的直流电压，时刻有电是IC能够保持状态的基础，如果掉电了，那就不用谈了。外部系统如何能够向芯片中存入数值呢？又如何读出数值呢？这就需要控制信号，即WR（Write）和RD（Read）两个引脚，当它们通电时，操作芯片数据才有意义。        以写入举例，可以先将8位二进制数值按照对应位给予芯片D0至D7这8个引脚进行通电，然后再向WR引脚通电（施加5V电压），这时芯片就会记录当前D0至D7对应的电压，使得给定的8位二进制数值被存储到芯片中。如果需要读出数据，可以向RD引脚通电，然后就可以 “感受”芯片D0至D7引脚对应的电压，而这电压就是先前存入在芯片中的二进制数值。        这么看来，使用芯片来存储二进制数据还是可以做到的，但实际上芯片中需要大量的晶体管来完成这个工作。如果是保存二进制整数，每个数据引脚就可以被设定对应不同的二进制位，这样64位芯片，在数据传输层面就至少需要64+4=68个引脚，虽然实现起来很复杂，但是64位的二进制整数已经可以描述非常大的数值了。整数可以这么做，那小数该怎么做呢？可以把芯片设计成，上面一排数据引脚表示小数点之前的值，下面一排数据引脚表示小数点之后的值，不就可以解决了吗？没错，只要你是计算机的设计者，这么做完全没有问题，但是仔细想想，这种方式描述小数的成本太高，或者说使用的引脚会很多。        考虑一个十进制小数：0.0001，转换成二进制后为：0.000000000000011010，如果使用之前的设计方式，就需要芯片下面至少有18个引脚，这才只是表达了一个十进制的0.0001，如果是十进制的0.00000001呢？可以看出来，这种方案虽然直接，但是无法在寸土寸金的引脚数量上做好文章。那么有没有“压缩”数值表达的方式？其实是有的，科学计数法，真的很科学哦。以十进制的0.00000001为例，就可以记为1 X 10-8，可以看到科学计数法描述的数值包含元素就只有尾数（1）、基数（10）和指数（-8）这三个，描述元素少了，自然就能用更少的引脚数量来支撑更丰富的小数了。以二进制科学计数法为例，相应模式如下图所示：        如上图所示，二进制科学计数法需要四个组成部分来描述一个小数（当然包括大于1的，比如：1.111），这四个组成部分包括：符号、尾数、基数和指数，由于专门用来描述二进制，所以基数就是2，那么就只剩下了符号、尾数和指数，以先前的二进制小数0.000000000000011010为例，它的科学计数法描述形式是：+1.101 X 2-1110，其中 -1110是十进制的 -14，但是它也等同于 +11.01 X 2 -1111，过于自由肯定不是好事，所以就需要有一个标准来描述尾数和基数了。        对于尾数而言，小数点可以左右移动，那么就以 “1.*”这种模式来统一对于小数的描述形式，比如：        （A）1111.1111 = 1.111111 X 211        （B）0.0011 = 1.1 X 2-11        （C）100.0011 = 1.000011 X 210        模式统一后，指数的符号又遇到了问题，毕竟指数也是带符号的呀！难道还要浪费一个指定的引脚？都已经到了这一步了，科学计数法表示的小数运算器都不一定搞得定，那就再拍一下脑袋，对于指数而言，使用一种特殊的表示系统，即EXCESS系统，它将二进制中间值作为0，向左为负数，向右为正数。以8位指数为例：01111111表示0，如果要正数3，那就是01111111 + 11 = 10000010，如果是-1，用EXCESS表示就是01111110。        小数用统一的模式来表达，尾数就确定了，而指数又用EXCESS系统表达，负数也能表示了，这样有限的引脚就能描述很大的小数了。在IEEE规定中，存在32位（或引脚）的单精度小数和64位的双精度小数，它们对于符号、尾数和指数的空间划分如下图所示：        如上图所示，单精度小数是32位的，它由1个符号位，8个指数位和23个尾数位构成，而双精度小数是64位的，它所能表示的范围和精度都要好于单精度小数。以二进制小数1011.0011为例，科学计数法表示为1.0110011 X 211，接着来看一下它在32位float下的存储结构。        首先，符号为是0，表示正数，这个很简单。然后计算指数，二进制11（十进制3）通过EXCESS系统，在8位二进制情况下，可以表示为10000010。最后，计算尾数，尾数为1.0110011，由于尾数的模式是 1.*，所以会省略 1.* ，可以得到7位二进制0110011，由于要放到23位的空间里，所以后面会补0。通过符号、指数和尾数的计算，二进制小数1011.0011在32位的单精度小数下会被表示为：11000001001100110000000000000000，这样32个引脚就能表示小数了。        可以看到小数的表达非常复杂，而小数的计算那就更麻烦了，想想两个指数不同的小数进行运算，感觉头都大了。因此对于计算机而言，整数运算要远比浮点运算简单和高效。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/program-program-program-05.html":{"url":"book/program-program-program-05.html","title":"热风-协同的奥义","keywords":"","body":"程序！程序！程序！—— 协同的奥义        程序的运行离不开软硬件的支持，硬件能够运行软件并体现出些许智能，就在于其自身也是由一个个逻辑构件组成的，同时它与软件之间定义了明确的接口，这个接口就是机器语言（或指令）。我们开发程序使用的编程语言，最终都需要依靠编译器将其转换为对应环境的（机器语言）本地代码。机器语言构成的本地代码实际就是针对硬件的指令和数据，与网络协议一样，它们一定是有意义的，虽然是一堆0和1组成的代码，但它如同音乐一般，会有节奏的律动。        CPU会按照约定取出指令进行执行，但计算机不止CPU一个构件，它还包括了内存、硬盘、显卡以及鼠标等，这么多构件需要相互协同，让使用者感觉到它们就像一个整体。接下来，笔者会尝试介绍计算机的主要组成部分，看看它们是如何配合完成工作的。 计算机的组成部分        冯·诺伊曼在1945年6月提出了现代计算机的范式，该范式被称为“冯·诺伊曼结构”，按照这种结构建造的计算机也被称为通用或冯·诺伊曼计算机。冯·诺伊曼计算机主要由运算器、控制器、存储器，输入和输出设备共计5个部分组成，它的的特点是：程序以二进制代码的形式存放在存储器中，所有的指令都是由操作码和地址码组成，指令按照顺序执行，以运算器和控制器作为计算机的中心。上述这段内容在 《计算机组成原理》专业课中都会介绍，但那时懵懂的少年是无法领悟这么抽象的结构和致密的内涵，而当工作多年后，随着在系统结构知识上的不断求取和实践，再次回想这些话时，就会有很多感慨吧。        冯·诺伊曼计算机对于存储器没有区分出内存和外存，而中央处理器所包括的寄存器也没有在结构中体现出来，不过这些都是对计算机运行效率优化而出现的构件，并不会对其运行的核心概念产生影响。冯·诺伊曼计算机的组成部分，如下图所示：        如上图所示，CPU由运算器（ALU）、控制器（CU）和寄存器（Register）组成，它是整个计算机的核心。存储器分为内存和外存，前者是我们常说的内存（Memory），CPU会与它直接通信，外存则是偏向I/O存储的设备，比如：硬盘。输入和输出设备都属于外设，常见的输入设备，比如：鼠标和键盘，输出设备也有很多，比方说现在正在观看本文章的显示器就属于输出设备。        这么多的设备构件需要协同起来，才能从输入获取问题，载入存储器，通过运算器和控制器进行计算，最终将问题的结果发送到输出设备上。这些设备的协同离不开总线，总线可以理解为公共线路，在公共线路上的信号都能被接入线路的设备所获取到，而总线分为三类：控制、数据和地址。        控制总线原点在控制器，它是核心中的核心，负责访问内存获取指令，解释并执行指令，它也会驱动运算器进行运算，当然访问输入设备和输出设备就更不在话下了。数据总线的目的是实现设备构件之间进行数据传输，不是任意设备之间都可以自由的进行数据拷贝，比如：快速的CPU就只会同内存打交道，就这CPU还会嫌内存慢，不会同低速的输入和输出设备直接交互，因此数据基本都会在内存中进行中转。地址总线主要是面向CPU和内存的，内存是一段连续的空间，但是它有地址编号，根据地址编号就可以存入（或取出）数据，实际内存更像一个Map结构，其中键是内存地址，而值是二进制数据，因此控制器操作内存就需要向地址总线输入地址信息。        接下来，我们详细看一下内存、磁盘、外设和CPU。 内存如何保存数据        CPU与内存联系是非常紧密的，现代计算机中，CPU到内存的访问延时在100纳秒以内，已经非常快了，虽然它们离CPU很近，毕竟CPU和内存之间是通过线缆来连接的，相对于CPU中的缓存而言，距离还是很远的。内存一般被称为RAM（Random Access Memory），它会分为两类：DRAM和SRAM，两者的区别在于前者需要不断的刷新才能保存数据，后者工艺复杂成本高，不需要刷新就可以维护数据，同时访问效率更高，常被用于CPU中的缓存，普通消费者购买的内存条一般都是DRAM，它长得大概是这样的，如果不是，可能穿了信仰加成的眩光马甲，如下图所示：        如上图所示，内存条上有黑色的内存颗粒，它们是用来存储数据的，而绿色PCB板上密密麻麻的布线以及金黄色的引脚感觉科技感十足，其中引脚一般称为金手指。布线过于复杂，这里不做展开，但可以看到布线连接的引脚们分为左右两个部分，这两个部分分别会连接到地址总线和数据总线。金手指上除了地址总线和数据总线的引脚，还有电源引脚和负责控制的引脚，从概念上来看，内存如下图所示：        如上图所示，内存芯片与《程序！数据的表达》中介绍用于存储数据的芯片，二者引脚结构很相似，只不过是多了地址信号的一组引脚，毕竟内存不是被设计成只能存储一个值的存储器。数据写入内存时，首先会指定内存地址，这就好比指定了数组的下标或者Map结构中的键，然后再将（二进制）数据（按位）输入给数据引脚，最后给WR（Write）引脚通电（也可以说设为1），输入的数据就会被保存在（内存中的）相应地址中。数据读出内存时，也需要先指定内存地址，然后给RD（Read）引脚通电，对应内存地址存储的数据会透过相应数据引脚，以电压的形式呈现。        内存中存储的数据不存在数据类型的概念，以地址（或数字编号）的形式访问内存，存取的是一个固定位宽的值。这么一看，内存挺死板的呀！很难想象程序中五花八门功能都会存放在一个如同棋盘般规矩的空间中，不过话说回来，不以规矩，无以成方圆，数据类型不在内存中，在编程语言中定义，而它们又能很好与内存一起工作。假设一款内存芯片，它每个地址只能存储8位的二进制数据，使用C语言定义了几个不同的变量，如下图所示：        如上图所示，byte、short和int这三种数据类型占据了不同数量的地址空间，内存不会将任意类型的数据都放在一个地址中，而是会跨多个地址来存放数据。对于byte类型的数据，例如：byte x = 123;这款8位的内存，使用一个地址对应的内存空间即可存下，所以它（或变量x对应的值）只会占据1个地址。short和int类型的数据就占据多个地址，其数据自身的地址是连续的，以int类型的变量为例，int z = 123;其中z会占据4个地址，由于123这个值对于32位的int而言，处于低8位，所以这种（多字节）数据排列方式称为小端模式（Little Endian）。        在一般的编程语言中，内存地址不会被直接操作，虽然能隐约感受到操纵的是地址（或引用），比如：Java语言中这行代码String s = “hello”;，对Java了解的同学都知道，s变量是一个引用，它指向一个字符串对象，字符串对象保存的有字符内容hello，而引用s就好像地址一样。这样看，Java语言不也是能够操作内存地址了吗？没错，Java根据引用（或地址）可以访问到对应的对象数据，但是它无法在地址上做运算，而这点C语言可以轻松做到。        C语言中的指针，是一种变量，它表示存储数据的内存地址，比如：定义一个字符类型的指针 char *d;其中d的值是地址，而char类型表示从地址一次能够读写的字节数。C语言中，地址可以通过数值计算进行更改，因此指针可以到处乱指，而直接操作内存地址的特性过于过于灵活，稍不注意就会导致问题。        接下来我们把内存条插到主板上，和CPU连起来，如下图所示：        如上图所示，CPU通过地址、控制和数据总线同内存插槽相连，内存条插入内存插槽中，CPU通过地址总线向内存输入地址信息，通过数据总线设置或读取（内存地址对应的）数据信息，而CPU对内存操作的读写模式则由控制总线来传递。看到这里，是不是对主板和内存感觉亲切了许多呢？ 磁盘如何存储数据        如果只有CPU和内存，计算机一旦掉电，重启后，所有的数据将不复存在，因为它们是没有“记忆”的。外存的出现使得计算机能够将数据持久化的保存起来，当需要的时候，会被载入到内存，而只有读入到内存中的程序（或数据）才可以被运行（和使用）。外存的访问速度相比内存会慢很多，而程序需要从外存载入到内存才能运行，所以在CPU和内存配置差别不大的情况下，使用SSD（Solid State Drive）作为外存的计算机，在启动（和运行）程序方面，要明显快于使用磁盘（HDD，Hard Disk Drive）的计算机。        不管是SSD还是HDD，它们大都会使用一致的接口同CPU和内存相连，比如：SATA或PCI接口，以HDD为例，它的结构如下图所示：        如上图所示，HDD下方有数据和电源接口，HDD最显著的还是它那银色的盘片以及盘片上的悬臂，以及看不见的马达。只要稍微接触过计算机的同学，纵使没有看过HDD的结构图，也能猜出数据是存储在银色盘片上的，那么它是怎样保存数据的呢？其实就是通过在磁性盘片上，使用正向或反向的电压来使得盘片上的磁性粒子处于不同的排列形态，而在不同电压下形成的排列形态就可以代表0或1了。悬臂顶端的磁头可以施加电压来更改盘片上相应位置磁性粒子的排列形态，同时也可以感知它们的排列形态，这就使得悬臂通过磁头能够写入和读取到盘片上的数据。        盘片是圆的，数据划分起来岂不是挺困难？如果盘片是正方形的，那横平竖直岂不来的简单？确实如此，当数据在被读取时，需要让悬臂处于数据对应盘片区域的上方，可以看到读取数据时需要给定数据以及数据对应的区域，该对应关系一般会被存储在HDD的缓存中。那读取数据时，盘片和悬臂都会动吗？是的，盘片通过马达驱动进行旋转，而悬臂会在多个同心圆上移动，因此，盘片还是圆形的好，便于旋转，而圆形的盘片会依据不同的半径被分为若干同心圆空间，称为磁道。操作系统会对磁道进行概念上的分割，形成多个扇区，如下图所示：        如上图所示，虽然磁道由内到外，每个长度不一，但是操作系统通过概念上的划分，把它们都分拆成了固定大小的扇区，比如：一个扇区有512字节，而操作系统访问HDD时，都是以扇区作为单位来进行读写的。与访问内存相比，外存访问的特点就是容量大速度慢，除了持久化保存数据以外，外存还会被用作虚拟内存。所谓虚拟内存，是操作系统中的概念，它是把外存中的一部分作为假想的内存进行使用。操作系统为了实现虚拟内存，就需要把程序在内存中的实际内容和外存上的虚拟内存进行部分置换（Swap），而置换过程对于运行中的程序而言是透明的。        虚拟内存的使用，使得每个程序都拥有适度宽裕的内存空间，这听起来不错，但实现起来还是有些麻烦的，该功能需要操作系统抽象出页（Page）的概念。程序全部载入内存后，如《程序！硬件的执念》中所介绍，程序的指令和数据会顺序的排列在内存中，如果我们将载入的程序看作是一本书，那书中的每一页都是程序的一部分，因为程序的执行是顺序的，所以运行起来就如同翻书一般，只不过遇到JMP类型的指令时，会在其中几页上翻来翻去而已。有了页的概念后，就可以将页放置在内存或虚拟内存中，并且以页为单位在二者之间进行置换，从外存将页载入内存称为Page In，反之，称为Page Out，该过程如下图所示：        如上图所示，载入内存的页完成修改暂时不用后，会通过Page Out更新到虚拟内存中，当后续需要使用时会通过Page In重新载入到内存里。虚拟内存中一般有程序全量的页，因此借助虚拟内存能够解决物理内存不足的问题，但Page In/Out会伴随着与低速外存的交互，应用运行就会变得卡顿起来。在计算机机箱上一般会有一个硬盘灯，当磁盘数据读写时会闪烁，而我们在使用程序过程中，如果发现程序变得卡顿时，往往也会发现硬盘灯也在狂闪，其实此时虚拟内存正在不停的置换，如果使用SSD，置换的效率就会变得高效，程序的响应一般就会变得灵敏许多。 外设如何工作        没有输入和输出设备的接入，计算机没有任何意义，因为它求解的问题来自于输入，对问题的解需要进行输出。输入/输出设备，也就是外设，包括了键鼠、显示器以及摄像头等，开发人员在键盘上输入字符，游戏玩家在FPS游戏中用鼠标移动准心瞄准敌人，其实都是在输入，对于计算机而言，它们都是一样的。        输入/输出设备的种类繁多，它们与方方正正的CPU、内存和外存形成了鲜明的对比，但是它们还是通过数据交换的方式同计算机相连。在输入/输出设备上，一般都有用来交换计算机与这些外设数据的IC，这些IC被称为I/O控制器。显示器和键盘等外围设备都有各自专用的I/O控制器，有些常用的I/O控制器会被集成到主板上，这些不同的I/O控制器通过I/O地址来进行区分。I/O控制器包含了端口，端口好比内存空间，是设备面向使用的抽象。        在汇编语言中，提供了IN和OUT两个助记符，用来同输入/输出设备进行交互。以IN为例，使用的方式是：IN 寄存器名, 端口号，它代表将端口号所对应I/O设备中的数据读取到指定寄存器中。OUT与IN相反，使用方式是：OUT 端口号, 寄存器名，它表示将寄存器中的数据输出到对应端口。开发者使用C语言，调用printf(“hello”);，期望在终端输出字符串hello，这个过程就涉及到操作输出设备，当然源代码没有与具体的终端设备打交道，而是通过调用操作系统提供的API函数加以实现，该过程如下图所示：        如上图所示，应用通过调用操作系统提供的API与外设进行交互，而API的实现会使用IN或者OUT指令同具体的设备进行通信。那输入/输出设备的访问速度怎么样呢？开发者在IDE里键入一个字符，显示器上会立刻出现对应的字符，感觉很快呀！分析一下这个过程，它涉及到输入和输出，一来一回能够在人们无法感知到延迟的情况下完成，这个速度不快吗？其实这个速度很慢，只是一来一回再快，也有若干毫秒，而在这段时间里，CPU可能已经完成了上亿次计算。        CPU与外存不会直接交互，原因就是怕被拖慢，跟外设的关系也一样，面对比外存还慢的外设，且伴随有人类更加缓慢的操作，这就需要一种新的机制来进行交互，该机制称为中断请求（IRQ，Interrupted Request）机制，这是一种用来暂停当前程序，并跳转到其他程序的必要机制。暂停当前程序，跳转到其他程序，这个同输入/输出设备与CPU的通信有什么关系呢？答案是关系很大，因为程序在运行时CPU会不断的执行指令，压根不会去理会输入/输出设备中的数据，就好比为了让闪电侠帮忙，只好不断的打他手机一样。        实施中断请求的是输入/输出设备中的I/O控制器，而负责实施中断处理程序的是CPU，该过程如下图所示：        如上图所示，输入/输出设备会不断的发起中断，而不同设备发出的中断请求又不一样，但是请求需要包括：设备信息和I/O端口，起码能够让CPU知道是哪个设备发起了中断。CPU收到中断请求后，会在适当的时间响应中断，发现键盘设备缓冲区有输入的数据，那么将数据拷贝到内存中，接着程序继续运作。显示器作为输出设备也会并行的发起中断，当处理对应中断时，发现内存中有了新数据，就会将新数据输出到显示器进行显示，这样键盘输入的字符就能显示在显示器上了，可以看出，输入和输出是异步进行配合的，只不过中断与中断处理很快，人们无法感知而已。        中断是针对程序的，运行中的程序需要暂停，让渡出CPU，而后又切换回来，这个代价其实挺高的。中断需要将当前CPU寄存器（以及栈）内容进行备份，然后将控制权移交给处理输入/输出设备的程序，这个程序一般是操作系统提供用来实现中断处理的，该程序执行完成后，会还原寄存器内容，使得原有程序得以正常运行。 离近一点看CPU        冯·诺伊曼计算机中的CPU，通过地址、控制和数据总线与内存交互，与外存和外设（或输入/输出设备）通过控制和数据总线相连，可以看出，CPU对其他设备就是进行命令控制和存取数据的，这也就是为什么它被称为中央的原因吧。我们稍微离近一点看CPU，它可以分为四个部分，如下图所示：        如上图所示，CPU由寄存器、控制器、运算器和时钟组成，二进制程序位于内存中，CPU会通过多个部件的协作，完成由内存中取出程序指令与数据，分析指令与计算，最终写回内存的流程。CPU部件之间的（主要）关系与作用如下表所示。 名称 作用 关系 控制器 分析指令并发出相应的控制信号 控制器访问内存，获取指令；控制器驱动运算器进行计算 运算器 负责对数据进行各类运算，主要是数学和逻辑计算 依据寄存器中的数值进行计算 寄存器 用于存放中间结果或其他信息的高速存储器 内存中的数据或指令读入到寄存器 时钟 产生电子脉冲信号，触发其他部件运转 周期性驱动控制器        可以看出来，CPU从冯·诺伊曼时代到现在有了长足的发展，但是其运行的基本逻辑没有变，只是做了更细的拆分。控制器是直面指令逻辑和发号施令的单元，而运算器和寄存器的分拆，这不就是计算与存储分离么？时钟会放大来自主板的时钟频率，宝岛称之为时脉，感觉更贴切一些，在一个时钟周期中，不同体质（或性能）的CPU会执行数量不等的指令，这也就是为什么现代CPU很多主频相近，但性能相差很大的原因。寄存器（Register）听起来就是做数据存储的，这不就是内存吗？其实它和内存差不多，不过具有专属的功能特性，并且是唯一被程序作为对象描述的CPU部件。        例如：汇编代码 ADD eax, 5，其中eax就是累加寄存器，它用来保存加法前的两个参数，以及加法运算后的结果。寄存器的种类有很多，不同类型的CPU也不一样，Intel的CPU提供的基本寄存器在《程序！硬件的执念》中有所介绍，接下来介绍几个常见的寄存器。        运算器在进行加法运算时，需要存储加法的两个参数，然后保存运算后的值，这里会涉及到加法运算状态的保存，此时就需要累加寄存器的帮助。X86下，32位的累加寄存器名称为eax，而64位为rax。累加寄存器的操作示意图如下所示：        如上图所示，指令 ADD eax, 5，表示将*eax旧有的值加5，结果仍会保存在eax寄存器中。该指令执行时会将两个参数放置到累加寄存器中，然后使用运算器进行加法运算，运算器计算后的结果保存在累加寄存器中。        累加寄存器保存了加法运算后的结果，而与之相似的标志寄存器则存储了逻辑运算后的CPU状态，标志寄存器能够提供给控制器（上次）指令执行后的状态，便于其进行流程控制，比如：程序中的if条件判断，都离不开标志寄存器，它的操作示意图如下所示：        如上图所示，当运行指令CMP eax, 100后，运算器会比对累加寄存器eax中的值与100的大小，这和C语言中的 if (eax == 100)一样，在C语言中 eax == 100会产生一个值，这个值可以表示是否相等，而它一般就存储在标志寄存器中。标志寄存器保存了逻辑判断的结果，使用一个（32位长的）二进制数值来表示比对结果。如图中所示，两个值比较大小和等于的结果都会分别按位存储，这样可以方便后续判断，比如：后续指令JLE short@4，表示如果比对的值是小于或等于，即第2和3位为真时，程序会跳转到short@4所标注的指令行。        程序跳转到某一行指令，这在C语言编写的程序中是很常见的，但对于CPU而言，它怎么知道该执行哪一条指令呢？这就需要有一个寄存器始终保存CPU接下来需要运行指令的地址，该寄存器被称为程序计数器。之前看到的指令JLE short@4，在进行指令跳转时，就是将需要跳转到的指令地址设置到程序计数器中，这样CPU在获取指令时就能拿到需要跳转到的指令了。从内存中拿到指令后，还需要将指令保存在指令寄存器中，方便控制器能够分析与执行指令，毕竟从内存中拷贝过来的指令需要有一个地方保存呀！        程序的指令跳转可以使用程序计数器以及指令寄存器来完成，而函数调用就离不开栈寄存器了，栈寄存器一般被记作esp，如果是64位，则为rsp。该寄存器始终保存指向栈顶的内存地址，方便进行栈的操作，关于栈寄存器在《程序！硬件的执念》中详细介绍过，这里不再赘述。        寄存器中多多少少都会保存内存地址，而对于内存访问时，往往需要指定一个地址获取数据，或者给出指定地址后，做一下运算，例如：在给定地址基础上移动4个字节，然后再取出对应地址的内存数据，这就需要基址寄存器和变址寄存器提供的能力了，它们的操作示意图如下所示：        如上图所示，使用基址寄存器，好比指定数组引用的地址，而变址寄存器就相当于下标，当二者给定时，就能运算出实际内存地址，CPU就可以依据实际内存地址去内存中取值了。        上述介绍的寄存器只是寄存器中很少的一部分，但是它们分别围绕着运算、执行流程以及数据访问三个基础特性，让我们把这些寄存器分门别类的填回到CPU中，再离近一点观察CPU，这样是不是就有些感觉了？        CPU、内存、外存和输入/输出设备，它们之间协同配合，有快入闪电的运算器，也有慢如蜗牛般的外设。这些纷繁复杂的设备通过控制、地址和数据总线连接起来，透过合理的调度，运转的天衣无缝，让人们觉得它们本来就是浑然一体的，甚是精妙！ By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better.html":{"url":"book/do-unit-test-better.html","title":"热风-如何更好的做单元测试","keywords":"","body":"如何更好的做单元测试        单元测试是一个沉重的话题，它的存在绝对不是为了制造工作量和麻烦，而是为了提升软件质量和开发效率。提升质量这点没有异议，但是怎么和效率相关了呢？如果你经常做单元测试，一定会认可这个观点，但对于不赞同这个观点的同学，想必也是不常做的同学吧，那就需要你阅读本文了。        本文包括以下内容： （1）软件开发与单元测试：讲述了单测的重要性，以及我们为什么要做单测； （2）Java程序员如何做单元测试：基于JUnit和Mockito的单元测试编写方式，它们是Javaer最常用的工具； （3）基于Spring的单元测试：在Spring环境下，如何编写单测，持久层的测试该怎么做； （4）维护好单元测试：单测的价值、覆盖率的重要性，它们值得投资； （5）体验测试驱动开发：介绍测试驱动开发，看看它的可取之处。        本文贯穿了作者十来年的开发经历，讲述了Java领域内，单测的执行方式，以及做单测的价值，希望能够给你带来一点帮助。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better-01.html":{"url":"book/do-unit-test-better-01.html","title":"热风-软件开发与单元测试","keywords":"","body":"软件开发与单元测试        计算机软件存在的目的是为了驱动硬件，在硬件为主的时代，人们看到计算机就会认为它是标准化、不近人情且小众的机器设备。随着计算机软件的大量出现，人们看到计算机就会觉得很亲切，认为它是我们无法分开的伙伴。有了软件的驱动，计算机“人性”的一面得以显露，但软件的规模要比我们想象的巨大，以Windows10为例，大约由5000万行代码组成，如果你想要把它的源码都读一遍，可能要穷尽你的一生。        不光是Windows操作系统，我们日常使用的软件或者移动端APP，背后都是由一大堆代码构成，而只要有代码的地方，就一定会存在bug。有编程经验的同学一定会认同，随着系统规模的不断变大，开发人员对系统的控制能力也在逐步削弱，如何能够让代码就像我们理想中的样子工作，如何能够避免问题的出现，其实祖师爷（图灵）早已经预言到了。        在1945年，二战结束的日子里，人们为世界恢复和平而庆祝，但人们都不知道，曾在英国的布莱切利公园，有一群天才们，通过研发出第一台计算机破解了纳粹德国的英格玛密码机。正因为他们杰出的工作，每天早上放在希特勒办公桌前的简报也同样的会出现在盟军首脑面前，二战因此提前两年结束。        那个时候，世界上只有一台计算机，它被称为克里斯托弗（来自于《模仿游戏》中的桥段），就在布莱切利公园，由图灵给它编写代码。这是真正意义上的代码，不是Ada编写的程序，而是二进制代码。设计者图灵使用整型和逻辑运算来编写程序，由克里斯托弗来解释执行，千把行的代码就攻陷了英格玛，算上图灵同一个团队的人，那时世界上的程序员可能不超过5个。        现如今，计算机到处都是，可穿戴设备和智能手机，都可以被算作计算机，数量肯定比人多了。虽然会编程的人占比不多，但据统计，也有2千万专业程序员了，他们大都像你我一样，利用IDE编写着代码，使用顺序、条件和循环堆砌着业务逻辑。有了这么多程序员，软件生态一定能够做的很好吧，但事实刚好与之相反。 软件开发面临的困境        图灵说 “我们需要大量有数学能力的人，因为我们面临很多这样的工作。我们遇到的困难之一是如何采取一种合适的规则，使我们的工作不会背离我们的意图。”，这段话显然是针对编程而言。编程这项工作需要人具备一定的数学能力，最起码是懂得逻辑，而将逻辑编写成代码的过程充满了挑战，因为人不是机器，不会记得全部路径和细节，总会犯错，从而导致编写出来的软件不能按照期望进行工作，这种与期望的背离，就是bug。        仔细回想一下图灵说过的话，他提到一种合适的规则，让我们所做的工作能够符合预期，可见，只写过上千行代码的图灵，已经预测到了今天开发者们所面临的困境。针对软件开发面临的困境，自软件诞生以来，随着规模不断的变大，越来越多的方法论和工具不断的涌现，都希望能找到图灵提到的规则。        但现实是残酷的，小到桌面应用程序出现bug无法使用，大到NASA的航天系统出现bug导致航天飞机升空爆炸，人们在编写程序的同时，也在编写bug，依靠测试人员也无法穷举所有的缺陷。 寻找圣杯        寻找图灵提到的规则，就是在寻找软件开发的圣杯。让我们先忘记圣杯，看看人们是如何应对软件规模的不断增长，这就要从很久以前开始说起了，当然如果你看过任何一本软件工程方面的书，你就一定很熟悉这段历史。        最早计算机硬件刚产生，软件只是附庸，那个时候是无“软件”概念的时代（1946～1955），软件程序主要围绕着硬件进行开发，规模很小，可以认为都是一些小工具。随着操作系统的出现，通用程序不断的下沉，越来越多的人进入到编程的世界，这个阶段是意大利面阶段（1956～1970），程序员是这个时代的宝贝儿，他们随意的编码，整个软件代码看起来就像意大利面一样杂乱无章，能用就行，还要什么自行车？而在这种环境下堆砌代码很容易“塌方”，最终后果就是软件一旦到了一定规模，就会不可避免的出现问题，由此产生了“软件危机”。        用代码去“码楼”会塌，但现实世界中，用砖去码楼，楼塌的却不多，这是为什么呢？因为建筑业遵循建筑规范，工程化的管理使得质量有保证，效率会更高。这时候有些聪明人就将建筑工程化思想引入到软件开发过程，软件工程这一领域变应运而生。软件工程强调用工程化的思想解决软件开发问题，瀑布模型、迭代模型、敏捷开发和RUP等软件开发方法论不断的推出，其目的就是为了提升效率，保证质量。除了软件工程，在软件开发的各个方面，都在进行着效率和质量提升的革命，如下表所示： 领域 所含内容 编程范式 面向对象开发、框架和DSL等 开发流程 迭代、敏捷和持续集成等 研发工具 IDE、代码审查和FindBugs等 经验习惯 设计模式和测试先行等        如上表所示，在软件开发的不同领域，人们使用各种技术来提升该领域的效率和质量，所含内容不断的推陈出新。要寻得圣杯，不是依靠一个简单的规则，更不是依赖某个工具，它需要我们不断的创新。寻得圣杯的人，必是开发者，而单元测试就是一种良好的经验习惯，掌握它，使用它，能够更好的指引你去寻得圣杯。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better-02.html":{"url":"book/do-unit-test-better-02.html","title":"热风-Java程序员使用JUnit做单元测试","keywords":"","body":"Java程序员使用JUnit做单元测试        在许多稍大一点的公司，会有自己维护一套单元测试框架的冲动，比如：在阿里巴巴，很早之前就存在一套基于TestNG的单测框架，叫JTester，随着时间的流逝，这个框架就GG了。自己维护框架，有这个冲动时需要冷静一下，因为公司自己搞一个单元测试框架，维护将成为大问题，而使用业界成熟的解决方案，将会是一个更好的选择。业绩成熟的解决方案，意味着有一组非常专业的人替你维护，而且不断地有新Feature可以使用，同样你熟悉这些之后，你就可以不断的复用这些知识，而不会局限在某个特定的框架下。        Java单元测试框架有不少，但以JUnit为事实上的标准，而JUnit只是解决了单元测试的基本问题，对于Mock和容器是不提供支持的。在Mock方面，Java也有很多开源的选择，诸如：JMock、EasyMock和Mockito，而Mockito也同样为其中的翘楚，二者结合起来，能够很好的完成单元测试的工作。 在Github上对开源Java项目的依赖分析中，JUnit和Mockito高居前十，其中JUnit拔得头筹。Kent Beck和Erich Gamma共同打造的JUnit，前者是敏捷宣言的发起人，后者是设计模式GoF之一，可谓是出自豪门。 使用JUnit单元测试        接下来让我们使用JUnit来进行单元测试，首先需要说明一下，从本章开始的代码，都可以在Mockito Sample项目中找到，这个项目实际开始于2014年，自项目创建到今天，单元测试在Java开发环境中出现了多次变化，比如：基于spring的测试和基于springboot的测试就有所不同，笔者随着这些变化也进行了多次增补维护。        Java应用大都使用maven进行依赖管理和项目构建的，在项目中通过声明依赖，就可以将junit引入项目，新增依赖如下： junit junit test        注意依赖坐标的scope是test，我们习惯将test作用域的依赖，放置到dependencies标签的最下面，就像这样： org.apache.commons commons-lang3 junit junit test        好处就是依赖管理很明确，感官上也会舒服很多，如果项目的依赖管理不讲究，散乱的放着一堆依赖，维护这种项目的同学，想必自己家里也是很乱的吧。Maven项目中对于单元测试和生产代码（以及配置）的目录要求如下图所示：        我们编写的测试类以及测试类路径上能够找到的文件或配置，分别放置在test/java和test/resources目录中。依赖junit后，我们就可以编写单元测试了。接下来，选择测试commons-lang3中的StringUtils，在test/java中新建测试类TestCaseSample，代码如下所示：        如上述代码所示，该测试类有三个方法，方法上需要增加 @org.junit.Test注解，有注解修饰的方法会被JUnit框架理解为需要执行的测试方法。@Test注解还支持属性的配置，相关配置和描述如下表所示： 属性名 描述 timeout 测试超时时间，单位是毫秒。如果该测试方法的执行时间超出了timeout所指定的时间，那么测试会失败。以timeoutTest()方法为例，如果该测试方法执行时间超过330毫秒，那么测试结果就是失败 expected 指定期望的异常类型。如果测试预期就会失败，比如：测试异常分支，那么可以通过使用expected属性来指定测试将会抛出的异常类型，如果测试执行时预期的抛出了指定类型的异常，那么测试会通过，反之测试失败。以exception()方法为例，expected指定了NullPointerException，如果测试方法执行时抛出了空指针异常，测试就算通过了 断言选择        测试执行时会输出结果，当然我们可以使用System.out.println来完成目测，但是有时候需要让JUnit框架或者maven的surefire插件能够捕获住测试的失败，这个时候就需要使用断言了。        如TestCaseSample类所示，我们使用org.junit.Assert来实现断言的判断，可以看到通过简单的assertEquals就可以了，当然该类提供了一系列的assertXxx来完成断言，比如：assertTrue。 使用IDEA在进行断言判断时非常简单，比Eclipse要好很多，比如：针对一个int x判断它等于0，就可以直接写x == 0，然后代码提示生成断言。 使用Mockito更好的单元测试        单元测试的标的是类，因此它更加关注的一个类的行为是否符合预期，但是现实中没有类是孤立存在的，自定义类或多或少的依赖其他的类。这时就需要使用Mock工具，它能够将被测试类的依赖都Mock掉，潜意识的认为依赖类都是正常工作的，只需要测试当前类即可。当然这么说也有些绝对，在持久层的单测中，还是会将持久层实现所依赖的数据层组件（比如：mybatis的sqlSessionTemplate）初始化，真实的和数据库进行交互，因为单测的目的是测试行为是否符合预期，而持久层的关键是数据操作是否正确，所以就需要与数据库进行通信来测试正确性，事情需要分情况来看。        对于业务层的单元测试，将持久层的接口进行Mock就没什么异议了。Mock工具一般会选择Mockito，通过添加以下依赖可以引入项目： org.mockito mockito-core test        Mockito采用了fluent形式的API，我们可以选择Mock一个类或者接口，生成一个mock对象，然后向mock对象中添加mock逻辑，以期望在后续调用这个mock对象时能够执行逻辑返回预期的值。接下来我们看一段代码，简单介绍一下Mocktio该如何使用，代码如下所示：        可以看到，通过Mockito提供的静态方法mock，能够创建出一个mock对象，该mock对象实现了接口List，如果调用该mock对象，它会返回一些默认内容，比如：如果调用返回类型是引用类型的方法，mock对象会返回空，如果返回类型是原型，则返回0。Mock对象创建完成后，就需要植入mock逻辑，如上述代码所示，通过调用when和thenReturn两个方法，能够完成指定方法的逻辑植入。Mockito.when(list.get(0)).thenReturn(“one”);表示如果调用mock对象的get方法，且输入参数为0，就返回字符串”one”。        使用Mock，我们就能够将类的依赖都替换掉，让它们返回我们预期的内容，这样就可以进行真正意义上的单元测试了。虽然Mockito能够生成Mock对象，并且可以让Mock对象接收请求时，返回预期的值，但有时我们的逻辑会比较复杂，比如：要求Mock对象能够根据参数的值，返回出不一样的结果，这样我们的单元测试可以做的更全面，也会更加真实。        Mockito提供了thenAnswer方法来解决这个稍显复杂的问题，代码如下所示：        如上述代码所示，当list（Mock）对象，接收任意整型的请求时，会使用lambda表达式中的内容来处理，这个lambda是一个Function，接收的参数是Invocation。Mock逻辑不复杂，从Invocation中获取参数，当输入为0时，返回字符串”0”，当输入等于2时，会抛出异常。而该测试方法会用到expected属性，最后调用list.get(2)时，会抛出异常，但是符合预期，测试通过。 使用JMockData生成Mock数据        Mockito能够让Mock对象返回我们期望的对象（或数据），但是它不会帮我们构建数据，如果你去操作这个对象，会发现它所有的字段都是默认值或者为空。这就需要我们自己构造对象，而构造方式是通过一堆set方法进行赋值。如果期望的对象只有几个字段还好，要是遇到一个几十上百个字段的数据结构，那就要了亲命了。        这时候，就需要JMockData来帮助我们完成Mock数据的构建了，首先添加依赖： com.github.jsonzou jmockdata 4.3.0 test 注意scope是test，不要忘了。        接下来演示一下Mock数据如何构造，代码如下所示：        可以看到，命令行输出内容中包含了Student和Hobby属性，一个Student可以有多个Hobby，只需要通过JMockData.mock(Student.class)方法，就可以创建出一个Student对象，该对象会被JMockData随机填充一些属性值，这样就方便使用者进行测试了。 如果属性值不符合预期，可以再通过调用set方法做一些微调。 一个真实案例        接下来，我们以会员注册为例，看看该如何做单元测试。对于会员注册而言，会有一些业务逻辑（或限制），不同公司的业务逻辑会不大一样，但是代码逻辑大都长成这样。        可以看到注册一个名称为name，密码为password的会员，主要逻辑是判断会员名长度、密码以及同名会员不能注册，如果条件都符合，则可以进行注册。这些逻辑不复杂，但是由于业务类依赖了userDAO，而业务层单元测试不会在测试会员注册服务类时连接数据库，这时就需要mock掉依赖的userDAO。        在Mockito的支持下，我们可以方便的完成mock，因为测试类依赖userDAO，所以需要调用一下setUserDAO完成引用的设置，该类的单元测试类如下：        可以看到，在测试开始的时候，利用了 @Before注解修饰的mockUserDAO方法，来完成Mock对象的构建。在当前测试类中的任意测试方法执行前，都会执行mockUserDAO方法，该方法能够保证Mock对象的初始化工作。        MemberWithoutSpringTest虽然能够测试会员注册类，但是它看起来有些不顺眼，因为MemberServiceImpl是直接构造出来的。另外一个坏味道就是UserDAO也是硬塞给MemberService的实现，在生产代码中，Spring会帮助我们完成依赖关系的装配工作，如果我们需要单测也有Spring的那种感觉，就需要使用Spring-Test来实现了。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better-03.html":{"url":"book/do-unit-test-better-03.html","title":"热风-基于Spring的单元测试","keywords":"","body":"基于Spring的单元测试        企业级Java程序基本都离不开Spring和SpringBoot，它们是目前企业级Java事实上的标准。我们的代码中或多或少都会使用到 @Autowired注解来引入依赖，或者在类型上声明 @Compoenent注解来将类型实例托管到（Spring）容器中，这就带来了一个问题，如果没有Spring，谁来帮我们组装类之间的依赖关系？        在前文中可以看到MemberServiceImpl依赖了UserDAO，通过调用setUserDAO方法，可以将MemberServiceImpl依赖的实例设置给它，可是如果需要测试的类型有很多依赖怎么办呢？还需要一一调用set方法设置吗？        答案是否定的。Spring从诞生的开始，就考虑到如何在Spring环境下做单元测试，而Spring-Test就是Spring提供的单测套件。Spring-Test是springframework中一个模块，也是由spring作者Juergen Hoeller亲自操刀设计开发的，它可以方便的测试基于spring的代码。 使用Spring-Test来进行单元测试        Spring-Test是基于JUnit的单测套件，由于测试会启动spring容器，所以需要依赖Spring配置，同时要继承Spring-Test提供的超类。在使用Spring-Test前，首先要进行依赖配置，依赖的maven坐标如下： org.springframework spring-test test org.mockito mockito-core test junit junit test        MemberServiceImpl依赖UserDAO，而Mockito.mock(Class clazz)方法可以创建一个clazz类型的Mock对象，在spring体系中，Mockito就如同一个FactoryBean，因此我们可以通过一段spring配置，将MemberServiceImpl装配起来，对应的spring配置如下所示：        如上述配置所示，熟悉spring的同学一定会觉得很亲切，MemberServiceImpl被声明为id为memberService的bean，而userDAO对应的bean是由Mockito创建出来的Mock对象。接下来，只需要使用Spring-Test提供的超类，就可以编写基于Spring-Test的测试了。        MemberService的单测（部分代码）如下所示： @ContextConfiguration(locations = {\"classpath:MemberService.xml\"}) public class MemberSpringTest extends AbstractJUnit4SpringContextTests { @Autowired private MemberService memberService; @Autowired private UserDAO userDAO; /** * 可以选择在测试开始的时候来进行mock的逻辑编写 */ @Before public void mockUserDAO() { Mockito.when(userDAO.insertMember(Mockito.any())).thenReturn( System.currentTimeMillis()); } @Test(expected = IllegalArgumentException.class) public void insert_member_error() { memberService.insertMember(null, \"123\"); memberService.insertMember(null, null); } /** * 也可以选择在方法中进行mock */ @Test(expected = IllegalArgumentException.class) public void insert_exist_member() { MemberDO member = new MemberDO(); member.setName(\"weipeng\"); member.setPassword(\"123456abcd\"); Mockito.when(userDAO.findMember(\"weipeng\")).thenReturn(member); memberService.insertMember(\"weipeng\", \"1234abc\"); } }        从上述测试代码可以看出，Spring-Test会要求测试类型继承AbstractJUnit4SpringContextTests，同时使用注解@ContextConfiguration指定当前测试需要使用的Spring配置。我们编写的单元测试肯定不止一个，当编写另一个单元测试类时，就有同学会想着用已有的配置文件，这么做，好吗？可以看到示例中，配置专门放在MemberService.xml中，没有使用共用的配置文件，目的就是让大家在测试的时候能够相互独立，避免搞坏别人的测试，让不同的测试相互之间不影响。并且在一个配置文件中配置的Bean越多，就证明你要测试的类依赖越复杂，承担的责任过多，从而提醒自己做重构。 现代化的Spring-Test使用方式        Spring从4.0后，就开始推荐使用Java配置方式了，也就是大家常用的@Configuration和@Bean，通过编写Java配置类来装配Spring的Bean。现阶段大家使用的Spring都比较新，因此我们可以将以传统形式配置的Spring-Test单测改造为现代化的配置方式。        使用Java配置方式的MemberService单测代码如下所示：        可以看到基于Java配置方式的单测比传统基于xml的Spring-Test要显得内聚很多，没有了测试配置文件，只有一个单元测试类，Spring的配置和测试类也可以是一体的。通过@Configuration修饰的MemberServiceConfig可以被配置在@ContextConfiguration中，被用来装配一个测试的Spring容器。        测试类也不需要继承AbstractJUnit4SpringContextTests，只需要使用@RunWith注解修饰即可。不需要xml配置，配置和测试是一体的，现代化的Spring-Test变得更好了，和配置文件一样，当配置类中的@Bean变多时，就要反思实现类是否职责过多或者依赖过重了。 SpringBoot环境下的测试方法        Spring框架诞生后，成为了企业级Java（与互联网Powered by Java）的事实标准，IoC无往不利，势如破竹，从代码组成和思维方式上改变了整个Java开发生态。在2.5.6发布后，Spring框架就进入了一个低潮期，Spring不愿意触及部署与实现，这种不愿下场的做法使得其多年没有任何实质变化，被淘汰只是时间问题，但是这时候Phil Webb等人发起的SpringBoot项目挽狂澜于既倒，点燃了Spring的第二春。        SpringBoot提供了开箱即用的starter体系，通过自动装配能够提供给应用更便捷的bean配置方式，同时它支持迅速打包为jar-in-jar的形式，并通过java -jar的形式进行运行，改变了企业级Java使用容器部署的主流方式。在改变部署形态的同时，提供了多环境和配置的解决方案，在微服务和容器化崛起的时候，顺势而为，一举成为企业级Java部署的现实标准。        可以看到相比Spring框架，SpringBoot会载入更多的Bean，同时由于测试类可能依赖了starter，就需要在单测执行前完成starter的配置，因此SpringBoot也提供了相应的测试套件，即Spring-Boot-Test。 Spring-Boot-Test 对Mockito进行了整合，在进行Mock时，相比Spring-Test会方便一些。        接下来，让我们用Spring-Boot-Test改造一下MemberService的单测，改造后的测试代码如下： @SpringBootTest(classes = SpringBootMemberTest.Config.class) @TestPropertySource(locations = \"classpath:test-application.properties\") @RunWith(SpringRunner.class) public class SpringBootMemberTest { @Autowired private Environment env; @MockBean private UserDAO userDAO; @Autowired private MemberService memberService; @Before public void init() { Mockito.when(userDAO.insertMember(Mockito.any())).thenReturn(System.currentTimeMillis()); } @Test public void insert_member() { System.out.println(memberService.insertMember(\"windowsxp\", \"abc123\")); Assert.assertNotNull(memberService.insertMember(\"windowsxp\", \"abc123\")); } @Configuration static class Config { @Bean public MemberService memberService() { return new MemberServiceImpl(); } } }        如上述代码所示，基于Spring-Boot-Test的单测需要使用注解@SpringBootTest标注，声明该类为一个SpringBoot测试类，同时与Spring-Test类似，通过classes属性声明当前测试类使用的配置，本示例中是SpringBootMemberTest.Config。        对于需要Mock的类型，可以使用@MockBean注解来修饰，它会生成对应类型的Mock对象，并将其注入到容器中。当然SpringBoot离不开application配置，可以通过@TestPropertySource注解指定当前测试用例所使用的application配置。        如果测试的类需要依赖一些starter才能工作，那就需要在测试类上增加@EnableAutoConfiguration，同时在application配置中增加一些属性，这样该测试就会像一个SpringApplication一样被启动起来。 SpringBoot环境下持久层测试        持久层的单测很重要，是应用单测的基础，而且由于持久层的单测一般不会选择mock数据源，因此测试过程除了正确性的保证之外，还需要确保测试过程对数据库中的数据不会产生影响。        还是通过UserDAO的单测来演示SpringBoot的持久层单测编写，由于测试需要依赖数据库，因此在示例中需要先使用StartDB启动一个hsqldb，然后再运行单测UserDAOImplTest。hsqldb是一款Java编写的嵌入式数据库，可以使用内存或主机模式启动，本示例中采用后者，以独立进程的方式启动，在数据库启动后，接下来看一下对应的测试，代码如下所示： @RunWith(SpringRunner.class) @SpringBootTest(classes = UserDAOImplTest.Config.class) @TestPropertySource(locations = \"classpath:test-application.properties\") @EnableAutoConfiguration public class UserDAOImplTest extends AbstractTransactionalJUnit4SpringContextTests { @Autowired private UserDAO userDAO; @Test public void findMember() { MemberDO member = new MemberDO(); member.setId(1L); member.setName(\"name\"); member.setPassword(\"password\"); member.setGmtCreate(new Date()); member.setGmtModified(new Date()); userDAO.insertMember(member); MemberDO name = userDAO.findMember(\"name\"); Assert.assertNotNull(name); Assert.assertEquals(\"password\", name.getPassword()); } @Import(MyBatisConfig.class) @Configuration static class Config { @Bean UserDAO userDAO() { return new UserDAOImpl(); } } }        可以看到，测试类需要继承AbstractTransactionalJUnit4SpringContextTests，这样任意测试方法都会进行回滚，避免对数据造成实际的影响。我们运行一个持久层的测试，不希望它更改已有的测试数据，也希望测试方法之间相互不存在影响，而该超类能够让所有测试方法的执行最终都会进行回滚，从而避免对数据库中的数据产生副作用。        测试方法需要进行数据准备，然后进行查询验证。除了验证数据是否为空，最好能够抽检几个字段，看看是否符合预期。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better-04.html":{"url":"book/do-unit-test-better-04.html","title":"热风-维护好单元测试","keywords":"","body":"维护好单元测试        单元测试只是在开发阶段用来测试生产代码是否正确运行的工具吗？它们的使命随着生产代码的发布就结束了吗？它们的地位注定比不上生产代码吗？不！绝对不是。我们编写生产代码时会使用高质量的三方组件，应用会部署在tomcat/jboss等容器中运行，我们对类似spring或tomcat这样的开源产品赞不绝口，从来不会怀疑它们的质量。这些框架（或服务器）的质量为什么这么好呢？答案就是规范、设计和测试，而测试就包含了充足的单元测试。        记得前同事罗毅讲过他们WebLogic Server的质量保证策略：有充足的单测覆盖率，行覆盖率在70%以上，重点模块会更高。每天会进行日常构建，执行单测、回归和性能等多种测试，如果一旦测试被阻断，所有正在进行的项目就会立刻停止，相关和不相关的人都会看到是什么问题阻断了测试执行，直到问题被找到和解决为止。当时我对这种处理方式很不解，这样做不是会经常阻断，导致效率很低呢？回答是：一年基本都不会发生一次，而且效率相当高。        这里不去探讨BEA是如何提高产品质量的，而是想说两点：第一，好质量是做出来的，是测试和验证出来的；第二，好质量是积累出来的，是聚沙成塔般堆起来的。如果从这两点去思考，你还会觉得单元测试没什么价值吗？ 开发资产与复利思维        开发资产除了生产代码，就只剩下配置、机器和单元测试了。应用代码、配置和机器，只是用来实现开发者的目标，让机器能够按照开发者的意愿去执行，而单元测试，只有单元测试，能够验证生产代码，测试其执行逻辑是否符合预期。单元测试不会在生产环境中执行，但是它是开发资产中真正对生产代码产生积极效应的一环，所以需要维护好单测，每次提交前跑通所有的单测，是一个良好的习惯。        有同学会认为，既要写生产代码，又要写单元测试，工作量不就变得更大了吗？有这个观点很正常，而且这个观点一般也是不常做单测同学的主要问题。试想一下：我们在写持久层功能时，有没有因为配置拼写错误，导致自测功能阶段才发现问题？此时应用已经经历了漫长的构建和部署阶段，十几分钟后，才发现一个低级错误。改正了配置拼写，再次启动应用，结果又发现持久层实现的参数没有传递，那只有再来一遍。这时候，如果持久层有单元测试，伴随着测试的运行，低级错误会在应用启动前全部被找到并修复，除了多编写了一个测试类，时间可能并不会慢多少，假设你经常做单测，那么效率可能会变得更高，这样下次再修改持久层代码时，会不会比没有单测显得更加自信一点呢？答案是一定的。        单测发挥作用是缓慢且持久的，跑不过的单测一定在向你告知问题，不要忽略它们。单测数量的提升，一定会让你的代码更自信，不用启动应用也能检验它们是否按照预期运行，效率变得更高。就像巴菲特投资一样，不求暴利，但求稳定的增长，当时间这个因素加入到公式中后，最终价值是巨大的，复利思维如此，单元测试亦是如此。不要因为没人做而不做，不要因为做的少就忽略，眼光放长远些，从结果和技能上看，这都是最划算的投资。        1453年君士坦丁堡陷落，奥斯曼帝国苏丹看到宏伟的圣索菲亚大教堂，赞叹不已，那是奥斯曼帝国无法企及的水平，之后的日子里只能在圣索菲亚大教堂旁立起一根小小的宣礼塔。感受到艺术冲击的奥斯曼人并没有放弃学习和尝试，随着一根比一根宏伟的宣礼塔在圣索菲亚大教堂旁立起来，帝国自认为建筑水平已经到了，1617年，在圣索菲亚大教堂对面落成了更加宏伟的蓝色清真寺。单元测试，开始做的时候感觉很困难，那只是不熟悉而已，等到时间久了，就会变得熟练，只要不放弃，最终都会运用自如。 单元测试覆盖率        如果做了单元测试，如何衡量单测的充分程度呢？这就需要引入单元测试覆盖率这个指标了。单元测试覆盖率会从行覆盖和分支覆盖等多个维度来考察单元测试对生产代码的覆盖情况，覆盖率越高，生产代码的质量也会更好。在项目中，可以通过引入jacoco插件来生成当前项目中的单元测试覆盖率。        可以在项目的根pom下配置jacoco插件，配置如下所示： org.jacoco jacoco-maven-plugin 0.8.7 prepare-agent prepare-agent report prepare-package report post-unit-test test report target/jacoco.exec target/jacoco-ut        上述配置会在执行mvn clean test命令时，在各个项目中的target/jacoco-ut目录下输出单测覆盖率文件，形式一般是html页面，文件打开之后，会看到类似页面：        可以通过点击对应的package，查看行覆盖率缺失的代码，随后可以对缺失的路径进行测试补充。这就像刻意的刷分一样，单元测试覆盖率也是一个我们追求的目标，当单元测试行覆盖率超过70%的时候，整个项目的质量会很不错。持续稳定的单元测试覆盖率，会保障一个应用一直处于较稳定的状态，后续投入维护的资源会降低。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/do-unit-test-better-05.html":{"url":"book/do-unit-test-better-05.html","title":"热风-体验测试驱动开发","keywords":"","body":"体验测试驱动开发        单元测试不仅改变了我们测试生产代码的方式，它也想尝试改变开发者的编程模式，假设我们先编写测试用例，再编写生产代码，这种激进的模式，不知道大家想过（或用过）没有？有同学会说，连生产代码都没有，单测测什么？这不是天方夜谭吗？事实上，这不是痴人说梦，JUnit的作者，就提出了设计驱动开发（TDD），并认为它能极大的提升软件质量和开发效率。 无限地逼近极限        测试驱动开发，从编写测试开始，到最终测试全部通过，功能自然而然的实现完全，这种魔幻般的开发方式如下图所示：        如上图所示，先从测试新增开始，然后再运行单测，这时，测试妥妥的无法通过。遇到不能通过的单测时，开始写实现逻辑，然后不断的完善测试和生产代码，直到该测试通过。为了让测试通过可能会在生产代码中采用一些比较粗暴的写法，此时就需要重构代码，优化结构，随后再次运行测试，通过后，就开始下一个循环。        随着这种看似极限的循环不断运行，生产代码一点点的被构建出来，同时配套的单元测试也被编写出来，代码质量自然不低。TDD的采用难度是不小的，但先写测试的目的是要开发者明白自己要干什么，聚焦到实现层面要做点什么，因为很多开发者在编程时，往往不知道自己在做什么。聚焦到一个单点问题，然后快速循环，才能专注且高效的解决问题，实现功能。 可取之处        TDD过于的极限，尤其是生产代码还没有的情况下就开始写单测，让人难以接受。生产代码写完了，再写单测，就像补作业，太晚了，也不好。那么取其中间，先定义好接口和主要方法，不做实现，然后开始写单测，这样单测就和生产代码一起长出来了。        虽然TDD不是强制要求，但是透过它还是能看到不少可取之处的，它能够让代码上生产环境之前，以使用者的角度审视编写的代码，至少可以做到以下三点： （1）如果代码难测，那就是对问题的分析还没有到位； （2）如果存在大量的Mock，那就是依赖过于复杂或职责不清； （3）能够通过反向刺激让我们看到代码的不足。        其中第三点的反向刺激包括：这个方法命名是否够妥帖？别人用这个函数会误用吗？这个类是不是承担了过多的职责？好的测试从来都不是在制造麻烦，而是让你有机会更早的发现问题，并且以极低的代价修复问题。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-01.html":{"url":"book/computer-network-01.html","title":"热风-感悟《计算机网络：自顶向下》","keywords":"","body":"感悟《计算机网络：自顶向下》（01.概述） 互联网的组成        随着科技的不断发展，互联网已经成为我们日常生活中不可或缺的一部分。我们打开手机，无聊时看主播、饿了就叫外卖和出去玩打滴滴，享受三驾马车经济的同时，就是在使用互联网。既然我们像呼吸空气一样使用互联网，那互联网是什么呢？ 互联网是什么？        互联网是人类有史以来创造的最大规模的复杂系统，而且它还在不断成长，尝试从几个方面理解它。        首先，互联网最重要的组成部分就是网络基础设施，包括：计算机、服务器、路由器和光纤等物理设备。这些设备通过无线或有线的方式相互连接，从而形成了网络，为互联网的运行提供基础支持。        其次，应用程序也是互联网的重要组成部分。这些应用程序包括：浏览器、邮件客户端、即时通讯软件、在线购物平台和短视频直播等等。这些应用程序通过网络基础设施来实现自身功能，使人们的生活更加便利和高效。        最后，数据也是互联网的重要组成部分。随着互联网的发展，人们可以使用互联网来共享和传输各种数据，包括：文本、图片、视频和音频等。这些数据在互联网上的共享和传输不仅可以促进信息的交流，也为人们的生活和工作提供了更多的选择和方便。        可以看到，互联网和计算机网络相互成就，因此可以给出该公式：互联网 = 网络基础设施 + 分布式应用 + 数据。        如上图所示，网络中的主机节点主要包括两部分：应用以及基础设施（操作系统和硬件），基础设施提供了socket编程界面，应用依赖socket使用网络服务。应用可以是客户端程序，比如：微信或者淘宝，也可以是服务端程序，比如：淘宝的交易中心系统。        除了主机节点之外，网络中还包含了大量的路由器，这些路由器可以理解为主机节点的缩小版，它们专注于分组交换和转发，主机节点和路由器构成了互联网的基础设施。 一些概念        计算机网络涉及的概念非常多，这里选择了部分概念做介绍，如下表所示： 名称 描述 举例 节点 主机和交换节点 主机可以是终端设备或服务器，交换节点比如常见的路由器或交换机 边 存在于主机到互联网的连接，或者交换节点之间的连接 一般来说，主机到互联网的连接称为接入网，交换节点之间的连接称为骨干（或核心）网 协议 语法、语义、次序和实体之间的动作 有场景和逻辑交互的意义 计算机网络发展历史        1967年，ARPA网建立，要求是没有中间节点，可用性高的网络，是不是军方要求核打击后能够继续有效工作，还是别的原因，这个不可知，但其结构目标是一切的基础。        1969年，ARPA网从1个节点扩展到了4个节点。        1972年，ARPA网进行公众演示，当时基于NCP协议。        1979年，ARPA网增加到200多个节点。        1983年，Cerf和Kahn提出IP协议，IP协议可以over到不同的数据链路上，ARPA网从NCP切换到IP协议上，开始整合其他网络。        1983年，UNIX捆绑TCP/IP协议栈，BSD版本内置支持，促成互联网大发展。        当互联互通的基础设施搭建完成，配套的系统和软件有了支持，互联网的发展就铺平了道路，而互联网上的分布式应用就迎来了大发展，一切变得日新月异。 分组交换与电路交换        互联网是计算机网络史上最大规模的应用，它是由网络基础设施和分布式应用构成的，或者说互联网就是构建在网络基础设施上的分布式应用。互联网和计算机网络是逐步发展演绎出来的，尤其是网络基础设施，它肯定不是为了这个庞大系统而全新设计的，它一定是基于现有的基础设施进行构建的，因为这样最经济。 电路交换        计算机通信看起来和电话电报通信非常相似，后者完成信息或语音的调制，调制后电信号的传输，收到电信号后的解调以及内容还原，从点到点传输数据上来看，计算机通信也需要完成类似工作，而电路交换就是将计算机网络建设在电路交换网络上。        电路交换需要建立物理连接，或者说物理连接是一直准备好的，家里会有一条电话线通过电线杆连接到电信公司，这根线路是独享资源的。在一根线路上，电信号是以正弦波的形式传播，同时接入网络的设备也不会只有一个，因此可以通过频率（FDM）、时间（TDM）和波长（WDM）等多个维度，将多个信息合并发送（或接收），将通路利用起来。利用波的叠加，同时传输数据，提升利用率。        无论是采用何种区分形式，在调制和解调两端一定会对这些波长、频率或时间（间隔）有一致的理解，过高的频率在传输中会产生损耗，抵达目标方后由于信号的衰减失真而无法被识别。同样，过多的波进行复合，也会导致一样的结果。因此说到底，就好比在一张白纸上用不同颜色的笔写字，少数几种一起写就好了，内容含量高也好识别，如果几十上百种颜色一起招呼上，那就变成鬼画符，没人认识了。        说到调制解调的过程，就需要双方进行协商，以前家里Modem进行拨号上网时，Modem拨号时会传出嗡嗡声，其实就是它和电信公司对端的Modem进行协商时的声音。虽然电路交换可以建立计算机网络连接，完成数据传输和交换，但是它存在一些问题，主要有以下几点：        第一，建立连接耗时。可以看到网络连接建立需要双方协商，以及线路准备，连接建立的时间长，存在比较大的开销。        第二，线路独享，浪费资源。在电报电话通信中，物理线路点到点是直连的，它服务于两点的通信，当两点之间没有数据传输时，也不能服务他人，这种独享的线路，资源利用率不高。        第三，可靠性不高。电路交换依赖两点可靠的工作状态以及线路的稳定，如果线路出现故障，影响面就会很大，因为数据传输并不会绕过去。        除此之外，计算机网络通信的特点与电报电话是完全不同的，前者是离散的，后者是集中的。计算机网络通信需要时不时的进行数据交换，交换目的随机且离散，而电报电话目的性很强，进行连接通话时数据交换集中，不会出现线路占用但上面没有跑数据的情况。        因此，计算机网络通信其实不适合采用电路交换技术。 分组交换        分组交换，核心是存储与转发分组，即Packet网络包。分组即包，包即分组。分组交换下的计算机网络都采用存储和转发的方式处理网络包，即输入，处理和输出。相比电路交换，虽然延迟高了，但共享性却提升了。网络节点可以处理多方发送的网络包，为了能够提升网络节点（比如：路由器）的处理能力，通常会使用队列，任何分组交换网络设备（包括：主机）都会有队列，这个队列一般存在于设备内置的存储中，处理过程如下图所示：        如上图所示，网络设备接收网络数据包，然后将其放置入接受队列中，分组交换设备从接受队列中取出数据包进行处理后，放置到发送队列中，再由分组交换设备从发送队列中取出后，从另一个网络端口进行输出。可以看到，对于网络包的处理，就是生产者和消费者模型，虽然它会出现排队导致的延迟，最差的情况下还会出现由于队列满导致网络包被丢弃，但是分组交换还是非常适合计算机网络通信，主要表现在以下几点：        第一，支持海量主机。不同于电路交换要求两点之间存在物理直连，分组交换设备以及线路是可以复用与多个网络设备的。        第二，需要拥塞控制。对于分组交换网络，如果某些节点向网络大量发送数据包，部分设备或者线路就会负载升高，数据包在队列中排队，产生数据拥塞，最终导致网络不可用。        如果用分布式消息系统来理解分组交换，是非常合适的，虽然分布式消息系统使用计算机网络，而计算机网络基本都是基于分组交换技术的。        分组交换相较于电路交换，成本低了，支持的设备量级大了，但延迟也增加了，其延迟主要表现在以下几个方面： 名称 描述 排队延迟 在设备中的队列进行排队的耗时 处理延迟 分组设备在接受和处理网络包时需要时间 传输延迟 分组设备将网络发送到线路上的耗时 传播延迟 网络包在线路或媒介中传输的耗时        使用分组交换网络需要关注延迟。 发展过程        分组交换比电路交换更适合计算机网络，既然理论都这么认为，那么就干脆独立新建一套分组交换网络来适应计算机？这固然不错，但随着计算机网络一起发展的计算机数量很少，并没有现如今数以百亿级的量级，还轮不到为它单独建立一套网络，因此还是在电路交换基础上进行逐步优化。        首先对于核心网络，它属于ISP（Internet Service Provider）共治的计算机网络，因此它的自主性很强，可以使用大量分组交换网络设备进行重新建设。由于分组交换设备能够同时服务于多个主机，所以互联网核心网络中大量使用分组交换设备进行互联互通。        其次对于接入网络，它就可以利用电路交换网络，以电路交换的形式接入，最终在电信公司完成电路交换到分组交换的转换，这样利用毛细血管般的电路交换网络，完成尽可能多的主机接入。当然国内也有公司不信邪，比如：长城宽带，自己铺专线，这个专线就采用分组交换方式进行接入，由于成本太高，最后垮了。        除了长城宽带这种非典型ISP，大部分互联网使用者最早接触的应该是56K Modem的拨号上网，它采用与固话通信频率一致的4KHz频段进行通信，当它工作的时候，别人打你家电话是处于占线状态的。除了上网时不能打电话，速度也很慢，后来DSL出现了，不再使用4KHz电话频段，而是在电话线上使用4KHz以上的部分频段，一部分频段作为上行，一部分频段作为下行，由于上下行频段数量不相同，下行大于上行，因此简称为ADSL。        既然是将数据调制成信号送到电信公司即可，有线电视的同轴电缆线也可以完成这个工作，因此基于电视Cable的ISP也出现了，比如：华数，通过对有线电视线路的轻度改造，也是可以完成上网工作的。        使用Modem以及Cable的接入方式，本质上还是在电路交换上完成数据传输，直到光纤入户后，使用全新铺设的光缆，就不一样了。因为在光缆上跑的就是分组交换了，接入网到这一步就算是迁移完成了。 ISP与ICP        主机节点通过ISP接入到互联网，而ISP的数量成千上万，它们之间需要相互连通。ISP也会有大小，就好比银行一样，小的ISP负责接入终端用户，大的ISP可以连接多个ISP。        通过多个ISP的互联互通，主机之间可以通过ISP实现相互访问，互联网的基础设施就构建起来了。ISP之间可以通过IX（Internet eXchanger）进行连接，一般这种设备出现在骨干网上，或者简单的情况，双方通过使用几个路由器建立对等网络来进行数据交换，也是可以完成ISP互联的。        ICP（Internet Content Providers），互联网内容提供商，比如：谷歌或者腾讯。早期ICP是在ISP的数据中心里租用几台服务器，然后搭建网站对外提供服务，但由于成本、效能和可用性等多方面因素，规模以上的ICP就会自建数据中心。        以谷歌为例，一个数据中心内有上万台服务器，它们会向全球提供服务。ICP自建的数据中心一般会放在比较大的ISP数据中心旁，通过光纤连到数据中心，完成互联网的接入。        如上图所示，ISP和ICP组成了多张复合的网，ISP负责将终端接入到互联网，而ICP也参与进来，建立数据中心，与ISP相连。 网络分层        计算机和计算机网络的发展是相互促进的，越来越多的计算机应用都需要使用到计算机网络提供的服务，为了避免计算机网络重走计算机“差异化”发展的老路，就需要制定一套规范，使得不同类型的计算机都能够接入到网络中，同时在不同类型的计算机系统上开发网络应用，也会有一致的标准。        通信背景的人在一顿晚饭后，认为7是一个不错的数字，在此基础上制定了OSI参考模型，也就是那个只存在于书上的OSI七层模型。我们现实中，实际是TCP/IP模型，7层的OSI和4层的TCP/IP之间的对应关系与描述如下表所示： 名称 描述 TCP/IP参考模型 应用层 定义应用程序使用的通信协议，比如：HTTP协议 应用层 表示层 应用数据格式转换为可以在网络上互通的格式，比如：编解码，JPEG格式等 应用层 会话层 管理通信连接，比如：逻辑会话，session 应用层 传输层 建立和断开连接，比如：网络连接，TCP/UDP 传输层 网络层 支持地址划分和数据路由，比如：IP协议、ICMP 互联网层（网络层） 数据链路层 互相物理连接的两台设备之间进行通信，比如：以太网，MAC 网络接口层 物理层 二进制到电或光信号的互转，面向媒介进行传递，比如：MAU 网络接口层        目前虽然TCP/IP是事实标准，但是用5层还是会好一些，也就是： 应用层，负责应用协议，以及应用逻辑关系，可能是C/S架构的应用下，支持客户端和服务端的语义传递； 传输层，操作系统网络协议栈需要实现的功能，并且提供一致的SDK，能够支持应用开发面向字节的进行网络数据传输； 网络层，支持IP地址对设备的标识，能够以IP为基础，进行网络包的路由，从而满足分组交换的基本诉求； 数据链路层，点对点有效通信，设定媒介上传输的数据格式，到这里还是数字格式，面向字节； 物理层，链路层字节数据与媒介中传输信号的转换，有含义的信号传输。        网络接口层还是分成数据链路层和物理层，前者是面向物理层的数据建模，它是连接网络层和物理层的桥梁，网络层可以依靠它来进行编程，就像IP协议需要以太网支持一样。数据链路层通过将MAC化后的字节，发送给物理层的MAU，再经过MAU的调制，就会转换为在物理层媒介上传输的信号。两个层次的职责还是很明确的，所以5层结构显得更加合适。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-02.html":{"url":"book/computer-network-02.html","title":"热风-感悟《计算机网络：自顶向下》（02.应用层）","keywords":"","body":"感悟《计算机网络：自顶向下》（02.应用层） 应用层简介        应用层处于计算机网络TCP/IP协议栈5层结构中的最上层，也是是最丰富多彩的一层，今天用户使用的互联网应用大都处于这一层，并且大部分知名应用（也包括游戏）都是分布式应用。例如，手机淘宝应用，在用户手机终端会安装手机淘宝APP，同时淘宝也会部署服务器，在服务器上跑着会员、商品和交易等系统，手机淘宝APP和服务器应用进行网络通信，其通信基于协议。        协议的目的就是就双方沟通而言，无歧义的讲清楚一件事，它包括：语法、语义、次序和动作，在文本随后常见应用层技术一节，会以HTTP协议为例进行介绍。应用层协议一般来说分为公开协议和私有协议，比如：HTTP协议就是公开协议，开发者可以自由的编写其客户端或服务端，并与其他的客户端或服务端兼容，只要你遵守这个公开协议即可，而私有协议是开发者或组织自行定义的，外部人员很少掌握，定义的目的往往在于偏科式的解决某些特定问题。        应用层位于传输层上面，使用（传输层提供的）socket编程接口来实现应用功能，应用层对于传输层服务有以下几点要求： 可靠，能否将字节数据可靠的传输到目标端； 延迟，数据传递时尽可能的少受到外界影响，尽快的抵达目标端； 吞吐，单位时间内传输的数据尽可能多； 安全，传输过程是否安全，能否做到通信私密、防篡改或不丢失。        这些点不是说所有应用都要求传输层能做到，并且传输层也不可能同时兑现这么多要求，而且有些点是相互矛盾的，这只能是一种tradeoff。以视频在线播放为例，短时间要传递大量数据，数据中偶尔有一些字节出错，或者部分包丢失，问题也不大，只是影响一点点画质，所以这种应用对于延迟和吞吐就很关注，对于可靠性就没有那么在意。再以电子邮件为例，它要求数据传输准确，不能有偏差，对于可靠性要求很高，但是延迟和吞吐就没有多少诉求。        因此不同的应用需要结合自身的需求，来选择传输层提供的服务。传输层一般提供的服务有两种：一种是面向连接可靠的传输服务TCP，另一种是仅在IP协议上做了薄层封装，基于用户数据报的传输服务UDP，它们的共性是都具备了端口这个概念。 进程通信        传输层拥有端口的概念，就是为了支持（应用）进程通信。手机淘宝APP和淘宝服务器应用的通信，是手机操作系统上一个叫手机淘宝的进程与部署在阿里巴巴数据中心里某台服务器上的一个服务端应用进程的通信。通过给定端口，可以指定对端的进程，比如：指定对端IP的80号端口，该端口可能就对应一个进程ID为4321的Apache服务器进程，因此标识一个进程，需要有三个属性，如下表所示： 名称 描述 主机IP 对端主机的IP 传输层协议 TCP或UDP 端口 端口号        可以看到，如果IP相同，端口也相同，但是提供的服务端进程还需要区分是跑在UDP或TCP上，在不同的传输层协议上，也是不同的，因为客户端在进行socket编程时，会明确的使用不同的编程接口。        应用层基于socket进行编程，不同语言SDK的网络编程界面有所不同，但是编程模型和方式都是类似的，因为本质上这些SDK都是通过socket与本地操作系统的协议栈打交道。对于网络的访问都是委托给操作系统协议栈来完成，而不同操作系统基本都集成了TCP/IP协议栈，遵守其规范，所以功能层面差不多，这就使得这些SDK的网络部分基本都长得差不多。        如果进行网络编程时，两端IP和端口，至少4个属性，用起来感觉不麻烦的话，大抵也就没有socket编程了。因为完全可以基于这些参数调用系统API，但是这样会显得非常凌乱，所以提供了一个封装了上述4个属性的数据结构来作为双方通信的标识，它就称为socket。 socket可以是编程界面，也可以是对网络编程数据结构的定义。        对于TCP而言，socket是一个四元组，可以表示为，其中dst表示目标，而src表示来源。这个四元组的内容仅本地操作系统协议栈知晓，对端并不知情，本质上就是一种便于管理的数据结构，它可以对应到本机系统中的一个进程，也属于一种本地标识。        对于UDP而言，socket是一个二元组，可以表示为，它在数据发送时，才会要求提供目标的IP和端口，当然它也是本地的一个标识，只是不用事先提供目标IP和端口而已。 网络架构        从应用层角度看，网络架构分为两种：一种是C/S，另一种是P2P。C/S好理解，我们使用的浏览器其实就是C/S，C是浏览器，S是某个网站，只是浏览器比较复杂，支持很多功能特性，说好听点就叫B/S，但本质上还是C/S。P2P是（无中心节点的）点到点通信，管理难度高，但是可用性和利用率上相较于C/S有优势，毕竟S的服务能力是存在上限的。        虽然应用网络架构分为两种，但是应用并不会执着于一种，而是存在混合架构的情况。以分布式RPC框架Dubbo为例，它就是一种典型的混合架构，其架构特点如图：        可以看到Dubbo在服务发现阶段是C/S架构，而运行时调用阶段又属于P2P架构。因此选择应用网络架构时，需要结合应用自身特点和需求来进行设计，而不是一种非0即1的选择。 常见应用层技术        应用层常见的技术有很多，我们就算简单的打开浏览器访问一个网站，也会同时用到多种技术，比如：域名需要解析为IP，会使用到DNS技术；访问页面，会使用到HTTP技术；如果页面中包含了一些图片或样式资源，会使用到CDN技术。可以看到不同的技术解决不同的问题，虽然用户做了一个简单操作，但实际会涉及到非常多的技术。接下来，简单介绍几种常见的应用层技术。 HTTP        HTTP是互联网中最常用的技术，它遵循请求和响应模型，HTTP协议的请求和响应结构如下图所示：        前文提到，协议包括：语法、语义、次序和动作，对于HTTP协议而言，语法就是命令式的指令，语义包括了HTTP的操作，比如：获取资源的GET、提交资源的POST，每一个语义都代表了HTTP协议的行为目的。HTTP协议由客户端发起，服务端响应，一来一回，这就是它的次序，而请求到达服务端，服务端如何处理，以及回复的响应，客户端该如何理解发回的响应，这就是协议对应的动作。 Email        Email在即时通讯以及社交软件走红后，大家用的越来越少了，但是在互联网早期它和HTTP网页应用是两个最主流的应用，它由用户代理客户端和邮件服务器组成，它们之间的关系如下图：        用户代理或邮件服务器通过SMTP协议向对端传送邮件，其中邮件服务器中会给每个用户一个专属的邮箱。邮件服务器之间也是通过SMTP协议来进行互传的，用户代理通过POP3协议从邮件服务器上拉取邮件到本地。        Email协议以及运作过程看起来像模拟了真实邮件的传递，邮件从一家邮局到另一家，而用户只需要到自己开户的邮局去取件即可。可以想象，如果对该过程进行建模，将协议中的需求放到HTTP协议中，也是可以实现电子邮件的，只是说在互联网早期，很多应用层协议就是根据自己的场景，专属化的被定义出来，其协议的数据结构也是良莠不齐，扩展性也很拙劣。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-03a.html":{"url":"book/computer-network-03a.html","title":"热风-感悟《计算机网络：自顶向下》（03a.传输层）","keywords":"","body":"感悟《计算机网络：自顶向下》（03a.传输层） 传输层简介        传输层位于TCP/IP五层结构中自顶向下的第二层，它向上提供socket编程接口，为应用层服务提供面向网络的编程界面，向下使用网络层提供的服务进行数据传输。传输层为运行在不同主机上的应用进程提供逻辑通信的能力，同时为了让应用层尽可能少的关注到网络传输细节，还支持流量控制和拥塞控制，使其能够最大限度的支持应用层对于网络服务的诉求。 结构        Socket作为传输层的编程界面，其实现由不同的操作系统各自提供，但是socket编程方式大都相似，这是因为传输层从标准上分为了面向连接的TCP和面向用户数据报的UDP，由于是标准协议，所以在行为上基本类似。不同的语言会封装不同的接口以及提供数据结构来支持socket编程，但它们的底层实现都会依赖具体操作系统的API，直白点就是，任何语言的网络编程本质上都是系统编程。系统API的实现是来自不同操作系统的协议栈程序，应用程序透过socket与网络的交互，实际上都是和本地协议栈程序的交互，委托它与对端系统（的协议栈程序）进行通信，上述过程如下图所示：        可以看到传输层是联通应用与操作系统协议栈的桥梁，也是网络编程的起点，它以软件的方式支持了应用层的构建。 TCP和UDP        TCP是面向连接的传输层协议，它能够提供可靠的传输服务，UDP不是面向连接的传输层协议，传输服务不可靠。UDP叫用户数据报协议，啥叫用户数据报？可以看看下图：        如上图所示，UDP相当于在IP之上增加了对端口（进程）的支持，时髦的话可以这么说：为基于IP的网络服务提供了租户（或命名空间）的概念，进一步赋能了网络层。网络层进行通信的数据单元称为分组或者包，也可以称为数据报（Datagram），而UDP是构建在网络层之上，面向用户使用的，所以就称为用户数据报（User Datagram）。        这么看UDP以源IP和Port为基础，向外进行多点调用，也就是进行广播。除此广播以外，UDP能干的，TCP全部能干，而且可靠，那要UDP干啥？原因在于，UDP协议短小，同时没有流量控制和拥塞控制，简单说就是不讲究，只管发送和接收，相对延迟小，比较适合实时流媒体，比如：webcam。现实是有些防火墙会禁用UDP协议，所以快这个好处也不见的那么好使。        TCP是面向连接的传输服务，叫面向连接而不是基于连接，就是站在网络传输的两端来说的，因为它们各自都向连接中发送或接收数据，感觉上好像和对端建立了一个现实存在的连接，但实际上并没有。使用TCP通信的双方会有连接建立的过程，那只不过是一种达成共识的手段，它需要双方系统内核（或者说协议栈程序）来维护这个共识，如果其中一方突然掉电关闭，对端的连接是不会关闭的，而是以为这个连接仍然存在。        面对这种意料之外的突然下线，依旧存活的一方还是可以向这个连接发送数据，只是没有响应，而操作系统内核会检测出发送数据的响应超时，进而采取关闭连接的行动。 RDT的设计与演化        TCP看起来很神奇，但在复杂的东西都是从简单的事物演进而来的，接下来就跟随本文来设计一款可靠数据传输协议。RDT(Reliable Data Transfer)，即可靠数据传输，我们通过思考如何构建一个可靠的数据传输协议来理解TCP协议面对的问题，以及根据RDT的解法来思考TCP中那些不易理解的知识点。RDT协议是构建在网络层之上的，因此它支持分组交换的基本特性，同时它接收应用层数据，并按照要求进行传输。        我们自定义的RDT协议接收应用层传输的数据并完成传输，也就是应用层需要调用RDT提供的方法进行数据发送和接收，在这里假设两个方法对应上述操作：rdt_snd(Segment segment)和rdt_rcv(Packet packet)。对于rdt_snd方法而言，应用层需要创建好Segment报文对象，其中包括需要传输的字节数组，然后rdt_snd方法负责发送到对端。对于rdt_rcv方法而言，从网络层收到分组Packet后进行转换处理，将其转为Segment后，调用应用层的处理逻辑，这样就实现了数据响应。        RDT协议构建在网络层之上，发送时需要将Segment转换为Packet进行发送，接收时将收到Packet转为Segment进行应用回调。对网络层的发送和接收分别假设存在net_snd(Packet packet)和net_rcv(Packet packet)两个方法，至于它们和数据链路层的关系，RDT协议就不关心了，只知道Packet在传输时，需要传入来源和目标IP。        RDT协议的设计无法一蹴而就，需要经过逐步的假设和推演逐步完善，而假设是基于网络层的服务质量而来的，如果网络层能保证可靠，rdt_snd和rdt_rcv就直接调用网络层提供的方法就好了，但现实没有这么简单，不过我们还是一步一步来。在正式开始前，对于传输可靠性我们需要有一个标准，这样才好确定协议的目标以及效果，一般来说传输层服务的可靠性体现在：不出错、不丢失和不乱序这三点上，简单说就是不错、不丢和不乱。 RDT 1.0        假设网络层提供的通信服务是可靠的，也就是数据传输： 没有比特出错，即不错； 没有分组丢失，即不丢； 没有分组乱序，即不乱。        在这么靠谱的网络层面前，连神仙都会给它点个赞，我们RDT就能像Spring一样，啥都不干的包一层，至于苦力活么？交给网络层（Tomcat）干去，自己写，指不定多少bug。        RDT 1.0数据发送的调用关系，如下图所示：        如上图所示，传输层需要提供进程到进程的通信解决方案，因此RDT协议要求：发送数据的应用进程需要给定一个标识，同时也需要给定对端应用进程一个标识。最合理的方式是进程ID，但是由于这个ID经常变动，所以就用一个命名空间来替代，叫：端口，一个整型就好。        发送方设置好Segment的srcPort和dstPort，也就是来源端口和目标端口后，就可以填充数据data进行发送了。rdt_snd方法的实现非常简单，只需要调用encode方法，将Segment转为Packet，然后调用网络层提供的net_snd方法即可。        RDT 1.0数据接收的调用关系，如下图所示：        如上图所示，接收方从net_rcv方法收到Packet，然后调用rdt_rcv方法进行处理。rdt_rcv只需要调用decode方法，将Packet转换为Segment，随后可以根据Segment.dstPort来确认数据包是发给哪个应用进程的。协议栈程序查找确认进程后，将数据从内核拷贝到对应的进程空间中，由于用户进程阻塞在IO读取的操作上，接下来唤醒用户进程，使其从IO读取操作上拿到数据并返回即可。        RDT 1.0基于可靠的网路层，抽象出了端口这个概念，解决了进程到进程的通信。 RDT 2.0        接下来，我们逐步拆除网络层通信服务可靠传输的假设，目的是最终能够在不可靠传输的网络层上，构建出一个能够支持可靠传输的传输层协议。对于网络层的通信服务，假设变更为： 可能比特出错，即有错； 没有分组丢失，即不丢； 没有分组乱序，即不乱。        传输层接收到应用需要发送的字节数组，转换为Packet后进行发送，如果对端收到内容，字节数组中某些位由于传输时信号干扰导致解码后出现错误，这样传输层就不是可靠的了。        解决办法也比较简单，在发送字节数组之前，给Segment设置一个属性checkSum，一个整数，它可以通过两端都具备的一个摘要函数来生成，比如：int check_sum(byte[] payload)。这样对端收到字节数组后，使用check_sum函数计算后，同Segment中的checkSum值进行比对，通过比对摘要值就知道数据是否出错了。        接收方进行数据校验时没有出错还好，如果出现错误，就需要通知发送方进行重新发送，这样就需要设计一个应答Segment。如果只是错误了才回复显得太局限，那就干脆设计成两种应答Segment，一种是成功应答ACK，另一种是失败应答NACK，二者都继承于Segment。        RDT 2.0数据发送的调用关系，如下图所示：        如上图所示，应用依旧调用rdt_snd方法发送数据，而该方法的实现有了改变。        首先，需要在发送数据之前调用check_sum方法计算出待发送字节数组的摘要checkSum，将其设置到Segment中。其次，将构造好的Segment转换为Packet，调用net_snd方法进行发送，发送后不能直接返回，还需要调用net_rcv方法，等待对端的响应。最后，如果响应Packet收到，将其转换为Segment，如果类型是ACK，则返回，发送成功；如果是NACK，则调用net_snd方法进行数据重发。        RDT 2.0数据接收的调用关系，如下图所示：        如上图所示，网络层收到分组Packet，将其传递给rdt_rcv方法，由传输层RDT来处理，由于checkSum和应答的出现，rdt_rcv实现会有所变化。        rdt_rcv依旧是将Packet转换为Segment，然后使用check_sum方法计算Segment.data的摘要，并与Segment.checkSum进行比对。如果相等，则调用net_snd方法，发送ACK应答，同时将Segment.data返回给应用进行处理，反之，发送NACK应答，等待发送方进行重传。        随着checkSum和应答机制的加入，在网络层传输可能出现比特错误的情况下，RDT 2.0依旧能够可靠工作，通过让发送端进行重传来承担起可靠性的责任。 RDT 2.1        RDT 2.0在发送字节出错的情况下，能够搞定可靠传输，不容易。接下来，我们再继续挖墙脚，对于网络层的通信服务，假设变更为： 可能比特出错，即有错； 可能分组丢失，即会丢； 没有分组乱序，即不乱。        RDT 2.0虽然具备纠错功能，但对于分组会丢的情况就无法处理了。应用层将需要传输的字节通过调用传输层的rdt_snd方法进行发送，如果传输中的Packet出现跨多个路由节点后丢失的情况，该如何应对呢？现实世界中，如果一方寄送包裹，另一方收包裹，接收方如何判断出有包裹漏掉了呢？答案是，给包裹编号。假设寄送包裹的一方会给包裹上贴一个面单编号，而且这个编号是自增的，那么接收方就可以根据这个编号来判断出有没有包裹漏掉，比如：接收方收到的包裹编号是{1, 3，4}，那么就代表{2}号包裹丢失了，这时就要通知发送方检查一下{2}号包裹的寄送情况。        发送时有编号，应答确认回复时能指出编号，因此需要在Segment上增加两个编号字段，类型可以是整型，一个叫：sequence，代表报文Segment的发送编号，另一个叫：ack，代表接收方确认收到的编号。假设发送端发送Segment.sequence为3的Segment，接收端收到的上一个Segment，其sequence为1，则接收端收到3号Segment后，就认为2号Segment丢失了。此时，最简单的做法是直接丢弃3号Segment，同时发送NACK.ack为2的应答。        发送方收到NACK，同时ack编号为2，此时就会将2号Segment进行重发。 RDT 2.1数据发送的调用关系，如下图所示：        如上图所示，rdt_snd方法变得更加复杂了，它会将需要发送的Segment先编号，然后放置到缓冲区snd_buf中。这个编号就好像数组的下标一样，当发送时，从Segment数组，也就是snd_buf中，依据snd_idx取出一个Segment进行发送，接着执行net_rcv方法处理应答。        如果响应是ACK，则将ACK.ack + 1作为发送游标snd_idx的值，即snd_idx = ACK.ack + 1，然后继续发送下一个Segment。如果响应是NACK，则将NACK.ack作为发送游标snd_idx的值，也就是snd_idx = NACK.ack，相当于要进行重发了。        发送方会根据ACK或者NACK中的ack编号来调整发送的Segment，由于发送和接收是对称的，所以从两端来看，如果发送端的Segment是src，接收端收到的Segment是dst，那么数据传输的可靠性就需要让以下两个等式成立，即：src.sequence == dst.ack和src.ack == dst.sequence。 为什么会有src.ack == dst.sequence，因为传输层是双向的，此时接收端，彼时发送端。        RDT 2.1数据接收的调用关系，如下图所示：        如上图所示，rdt_rcv方法也需要准备一个缓冲区rcv_buf，顺序的将接收到的Packet转换为Segment后，根据接收游标放置到缓冲区数组中。        接收方需要定义一个期望接收的游标，称为rcv_idx，然后根据它与Segment.sequence进行比对，在check_sum方法比对的基础上，还需要看一下sequence是否是自己想要的。如果rcv_idx == Segment.sequence，则将Segment放置到数组中，并将数据返回给应用程序，同时调用net_snd方法发送ACK，其中ack设置为rcv_idx，发送成功后，将rcv_idx更新为rcv_idx + 1，代表接收方想收下一个Segment了。如果rcv_idx != Segment.sequence，则代表rcv_idx编号的数据包没有收到，只能丢弃掉当前处理的Segment，同时调用net_snd方法，发送NACK，其中ack设置为rcv_idx。        可以看到，接收方知道自己的目标序号，回复ACK或者NACK给发送方，如果收到的Segment.sequence为目标序号，则表明之前的数据接收都是成功的。从发送方的角度看，snd_idx指的是发送Segment的预期，而一旦接收到的ack值与之相等，则代表对端成功接收到了数据，所以通信的两端都对收到的数据sequence或者ack有预期，而这个预期就是维护在各端内存中的状态（比如：snd_idx或者rcv_idx）。 RDT 2.2        RDT 2.1已经相当完善了，在一定程度上能够可靠的工作，不过代价就是太复杂了，比如：接收方需要回复ACK或者NACK，而发送方也需要处理ACK和NACK。到这个阶段需要做一些协议重构，让它变得统一和简单些，在假设不变，即网络传输有错、会丢但不乱的前提下，看看该如何优化现有的RDT协议。        从ACK和NACK下手，通过观察传输层的工作，其实就是一个按编号搬运数据的过程。NACK实际上可以被ACK所统一，也就是说：ACK(0) == NACK(1)，NACK表示需要重传{1}号Segment等同于ACK表示接收方已经收到了{0}号数据包，因为编号自增是双方的共识。        ACK和NACK的对应关系，如下图所示：        如上图所示，可以提炼出二者之间的关系，即：NACK(N) = ACK(N - 1)，在此基础上，可以尝试对RDT 2.1进行优化。另外一点，既然Segment带有ack属性，那么完全可以不使用Segment的子类ACK，直接基于Segment就好了。        RDT 2.2数据发送的调用关系，如下图所示：        如上图所示，rdt_snd方法对于数据发送的行为不变，而是调用net_rcv方法收到Packet后的处理逻辑有所不同。将Packet转换为Segment后，如果该Segment.ack == snd_idx，则代表发送的数据被确认，没有丢失；如果该编号不等于snd_idx，则重发Segment.ack + 1的Segment。        举个例子，如果发送的Segment.sequence = 20的数据包到了对端，接下来应该发送编号为{21}的数据包，但是发送端却收到了Segment.ack = 19的回执Segment，它代表接收方收到了[0, 19]的数据包，这里我们假设sequence从0开始。如果接收方收到了{19}及其之前的数据包，那么{20}号数据包就是没有收到，发送方就需要发送第{20}（19 + 1 ）个Segment，这表示在重发了。        RDT 2.2数据接收的调用关系，如下图所示：        如上图所示，rdt_rcv方法在处理Segment时，只需要关注Segment.sequence和rcv_idx的关系，如果符合期望，即Segment.sequence == rcv_idx，则将Segment添加到rcv_buf，随后交给应用进程，否则做重传处理。响应的结果不用区分ACK或NACK，而统一用Segment的形式发回，只是在ack的设置上有所不同。        如果请求Segment.sequence == rcv_idx，表示符合期望，则响应的Segment.ack会设置为rcv_idx，同时rcv_idx自增。如果不符合期望，则响应的Segment.ack会设置为rcv_idx - 1，代表收到了编号小于等于rcv_idx - 1的所有Segment，期望对端重发rcv_idx的Segment。        从响应Segment看，无论收到Segment.sequence是否符合期望，响应Segment.ack始终表示接收端已经收到的（最大）编号，核心思想就是：作为接收方，我已经成功收到第几个数据包。 RDT 3.0        RDT 2.2在字节发送出错，且发送分组可能丢失的情况下，能够完成可靠传输，并且将发送和响应的数据结构做到了统一。接下来对于网络层的通信服务，假设变更为： 可能比特出错，即有错； 可能分组丢失，即会丢； 存在分组乱序，即会乱。        如何在一个有错、会丢和会乱的网络层通信服务基础上，支持可靠的数据传输？RDT 2.2其实在一定程度下是可以做到的，只需要假设网络层的消息传递是完全同步的，也就是每个数据包到对端后，对端响应发回来，然后才可以发送下一个数据包，其本质就是一个同步的半双工网络。        如果只是两点传输，同步半双工网络慢也就算了，但是在分组交换网络中，如果完全同步，就会造成大量的带宽容量浪费。因为发送后同步等待响应的效率太低，如下图所示：        如上图所示，发送方和接收方之间分组交换设备的带宽只有发送传播和应答传播两段时间在服务，其他的时候均是空闲的，网络容量无法跑满。提升网络带宽的利用率，最直接的做法就是让发送方做到批量发送，异步接收响应，这样就可以尽可能的跑满带宽。        如果发送方一次发送多个数据包，每个Segment的sequence可以做到自增，但接收方响应的ack该如何判定？与此同时，接收方如果由于网络延迟导致应答的很慢，发送方该如何确保数据一定会重传？发送方如果一次发送多个数据包，接收方如果只接收一个，那么效率也会变低，如果同时接收多个，超出了接收缓存rcv_buf的上限该怎么办？        分组乱序需要做到：发送方异步多发，接收方异步多收，这样的异步全双工网络实现起来就很有难度了。如果是全同步的网络，它会非常符合人类直观思考，但其效能也是非常低的。要建立异步全双工网络，这么多问题需要逐一解决。        首先，对于N个发送的Segment，接收方回复不必是对等的N个响应Segment，而是M个Segment，其中M ，也就是用更少的响应来支持传输会话。举个例子，如果发送方一个批次发了[1, N]共N个Segment，由于接收方收到这些数据包有先后，当先收到完整的[0, X]后，就回复响应Segment，其中Segment.ack = X，代表前X个数据包收到了，再等到所有数据包都收到后，最后回复对第N个Segment的确认。这样只需要两三个响应就让发送方和接收方达成了共识。每次发回的响应，从接收方发出的一瞬，ack编号始终是递增的，如果每收到一个包就发送对应的响应，那么ack编号在发送方看来，由于乱序收到响应，ack编号就看来就显得飘忽不定了。        其次，顺着这个思路，站在发送方的角度看，收到的ack就是跳跃的，那么如何能够确定发送的Segment中有哪一个被漏掉了？对于重传的确保，需要对Segment发送设置超时时间，这样可以保证数据一定会进行重传发送。当一个Segment进行发送后，会设置一个计时器，如果在一段时间内，没有收到超过（或等于）它编号的响应，就发起重传。有了超时重传的兜底，发送方就更具备容错和防丢失的能力了。        最后，超出接收缓存上限的问题。该问题的本质是发送方不断的发送Segment，而由于接收方处理的速度较慢，导致接收缓存rcv_buf被占满。处理的方式其实很简单，如果发现rcv_buf满，直接丢弃即可，因为增加了发送方的超时重试，所以不用担心数据会丢。这么做简单粗暴，但是丢弃的网络流量还是浪费了，如果能够告知发送方，接收方的rcv_buf还能容纳多少Segment，那就最好了，因为发送方可以根据接收方rcv_buf的大小来调整发送数据包的节奏。        可以尝试在Segment中定义一个接收窗口的属性，叫：rwnd，当每次响应的时候，就将当前可用的缓存大小设置到该属性，由响应Segment带回。发送端收到响应Segment后，不仅要做数据完整性检验，也需要看一下ack做好重发处理，同时还要根据rwnd属性来调整一下接下来发送的数据量。        通过不对等的发送和应答，增大了RDT的吞吐量，同时超时机制的引入，使得不对等发送和应答的基础上，也能保证重传，rwnd的引入，接收方无形中可以影响到发送方的发送速率。如果我们离远一点观察RDT 3.0，你会发现发送方和接收方通过Segment的属性，以及维护在两端各自的缓存和游标，相互牵引，相互制约，将数据从发送方完整的搬到接收方。        当前的RDT 3.0协议，已经和TCP协议非常像了，或者说它已经是一个原始版的TCP协议了。        RDT 3.0数据发送的调用关系，如下图所示：        如上图所示，其中黑色线路为发送流程，红色线路为确认流程，而绿色线路为超时流程，这些流程分别运行在不同的进程中。应用层调用rdt_snd方法后，只是将Segment存放到发送缓存snd_buf中，而缓存中每次会选定一个固定的数量进行发送，可以称为发送窗口snd_wnd，超出snd_wnd的Segment暂时不发。snd_idx指向snd_window的起始位置，然后顺序设置Segment.sequence并调用net_snd方法，将Packet发送到网络，当snd_idx到达snd_wnd的结束位置时停止。        由于net_snd方法是异步执行，所以引入了ack_idx来表示发送方收到的ack编号，它和snd_idx一样，会一直递增，只是ack_idx的设置来自于对端的应答响应，而不是像snd_idx那样的主动自增。发送每个Segment时，还会注册对应的定时任务，每隔几秒会检测当前任务中Segment的sequence和ack_idx之间的关系，如果ack_idx >= sequence，则返回，否则定时任务会重新调用net_snd方法重传当前定时任务负责的Segment。        net_rcv方法接收来自对端的响应，如果响应Segment.ack >= ack_idx，则更新ack_idx为响应Segment.ack，代表目前已经有ack及其之前的Segment都被收到。收到响应的一刻，snd_idx - ack_idx ，这代表在确认路途上的Segment数量已经小于发送窗口snd_wnd的长度了，此时snd_wnd开始向前移动，snd_idx可以突破旧有的限制，继续将向前移动多出来的Segment发送到网络中。        可以看到基于缓存和超时rdt_snd方法已经变得很复杂了，但是它的效能和适应性也得以提升，发送的频度受限于接收的能力，RDT 3.0已经建立起了具备反压特性的传输机制。        RDT 3.0数据接收的调用关系，如下图所示：        如上图所示，其中黑色线路为接收流程，而绿色线路为定时触发流程，这些流程分别运行在不同的进程中。rdt_rcv方法不是在单个的处理Segment，对端发送窗口内的Segment会并发到达。rcv_idx指向接收缓存rcv_buf中的起始位置，当接收到Segment后，如果sequence ，则丢弃并返回，否则将Segment存入到rcv_buf中对应的编号处。        接收方会周期检查rcv_buf，如果从rcv_idx开始有连续若干个Segment都收到了，则将连续的几个Segment中的data数据交给应用进程，并前移rcv_idx，同时发送响应Segment，其中ack等于rcv_idx，表示rcv_idx之前的数据包都收到了。        响应Segment发送到发送方，又会触发发送方的snd_window前移，进而发送后续的数据。 RDT 3.1        RDT 3.0在一个有错、会丢和会乱的网络层基础上，不仅建立起了可靠的传输服务，还支持并行发送以及乱序接收，极大的提升了网络的利用率。RDT 3.0要求在两端保持一些状态，比如：rcv_idx、snd_idx和rcv_buf等，这些状态是服务于两端进程的，所以它们需要基于IP和端口被单独维护，如下图所示：        如上图所示，以发送端为例，会以为KEY，开辟独立的通信属性，包括：rcv_idx、snd_idx、rcv_buf、ack_idx、snd_buf和snd_window，其中src是发送方，dst是接收方。虽然是发送端，但是也会分配rcv_idx和rcv_buf，原因是发送端不仅发送数据，也会接收对端的响应数据，传输层是全双工的。        发送端发送数据后，如果接收方响应数据的Segment很多，超过了发送端的rcv_buf限制，就会导致无效的重传。同时发送端的snd_idx起始值也需要给到接收端，这样方便接收端可以以此来确定自己的rcv_idx，所以RDT协议还可以增加一个控制数据包的概念，代表这个Segment中的数据是为了设置对端的通信属性。        可以使用一个整型，按照二进制位的方式来表示Segment的特性，比如4位二进制：0001，第一位为真表示该Segment是为了建立连接，接收方可以用Segment中的sequence属性来初始化自己的rcv_idx；0010，第二位为真表示该Segment的ack属性有效，可以用Segment中的ack属性来更新ack_idx；0100，第三位为真表示该Segment是为了断开连接，接收方收到该Segment代表它是发送方最后一个包含数据的Segment，同时也可以准备回收分配的通信属性了。        该整型定义为sign，这样升级为3.1的RDT就比3.0显得更加体系化了，以连接建立过程为例，如下图所示：        如上图所示，发送端发起一次连接建立操作，接收端也要进行一次连接建立操作，这样两端就完成了通信属性的交换，双方对于snd_idx，rcv_idx、ack_idx以及rcv_buf等有了共识。        由于sign可以合并，所以可以尝试将第二步接收端的响应和连接建立操作（第2步和第3步）进行合并，也就是Segment.sign = 0011，而Segment.sequence=13, Segment.ack=27，也就是只需要三次传输就能达成共识了。RDT 3.1可以说与TCP协议非常接近了，但是它还是局限在发送和接收两端，没有将两端之间的网络考虑在内，没有考虑网络拥塞的问题，但是它对于理解TCP协议是很有帮助的。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-03b.html":{"url":"book/computer-network-03b.html","title":"热风-感悟《计算机网络：自顶向下》（03b.传输层）","keywords":"","body":"感悟《计算机网络：自顶向下》（03b.传输层）        在完成RDT协议从1.0到3.1的演进后，可以证明在一个不可靠的网络层服务商上是能够利用软件协议构建出一个不出错、不丢失和不乱序的传输层服务。如果只是局限在两端通信的维度，将两端之间的分组交换设备视作透明的，那RDT 3.1其实已经算是一个挺不错的传输层协议了，但是网络的复杂性始终在向你证明，看似简单而相同的节点所组成的网络，随着规模的扩大，有很多意想不到的问题会出现。        分组交换由于具备高可用和低成本的特点，从而取代了线路交换，计算机通信终于摆脱了电报电话网络。而在分组交换上构建网络时，面对的是众多接入方（以及旧有网络），势必需要提出一致且简洁的协议来应对需求，这就是TCP/IP协议。其中TCP向上承担不同种类网络通信SDK的接入，向下利用不可靠的IP协议构筑可靠的通信服务，同时能够观察网络状况，避免拥塞。TCP协议自1968年开始出现，随着1995年的互联网大发展而成为使用最为广泛的传输层协议。 协议结构        回想RDT 3.1协议中对报文Segment的定义，其中包括属性：srcPort、disPort、data、checkSum、sequence、ack、rwnd以及sign，共8个。用一句话概述就是，报文Segment定义了来源端口srcPort，表示数据来自srcPort对应的进程，需要发送到目标端口dstPort对应的进程，同时携带了数据data以及data的签名checkSum，该Segment的序号是sequence，而本地已经成功收到了对端ack个数据，Segment的业务含义见sign，收到请求后如果发回响应数据，建议连续发送报文的长度不要超过rwnd。        这么看Segment考虑的挺周到，接下来看一下TCP协议是如何设计的，如下图所示：        如上图所示，TCP协议设计的比较紧凑，从第4行的数据偏移量，也就是协议头的长度可以看出来，TCP协议是一个变长协议。协议自上而下，分别是发送端和接收端的端口，发送编号与响应编号，以及6位的保留位，6位的控制位和16位的窗口大小。如果对应到RDT 3.1的Segment上，缺失的概念就只剩下数据偏移量和紧急指针了，后者协议栈实现可以不关注，算是个可有可无的概念，接下来还是看一下数据偏移量。        数据偏移量Headersize表示的是TCP头的长度，字节数一般会乘以4，主要是只有4位的Headersize最多也就表示15，所以用一个乘4的方式可以多表示一些内容。如果只有TCP头的长度，那么携带的data长度如何表示呢？这就需要用到IP协议，因为在IP协议上会标出IP分组的长度以及IP头的长度，由于TCP报文是作为IP的payload，所以用户需要传输的data长度可以通过公式(IP分组长度 - IP头长度 - TCP头长度)计算得到。如果从变长协议的规范看，变长协议至少要包括数据报的总长度以及协议头长度（或者数据体长度）两个长度才能正常工作，因此TCP协议其实算不上一个能独立运作的协议，更直白的说，TCP是IP上的一种功能应用，是IP协议上的一种处理策略。 TCP/IP协议最原始的版本TCP和IP本来就设计在一起。        UDP协议是定长协议，也就是头部始终是8个字节，但是其数据内容长度字段是单独标识的，所以UDP协议可以算是一个能独立运作的协议，从这点看TCP协议设计的水平还不怎么高，不自洽，不完备，但是和很多计算机底层技术一样，设计的不完美，但是不妨碍用。        对于控制位，功能也就是类似RDT协议中Segment.sign，它是6位的，也就是有6种功能，由于是按位表示功能，所以这些功能可以叠加。具体功能和描述如表所示，其中位数是自右向左： 名称 位数 描述 URG 6 表示紧急指针有效 ACK 5 表示ACK号有效，因为ACK号是每次TCP传输中都携带的，假设第一次建立连接的TCP报文，其ACK号字段显然是无意义的，此时就需要控制位中的ACK来告知当前报文中的ACK号是否有意义 PSH 4 Flush操作，将发送数据从缓存中发到网络 RST 3 强制断开连接 SYN 2 发送和接收相互确认的需要，回忆RDT协议中建立连接的过程，目的就是同步序号到对端的游标，目的是达成共识 FIN 1 断开连接        TCP协议的实现依靠协议栈程序，不同操作系统的实现会有不同，但由于TCP协议是标准的，所以不同操作系统的协议栈程序依旧能完成相互通信，简单说就是不同操作系统可以使用网络进行相互通信。 建立连接        在RDT协议的设计演化过程中，3.1版本的RDT提到了连接建立，它是通过三次单向的报文请求来实现通信两端对于连接状态能够达成共识。TCP协议也是一样，通过“三次握手”来建立连接。连接建立除了交换发送编号、接收数据的窗口大小，还包括链路层帧的大小，也就是MTU（Maximum Transmission Unit）的大小。        传输层的连接建立关数据链路层什么事？管得也太宽了吧。在介绍原因之前，先简单看一下MTU是什么。        MTU表示数据链路层一个帧所能携带的最大数据量，帧的单位还是字节，毕竟在二层以上还属于字节，到物理层就是信号了。数据链路层的实现由很多，目前使用最广泛的就是以太网，以太网默认是1500字节，其结构如下图所示：        可以看到IP分组被装在一个帧里面，如果一个IP分组的大小超过了当前数据链路层的上限，它会被拆分到多个帧中，那么除了第一个帧还可以从IP分组头部信息中知道自己是谁，后面的帧里面装的就是阿巴阿巴了。从完整性的角度考虑，IP分组需要能够按照当前数据链路层定义的MTU大小来做好自身分片的规划，IP协议如此，TCP协议作为IP协议上层的一个“应用”，连协议头都不完备的它（TCP）就更没有资格讨价还价了。        虽然数据链路层定义了MTU，但是传输的数据中包含了IP与TCP的头信息，所以还需要将头信息数据从MTU的载荷中减去，由此得到MSS（Maximum Segment Size），最大的报文长度，看Segment就知道它是面向传输层的。        对于数据传输而言，上述分片策略如下图所示：        如上图所示，对于一个完整的HTTP协议消息（注意：称消息表示该协议属于应用层），它的尺寸无疑是相对较大的，因此它会被按照MSS做分拆，拆好的数据会添加对应的TCP协议头，最终被放置到IP分组中发往对端。        不同数据链路层通过适配TCP/IP协议来做到互联互通。如果一个MTU是6400字节的数据链路层实现与以太网（1500字节）进行通信，以太网向对端发送的帧理论上能够被其识别，反之则不然。因此在进行TCP建连时，还需要交换自身的MSS大小，双方需要协商出一个用于通信的MSS，也就是取得min(MSS[src], MSS[dst])作为当前TCP连接所使用的MSS。        发起建连的一方，TCP报文的控制位SYN为真，sequence是随机生成的，只是表示一个位点，随后发送多少字节，它就会增加多少，而随机初始化一个值的目的主要是出于安全考虑。至于接收窗口是TCP报文头中定义的，而MSS这类的信息会放在TCP协议定义的可选字段中携带过去。        TCP协议建连的“三次握手”过程，如下图所示：        如上图所示，客户端发起建连操作，由程序调用socket来完成，一般步骤都是先定义服务端的IP和端口，也就是准备好服务端的Endpoint，然后客户端调用connect方法进行连接。        在发送SYN报文之前，客户端协议栈需要为当前TCP连接分配好接收与发送缓存，同时完成本地（随机）端口的绑定，该端口的目的是向协议栈注册进程与端口的对应关系，保证对方回报文后，能根据它找到来时的路。当缓存开辟完成，准备工作就绪后，还需要使用网络层提供的Path MTU Discovery机制，查询出当前数据链路层的MTU大小。一般该机制通过网络层ICMP协议实现，也就是调用对端之前，先撸一下自己，得到MSS后，就可以发送SYN报文了。        SYN报文包含了随机生成的sequence，接收窗口rwnd以及MSS等信息，然后将报文转换为分组并依托IP协议的路由转发，如果一切顺利将会抵达服务端。如果发送的SYN报文丢失，TCP还会有重试机制，通过报文重传来确保可靠性，当然服务端收到报文后，也会校验一下，如果通过了，接下来就该服务端出牌了。        服务端收到SYN报文，由于服务端程序先期绑定到了对应网卡和端口上，所以SYN报文中的目标端口是可以找到服务端程序的。协议栈根据SYN报文以及分组中的端口和IP，创建出一个socket连接，该连接可以由唯一确定。 以服务端的角度看，服务端程序绑定端口启动起来，外部有一个客户端通过TCP连接上来，此时会存在两个连接，一个是用来接收建连的连接，另一个是客户端和服务端之间的连接。        服务端连接一旦创建，就需要分配接收与发送缓存，同时会将连接信息注册到协议栈，这样协议栈可以通过来定位到对应的TCP连接，也能从该连接找到服务端程序。服务端的初始化工作完成后，就需要将服务端生成的序号以及针对SYN报文的响应进行回复，也就是发出SYNACK报文，这个报文同时包含了SYN和ACK，简称为SYNACK报文。        SYNACK报文中SYN相关的部分就是服务端随机生成的sequence，以及和先前客户端发送的SYN报文中相类似的rwnd以及MSS等信息，而ACK相关的部分就是针对SYN报文中sequence的回复。假设SYN.sequence=23，那SYNACK.ack就会是24，表示24以前的数据已经收到，目前需要收到从24开始的数据。        SYNACK报文发回客户端后，客户端协议栈程序能够根据确定是哪个TCP连接，以及定位到哪一个进程。根据SYNACK中src-Port以及分组中的src-IP，将对应的数据更新到协议栈，同时协议栈针对SYNACK报文中的SYN部分进行响应回复，发出ACK报文。        客户端发送的ACK报文主要包括了对SYNACK.sequence的回复，如果服务端发送的SYNACK.sequence=10, 则ACK.ack=11，这和之前的服务端行为是类似的。需要注意两点：第一，ACK回复的是SYN或者说SYNACK的SYN部分，不会存在针对ACK的ACK报文；第二，上述建连过程都是操作系统内核中的协议栈进程来完成的，对用户进程是透明的，或者说无感的。        通过“三次握手”，TCP连接就在双方的共识中建立起来，客户端和服务端如何知晓该发送SYN或者SYNACK报文呢？答案是状态，根据各自的连接状态来期望得到的报文，以及得到报文后所做出何种动作。建连状态的变迁如下图所示：        如上图所示，CLOSE、LISTEN、SYN-SENT、SYN-RCVD和ESTABLISHED这5个状态构成了TCP连接两端的状态全集，一旦连接建立完成，一切顺利的情况下两端状态最终都处于ESTABLISHED。客户端和服务端双方动作不一样，服务端有监听端口和接收连接建立请求的动作，所以状态也有所不同，客户端具有的状态是CLOSE、SYN-SENT和ESTABLISHED，服务端是CLOSE、LISTEN、SYN-RCVD和ESTABLISHED。        从服务端开始，创建了TCP连接，绑定到某个网卡接口（IP）和端口，状态从CLOSE变为LISTEN，代表该监听连接正常工作，可以用来接收其他Endpoint的`建连请求。        客户端开启“三次握手”的第一步，发出SYN报文，客户端连接状态由CLOSE变为SYN-SENT。服务端监听连接收到SYN报文，复制并初始化一个连接，该连接状态为SYN-RCVD，该连接和客户端连接是对应的，然后发出SYNACK报文。        客户端处于SYN-SENT状态，收到SYNACK报文后，更新本地连接信息，同时将连接状态变更为ESTABLISHED，发出针对SYNACK的确认报文，即ACK报文。服务端收到ACK报文后，状态从SYN-RCVD变为ESTABLISHED，两端TCP连接建立完成。 传输数据        两端连接建立完成后，就进入数据传输阶段，该阶段的执行过程与RDT协议类似，采用发送与确认的方式来确保数据可靠传输。以两台主机之间echo协议为例，TCP协议传输过程如下图所示：        如上图所示，主机A向主机B发送字符c，按照echo协议，主机B会回复相同的字符给主机A。主机A发送的报文序号为42，而确认ack是79，这代表当前报文的字节序号是42，而已经收到了78个字节，接下来期望从第79个字节收。主机B回复主机A，该报文不仅有对42号的确认，也就是确认ack为43，代表已经收到42个字节的数据，同时序号是79，也是主机A所期望的。        用户在主机A上输入了字符c，主机B返回了字符c，最后主机A针对主机B的回复做了确认，表示已经成功收到了79个字节。        报文序号并不是根据报文的数量来进行自增的，TCP传输的标的是字节，因此是按照字节序号来定义，这点和RDT有些不同，不过本质没有区别。可以把TCP的工作理解为将一根香肠从一台主机搬到另一台主机，这根香肠无限长，TCP就根据MSS来切，它可以切成N段，然后一段段的传递过去。每一段都有长度，可以使用毫米计数，这样第N段香肠的序号就可以是Length(N - 1) + 1，也就是前N - 1段长度的毫米数再加1，这样序号可以保证自增，同时序号也可以用来作为传输香肠长度的参考，比如：传输的香肠段序号是1234，不用关心它到底是第几段，而是能知道已经有1233毫米的香肠被传送到对端了。        报文发送离不开发送缓冲区，这点与RDT也差不多，过程如下图所示：        如上图所示，应用需要通过网络发送的数据不断的追加到缓冲区中，而将数据发送到网络后，需要有确认才能继续发送，为了解决可靠性和效率这两个矛盾的问题，使用发送窗口swnd来进行调和。TCP超时任务会与发送窗口的baseseq相关联，定时关注swnd中发送较早的数据是否收到响应，而对端传回的响应会推动baseseq向前移动，使得更多的数据能够从缓冲区中发往网络。        发往网络不是同步过程，只是操作系统协议栈将报文最终转换成为数据帧，由网卡驱动将二进制的帧变为电信号，并通过网卡的端口将信号已高低电平的形式“表述”一遍即可，至于连接线材那边的事情，当前主机一概不管。        使用滑动窗口的方式来管理数据发送和ACK响应，目的就是发送报文后不必等待ack响应而是继续发送下一个报文，这样就可以充分跑满网络，有效提升利用率。因此TCP协议也有类似RDT协议的rwnd属性，用来告诉发送方，自己还能收多少数据，也就是你还能不看ack无脑的发多少数据。与RDT协议类似，TCP依靠序号解决发送的顺序问题，依靠ACK解决接收可靠性问题，依靠缓冲区解决发送和消费的效率问题，再通过滑动窗口解决发送和响应能够异步高效处理的问题。        因为有了滑动窗口的存在，TCP响应除了完成ACK响应的工作还需要支持rwnd大小带回（给发送方的）工作。这两个响应分开发送是没有问题的，但是TCP协议能同时传输ack和rwnd，所以会合并到一起。如果频繁的发送响应，会导致网络效率变低，所以TCP会有一定的积蓄效应，就是将响应累积一下再发，比如：两个响应报文，ack分别为100和300，第一个响应报文一创建就发送不如等几秒，第二个响应报文创建后，直接发ack为300的一个报文更高效。这种累积效应不仅在接收端生效，发送端也是一样，需要发送的数据放入发送缓冲区后，尽可能将报文接近MSS后再发送，充分的利用网络。        上述策略是专门的算法，叫Nagle，但对于时间敏感型应用就无法接受了，所以会通过配置SO_TCPNODELAY属性来禁止它，也就是告知协议栈，对于当前连接，当数据进入发送缓冲区后，立刻发送。这和JVM的GC策略很像，面向吞吐还是响应优先，二者是有矛盾的，需要具体情况具体分析。        报文抵达对端，协议栈收到数据，检查完整性，并将多个连续报文中的数据连接起来，还原出来的数据会复制到应用进程相应的内存地址中，再触发中断告知应用进程可以读取数据。当应用进程消费相关数据后，协议栈就会找合适的时间发送响应，响应包含了ack和rwnd。 断开连接        TCP连接建立完成后就可以进行数据传输，当通信双方目标已经达成，就可以选择断开连接，参与通信的两端都可以发起断开连接的操作。        假设服务端发起断开连接，这需要使用到TCP报文控制位中的FIN，表示连接完结，两端交互的流程如下图：        如上图所示，通过“四次挥手”两端完成TCP连接的拆除，两端各自发出了FIN报文，同时也对远端的FIN报文做出了ACK响应。由于TCP连接只存在于本地，所以TCP连接在发起断开后不会立刻删除，如果服务端发出FIN报文，客户端没有响应，服务端会进行重发，这样最大限度的让双方对于连接断开能够达成共识。        上述过程都是由操作系统协议栈负责的，对于应用进程而言是透明的，假设通信双方其中一方应用进程崩溃，上述断开连接的动作还是可以由协议栈程序来完成的，但如果是系统掉电这种突发情况，对端就不会认为连接已经断开，只能经历若干次重传无果后强制断开。        和建连一样，断开连接也需要进行状态控制，断连状态的变迁如下图所示：        如上图所示，由客户端发起断开连接操作，此时客户端TCP连接的状态是ESTABLISHED，客户端进程调用close方法准备断开连接。客户端的FIN报文发送后，客户端TCP连接状态变为FIN_WAIT_1，此时如果客户端进程再调用socket的写方法将会报错。        按照断开连接的契约，服务端会回复客户端的FIN报文，也就是发送ACK报文，客户端收到服务端发来的ACK报文后，状态变更为FIN_WAIT_2，该状态就开始关注服务端何时发出FIN报文了，其实就是等待对端调用close方法来关闭连接。        服务端的FIN报文到达客户端后，客户端会针对该FIN报文做ACK回复，同时状态变为TIME_WAIT，由于收到了服务端的FIN报文，所以理论上没有数据再会由该连接到达客户端，客户端会等待一段时间，将连接状态变为CLOSE，随后拆除。        以客户端视角看完后，接着以服务端视角来看看。服务端收到客户端发来的FIN报文后，连接状态变为CLOSE_WAIT，服务端回复ACK后，向客户端发送的数据已经完毕，就调用close方法，向客户端发出FIN报文。FIN报文发出后，服务端连接状态变为LAST_ACK，当客户端的ACK报文抵达服务端后，由于客户端之前已经不会再传输数据过来，所以直接将连接状态变为CLOSE，随之拆除当前连接，回收其缓冲区等分配的资源。        从TCP连接的建立和断开来看，除了传递请求和响应数据，就单独为了维护两端连接状态就需要7次往复，除去两端缓存资源创建的开销不论，对于系统之间存在频繁的远程通信场景而言，选择短连接通信是非常不明智的。 流量控制        TCP为连接两端提供了流量控制机制，实现的方式是基于rwnd。和RDT协议类似，在TCP报文中存在rwnd属性，它用来告诉对端自己的缓存还剩多少，如果可以发过来的数据尽量不要超过它。        网络传输的双方，只要一方的接收缓存快满了，原因可能是应用程序处理的比较慢，也可能是系统负载非常高，这样协议栈回复给对端报文中的rwnd值就小，这就可以压制对端发送数据的速率，从而间接的控制了流量。 拥塞控制        TCP工作在分组交换网络上，通信的两端各自在本地虚拟了一个连接，但传输的数据要真实的穿过路径中的若干节点。每个节点都像一个消息处理器，接收外部分组，根据IP进行分组路由，这种接收、存储和转发的工作在每个分组交换节点上时刻进行着。        如果设备使用TCP快速的向网络中发送数据，虽然目的是单纯的，地址也是正确的，但由于分组数据包太多，导致网络中某些节点超载，就会使得通过超载节点的所有数据包都发生延迟，影响的设备就不止一个了。这就要求TCP传输时，需要关注链路中不同节点的工作状态，不能太快，也不能太慢的发送数据，但这对于参与通信的两端来说，要求太高了。既然无法观测路径中的节点状态，那就退而求其次，通过观察网络，也就是对端回复数据包的情况，猜测当前网络的状态，根据间接观察到的结果，来决定发送数据的速率。        TCP观测哪些结果呢？一般有两个，即超时和3次连续相同冗余确认。对于超时而言，如果发送的报文在一段时间内没有被确认，这就代表路径不太畅通。连续收到3次冗余确认是指，发送方发送报文后，对端回复的多个报文中ack号相同，这就代表发送的若干报文中存在丢失，而只有出现缺失报文时，对端才会回复需要从某个序号开始的报文。从拥塞程度上看，超时被认为是严重的拥塞，而3次连续相同冗余确认被认为是较轻的拥塞。 3次的原因是置信度较高且经济。        随着对端的响应到达，本地就能观测出拥塞情况，TCP协议通过引入拥塞窗口（Congestion Window，简称为cwnd）来干预发送速率。一般来说cwnd会从1开始，逐步增大，观测到拥塞后，再减小，一旦发现恢复后，再次增大，这种不断挑战网络传输底线的方式就构成了拥塞控制的解决方案。因此，发送窗口就是由接收窗口和拥塞窗口来决定的，如下图所示：        如上图所示，对于cwnd的增加或减少，可以影响到swnd，也就是影响网络的传输效率，而TCP的目的就是在保证网络（或者说大家）可用的情况下，尽可能快的传输数据，提升网络利用率。因此，会有很多拥塞控制算法来优化这个过程，但基本思路就是在没有触发拥塞的情况下，逐步增加向网络中发送的数据量，如果一旦观察到拥塞发生，就降低发送速率。        cwnd常见的变化过程如下图所示：        如上图所示，在初期cwnd会开启慢启动阶段，虽然叫慢启动，但是cwnd的扩张其实非常快，因为它是指数级别的，只是它从0或者1开始的。如果发现超时，cwnd会跌到1，基本处于跌停状态，然后通过慢启动恢复到原来最高点的1/2，随后线性增长，而保守的线性增长阶段称为拥塞避免阶段。如果发现3次连续冗余ACK，代表出现了较轻拥塞，cwnd会跌到当前的1/2，而不是跌到1，随后开启拥塞避免阶段。        可以看到对于超时，TCP会使用慢启动配合拥塞避免的方式来逐步恢复流量，而对于轻度拥塞，会直接使用拥塞避免来处理。这样复杂的处理策略目的不是为了限制速率，而是为了提升传输效率，保持两端尽可能的接近拥塞发生的临界点，在网络能够承载的前提下，尽快的完成传输。不计其数的分组交换设备组成的互联网，在TCP的支持下，仿佛一张巨大的网，在有节奏的上下律动着。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-04a.html":{"url":"book/computer-network-04a.html","title":"热风-感悟《计算机网络：自顶向下》（04a.网络层）","keywords":"","body":"感悟《计算机网络：自顶向下》（4a.网络层）        开发者平日能够触及到的网络最底层也就是传输层了，因为网络编程界面，也就是socket，就属于针对传输层的抽象，目的就是为了完成数据从A到B的传输，无非传输方式是采用可靠的TCP还是不可靠的UDP，只需要在使用socket时做好选择就可以了。其实从网络层看来，TCP协议或者UDP协议，都只是建立在IP协议上的一种策略应用，但由于两个协议格式完全不同，所以二者是有排他性的。        如果用现在的观点去看TCP/IP协议，在传输层这一块，设计的很一般。TCP协议具备面向连接、可靠传输、流量控制和拥塞控制等特性，而UDP则是IP协议上兑现了进程到进程的薄薄一层抽象，其实应该有简洁一致的传输层协议，将进程传输、可靠、流控、拥塞控制和安全等特性以插件的形式组装到协议本身，而传输层协议就会变成一个体系，而非面向功能支离破碎的产物。就像谷歌在HTTP/3.0中又在玩UDP一样，面向互联网的UDP要做到可用，流控和拥塞控制又要重复建设一遍，有点蠢。        经过传输层，就到了网络层，它是解决主机到主机传输问题的，需要将报文（Datagram）以分组（Packet）的形式，从一台主机传输到另一台主机，而两台主机并不一定在一个子网中，这就需要考虑组网的形式以及传输的方式。网络层从功能层面分为两个部分，即数据平面和控制平面，听着和叫好不叫座的ServiceMesh很像，其实它们分别对应转发和路由，从先后顺序来说，应该是ServiceMesh抄网络层的概念。        转发，指的是分组从一个端口进入，然后从另一个端口输出。转发属于网络层的局部功能，在任意分组交换设备上，一般会同时具有多个端口，每个端口会连接对应的主机（或分组交换设备）。连接着分组交换设备的主机顺着端口发送分组，分组交换设备将分组收下，根据分组的目标地址进行路由计算，得到目标端口，然后将分组转发到目标端口上。        路由，是决定分组传输路径的全局功能。两个主机之间进行分组传输，分组需要通过多个分组交换设备，这就要求每个分组交换设备虽然可能是不同厂商生产的，但是由于它们之间由物理链路直接或间接连接，那么就一定可以使用一致的路由算法，使得分组能够顺利通过若干节点，抵达目标主机。        数据从路由器某个端口入，某个端口出，路由器实现数据平面和控制平面的方式有两种：第一种是目标地址+路由表的传统模式，第二种是基于分组中多个字端过滤+流表的软件定义网络SDN模式。        如上图所示，传统路由器的数据平面和控制平面紧耦合，数据转发依赖控制平面提供的路由算法，而路由器之间通过特定的协议进行子网信息交换，各自独立的维护着路由表，路由表会在分组路由计算时发挥作用，而整个过程就像大洋上航行的轮船，相互之间只能看到最近的几艘船，船队的整体方向难以掌握。        虽然这种模式去除了中心依赖，但是想要对网络进行宏观维护就变得困难，因为任意节点的调整引起的涟漪是不可预知的，尤其是在网络规模越来越大的前提下。        如上图所示，SDN模式是一种集中式的网络控制模式。它要求在各个路由器上部署agent，路由策略上传到分组交换机上，由分组交换机完成路由计算，并将计算结果下发到各个路由器上，当然每个路由器上的agent都会时刻将状态信息汇报给分组交换机，这样它在计算时就能做到有的放矢。这么一看SDN有点像分布式服务中的服务发现呢，其实除了名字换一下，大致流程都差不多。        无论是数据平面的转发，还是控制平面的路由，在处理过程中需要依凭的信息就是网络层的IP地址，IP地址是任何分组交换设备（也包括主机）的状态信息，可以简单的认为在分组交换网络中，控制平面中路由器的路由表以及分组交换设备的IP地址是各自设备上的状态信息，任何转发和路由，都要基于状态信息。路由表信息中的路由规则也包括了IP地址信息，所以我们先来看看IP地址。 其实任何分组交换设备，包括主机，都同时具有路由表和IP地址信息。 IP地址        IP地址目的是标识一个分组交换设备，无论它是一台主机还是一个路由器，如果给定了一个标识，就能定位到它。假设从定位资源的角度出发，IP地址可以使用一个数字类型的值来表示，比如：2333表示的是某一台位于杭州的主机，可是IP地址设计的目的没有这么单纯，它需要完成以下多个工作： 标识设备，IP能够定位唯一的设备，报文的来源和目标都需要基于这个标识，也就是IP； 关联承载网，IP是软件概念，或者说它是构建在不同承载网上的抽象，依靠IP能够完成不同承载网之间的互联互通，这是IP地址前期最重要的使命； 实现路由，在分组交换网络中，任何分组交换节点对于分组均采用存储与转发的方式进行处理，而转发（或者说路由）的前提就是基于IP。        关联承载网是能够通过IP协议设计以及承载网自身实现的努力适配做到，一定程度对IP地址的细节没有过多要求，比如：ARP协议，它属于数据链路层（或者说以太网这个承载网上），提供将IP地址转换成为MAC地址的服务，其中MAC地址是属于以太网的概念，IP并不知情。        如果IP地址用整型，标识设备这个工作也没有多少问题，操作系统可以支持指定或者自动获取的方式来设置网卡设备的IP地址，实现设备的标识。路由就没有这么简单了，如果IP地址是一个整型数字，实际上它就是，那么两台主机间存在的路由器势必要记住所有的IP地址，比如：IP地址为1到2000，向端口1转发，IP地址为2001到100000的，向端口2转发。可以想象，如果使用IP离散的标识设备，对于主机间的路由设备而言，将是灾难的，因为它们无法存储这么多地址与端口的对应关系。        地址与端口的对应关系就是路由表，路由表的尺寸必需缩小，能想到的方式就是将需要路由的条件和地址分离，通过缩小路由条件的规模来缩小路由表，这就需要赋给IP地址一些特殊的意义，也就是子网。可以将一个IP地址分为两部分，第一部分是子网，第二部分用来描述子网中的主机号，这样放在一起能够定位到主机，分开进行路由时，就只用关心子网。        因此，一个IP地址需要表达两个含义：第一，子网编号；第二，主机号。        如果对于一个数字，需要能够同时表述两个含义，可以通过规定特定的位数来分别表示二者，比如：前N位表示子网，后M位表示主机。这样一个分组需要发往目标IP地址，就可以通过N位子网信息来确定该从哪个端口输出，而N位子网数据是相对有限的，并且子网就是一组高位相同且无需路由器介入，在物理上相互可达的网络。        IP地址（的V4版本）选择使用32位的整型来描述，如果是十进制表示，也就是最大能够描述40亿左右的设备，可是一旦加上子网的概念，数量就会急剧下降，毕竟某些子网中的主机数不会全部使用。如果要在数字上进行按位赋予含义，二进制相比十进制是有些许优势的，比如：指定前8位为子网，后24位为主机号，但是一个子网内是不可能存在1600多万个主机，也就是由24位二进制描述的主机的，所以IP地址就需要进行一些规划，目前常见的两种规划方式：地址分类和无类域间路由（CIDR: Classless Inter Domain Routing）。 地址分类        IP地址分类的思路非常直截了当，它由三部分组成，第一部分是类别，它是固定的，第二部分是根据类别约定的子网，剩下的第三部分就是主机了。地址分类下的子网与主机数如下所示：        可以看到地址分为4类，分别是：A，B，C和D。A类地址是最高位为0，对于A类地址描述子网的是接下来的7位，也就是A类地址能描述126个子网，每一个子网可以包含1600万个主机，比如：用点分十进制描述的1.1.1.1，这个IP就属于Cloudflare维护的DNS服务器。A类地址规模太大了，B类地址就收敛一些，对于最高位是10的，那么接下来14位用来描述子网，剩下的16位描述主机，也就是允许规划16382个子网，每个子网可以最多容纳6万多个主机。        B类地址看起来友善许多，很多大公司或者机构都开始申请，比如：128.255.255.1，这个IP就属于美国爱荷华大学，如果再仔细看一下，其实128.255这个子网下，6万多个IP都分配给了爱荷华大学，也就是128.255.3.3这个IP也属于爱荷华大学。1万6千多个B类子网，也是胡乱分配完成了。 早期在进行IP分配时，往往以分类子网的形式，将一个子网分配给机构。比如：IBM就拥有A类地址，但是IBM怎么可能有1600万台主机呢？        更多的组织或者企业其实也就是几十上百台主机，它们要求自己的主机在一个子网中，这就要求需要IP地址倾斜更多的位数到子网，因此C类地址就出现了。它以110开头，随后的21位描述子网，最大可以支持200多万个子网，每个子网可以有253个主机。比如：192.192.192.192就属于中国台湾省屏东县的大仁科技大学。        D类地址是用于多播的，也就是以1110开头，后面的28位均用来描述子网，而A，B和C类地址是单播。根据IP地址的前4位可以判断IP类型，通过IP类型再使用对应的逻辑可以找到子网与主机。 私有地址        在介绍无类域间路由之前，先谈一下私有地址。随着支持IP的设备越来越多，适配IP（或者说基于socket）能够提供面向网络的软件生态快速发展，企业或组织有自建网络的需求，同时这个网络可以不连接到互联网，只用于企业或组织内部。 通过使用NAT，能够支持内部网络（或者说私有地址）与互联网的互访。        IP设备成为了国际标准，在不同国家的市场上，兼容IP的设备和软件不断涌现，这就产生了一个问题，IP地址不够了。由于之前IP地址分配的随意性，导致新的企业和组织无法获得有效的IP地址，连自己组网的需求都无法完成。这时负责网络的标准化组织想到一个主意，从现有IP地址池中拿出一些还没有分配出去的地址，让这些地址能够给到不同的企业或者组织使用，只需要让这些企业或组织保证在自己的子网中IP地址不冲突即可，而不同的企业或组织可以拥有相同的IP地址。        私有地址从A，B，C类地址中各拿出了一部分，如下所示： A类：10.0.0.0 ～ 10.255.255.255，1个子网 B类：172.16.0.0 ～ 172.31.255.255，16个子网 C类：192.168.0.0 ～ 192.168.255.255，256个子网        通过在ipshudi.com上查询10.10.10.10这个IP，可以看到如下内容。        如上图所示，可以看到10开头的A类地址是私有IP地址，如果使用过云服务，可以看到内网IP一般都是10开头，这表示云服务厂商为了支持很大的节点规模，从而选择10这个网段。        如果在类UNIX系统中，可以使用whois命令来进行查询。        查询10.10.10.10这个IP，可以看到该IP属于Private Use，代表它是一个私有地址。 全局地址        IP地址在设计之初就是面向全局的，每个支持分组交换的设备都拥有一个IP地址，毕竟有上亿的IP地址，应该是足够的。随着接入互联网的设备越来越多，相同的IP会导致冲突，数据无法进行路由，因此IP地址需要进行管理，管理它的组织就是ICANN（Internet Coporation for Assigned Names and Numbers），在中国这个组织就是CNNIC。        当需要接入互联网的组织或企业进行互联网接入时，就需要向所在地组织申请IP地址。 无类域间路由        一个IP地址由子网和主机号组成，在一个子网内，多个主机能够相互通信，同时不同子网的IP地址也能够通过路由器的分组转发做到互联互通，一方面子网提供了部分主机的管理诉求，另一方面按照子网进行分组路由达成了数据互通的目标，这就是IP网络最开始的朴素愿景。        最开始IP地址分配按照类来进行的，比如：类似IBM这种企业，轻易的就拿到了A类地址，这也就是说它的一次申请，拿走了1600万个IP。通过这个例子可以看出，技术没有问题，但是治理上却糟透了。由于大量的企业不能申请A类地址，转而疯抢B类地址，因为C类地址的主机数不够用。        这样看来按照分类去规划IP地址的方案不能继续了，需要一个方案能够在一个A类地址基础上，将其切成更小的块，简单说就是子网变大。比如：对于11开头的A类IP地址，可以将前2个字节都算作子网，这样IP地址的利用率就提高了，因为11开头的IP地址可以被分拆最多256个子网。        那问题就变成了如何确定子网有多大，这就是无类域间路由（Classless Inter-Domain Routing）的工作了，它通过使用子网掩码来解决这个问题。        对于10.10.10.10这个IP，10是子网，而10.10.10是主机，但通过设置子网掩码为255.255.255.0，也就是高24位为1，可以将这个IP的子网设置成10.10.10，而主机号是10，这样一来，子网就多了。 特殊地址        还有一类IP地址很特殊，它们被保留下来，比如常见的127.0.0.1，这种以127开头的地址称为回环地址。        如上图所示，来自传输层的报文，经过网络层后，如果发现目标地址是回环地址，就会将数据反向发回去，回环地址的存在极大的方便了网络程序本地的开发调试。 获取IP地址        现在的计算机都离不开网络，而且还是基于IP的分组交换网络，这就需要操作系统启动后在子网中拥有独立的IP地址。有两种方式获取IP地址，一种是手工设置，另一种是通过DHCP（Dynamic Host Configuration Protocol）。        先看一下手工设置，在操作系统中一般可以配置IP地址和子网掩码，这个IP地址是使用者自己指定的，如果子网中已经存在想要设置的IP地址，某些系统会给出提示。如果系统启动后需要访问互联网，手工设置还需要配置网关和DNS服务器，前者一般是路由器IP地址，为了提供向外部网络路由的入口，后者提供域名解析服务，往往也是路由器IP地址，因为在路由器上会运行DNS服务端。        接下来是DHCP，它提供了一种从DHCP服务器获取IP的能力，主机不需要指定IP，而是由一个服务节点来进行分配，这个服务节点保存和管理了当前子网中所有已经分配的IP。当系统启动时，系统通过DHCP客户端向子网进行广播，请求IP地址分配，因为客户端此时并不知道服务端的具体位置。DHCP服务端收到客户端发来的广播请求后，会以单播的形式提供给客户端Offer，比如说：你可以使用192.168.31.111这个IP，子网掩码设置为255.255.255.0等。DHCP客户端收到Offer再进行申请操作，分配过程才算完成。        一般来说DHCP服务端在子网中往往部署在路由器上，运行端口是67，协议采用UDP，可以通过nc命令进行查看。        如上图所示，使用nc -uz尝试连接目标IP的UDP端口，可以看到能够成功连接到路由器的67号端口，而输出文本中的bootp就是DHCP协议的前身。 IP协议        IP地址与路由器（或者任意分组交换设备）中的路由表，是网络层状态的体现，前者用来描述具体的分组交换设备，后者用来指引对分组的存储转发。这里提到的分组，就是Packet，它是面向字节的，虽然看起来都是0和1，但是它是符合一定规范的，这个规范就是IP协议。        IP协议定义了在分组交换网络中传输数据的格式，它设计的较为紧凑，在展示IP协议格式时，还是先想清楚设计它的目的是什么？分组交换设备之间进行点对点的数据传输，这里的点对点不是相同子网内的两点，而是跨子网的两点。如果要设计分组交换设备之间的传输协议，首先需要指定发送方的IP地址，然后是接收方的IP地址，不可避免的还需要包括针对传输数据长度以及当前分组起始位置的描述，最后还要包括传输数据的种类以及当前分组的序号，这样看来至少要有6个属性。理解了想象中的IP协议，那就看看现实中IP（v4版本）协议的格式，如下图所示：        如上图所示，可以看到除了可选字段，有12个属性，比我们预想的要多一些，主要多了哪些呢？ 名称 比特长度 描述 版本号 4 描述IP协议的版本 服务类型 8 分组的优先级 Flag 3 分片的标志位 分片偏移量 13 如果IP分片，当前分组处于的位置，目的是将数据能够在对端拼起来 TTL 8 存活时间，一般是个正数，每次经过一个分组交换设备时就-1，如果到0就丢弃该分组 头部校验和 16 针对IP协议头部的校验和        可以看到多出的6个属性，除了版本号以及TTL，其他4个没多少价值。以服务类型为例，分组交换设备本来处理分组的方式就是存储与转发，在设备内维护了队列，如果你指定了某个分组的处理优先级，难道还要求后面链路中的所有分组交换设备都按照你的要求处理？使用优先队列的方式？太复杂了，加重了转发设备的负载和空间开销。Flag和分片偏移量用来描述当前分组是否进行了分片，以及所处的位置，二者按位合并到一个属性中即可，或者放到可选字段中，因为基于TCP传输的应用压根不会分片，16个比特完全被浪费了。头部校验和更没有存在的意义，因为TTL一直在变化，让链路上的分组交换设备一直比对，再设置新的TTL，重新计算校验和完全是浪费资源。        版本号对于协议向前兼容是有价值的，TTL为了防止网络中出现环，定义分组的生命周期是很有必要的。所以这么看来IP（v4版本）协议设计的也就这样，一般般。        4比特的IP头部长度和16比特的总长度，可以计算出IP分组携带的数据长度，也就是用后者减去前者。头部长度的单位不是字节，而是4个字节，是不是有点意外？这点与TCP协议的头长度单位一样，有点草台班子的感觉，不过也不用在意这些细节。 一个IP分组理论上最大载荷在65K字节左右。        协议号长度是一个字节，比如TCP协议的值就是6，UDP是17，支持的种类是挺多的。IP协议本质上也是变长头和变长体，通过给出头大小以及总体积来实现。 ID号与分片        IP协议中包含了ID属性，这个ID属性与TCP协议中的sequence差不多，代表着当前分组的编号，它可以是不断自增的。由于IP协议的目的是完成主机之间的数据传输，如果使用IP协议传输超过IP协议长度（或者说超过链路层MTU）上限，就需要进行分片，否则就需要使用IP协议的用户自己切分数据，那就太麻烦了。        对于跑在IP协议上的TCP，它需要保证自己一个报文不能被切成两份，因为对于TCP而言，TCP报文头太重要了，所以IP分片对于TCP来说没有什么用。TCP的实现会调用IP获得当前（以及对端）所能支持的MTU，并小心翼翼的将报文尺寸控制在MTU之内，避免出现分片。        对于UDP而言，传输的数据尺寸就没那么多讲究，一堆byte交过来，就要进行传输，IP协议就会在超出限制时，选择分片。多个分片之间的ID是相同的，是否分片由Flag属性描述，同时当前分片所在位置由分片偏移量来指定。 数据Padding        头部长度单位是4个字节，所以IP协议的头部需要凑个整，如果一个IP分组的头部包括一些可选字段，那么就需要在可选字段基础上，做好对齐。        IP分组头部的字节数是4的整数倍，如果不足，就补齐。 查看一个IP包        使用wireshark随意抓一个IP分组，可以看看IP协议对应的属性是什么。抓到的IP分组如下图所示：        如上图所示，展开IP部分，按照序号从上到下看一下。 版本，可以看到当前IP分组的版本是4； 头部长度，值是5，也就是20个字节，常规IP头部都是20个字节，没有啥可选字段； 总长度，1493字节，小于以太网的MTU； 分片属性，不分片； TTL，初始值一般是64，也就是允许有64个HOP，能跳64下； 协议号，值是6，代表内部装的是TCP报文； 发送方和接收方IP，两个端点的IP地址。        从IP报文的头部校验和属性描述可以看到，压根就不会对这个属性做校验，是忽略的，该属性没什么用。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-04b.html":{"url":"book/computer-network-04b.html","title":"热风-感悟《计算机网络：自顶向下》（04b.网络层）","keywords":"","body":"感悟《计算机网络：自顶向下》（04b.网络层）        网络层负责分组的路由和转发，而核心设备就是路由器，它是分组交换网络的核心设备。现在家庭接入互联网，大部分采用的是ADSL或者FTTH，可能FTTH越来越多了，在安装完ISP（电信或者移动等）提供的Modem后，一般就需要选择购买一款路由器了。现在的家用路由器一般都具有Wi-Fi无线功能，它长得样子可能类似这样，如下图所示。        如上图所示，这是小米的AX9000路由器，价格不便宜，性能不错，它有环绕机身的4根天线，外观看起来很科幻，底部有五个RJ45的接口，其中4个并作一排，另一个蓝色的单独在一边。路由器的4个接口用来连接家用有线设备，比如：台式机或者NAS存储，蓝色的RJ45接口用来连接光猫（其实也是路由器），光猫的样子一般如下图所示：        如上图所示，一个光猫一般会有许多不同的接口，除了DC电源接口，比较特殊的就是光纤接口（或者电话线的RJ11接口），图中蓝黄色的线就是光纤，而它连接的就是光纤接口，剩下的RJ45接口就是Modem提供的接口。光猫一般也是一个路由器，所以通过RJ45接口连接的设备也会被分配IP地址，无论它连着主机或另一个路由器，现实中光猫的RJ45接口一般就只连接到家中自费购买的路由器上。        这样看来，从家里任意一台连接到路由器的设备访问外网，至少会经历两跳（HOP），第一跳是本地主机到路由器，第二跳是路由器到光猫，然后光猫最终从光纤发射信号。我们可以通过traceroute命令来看一下分组的路由情况，执行过程如下图所示：        如上图所示，traceroute -m 3 www.taobao.com命令可以对分组路由执行跟踪，通过设置IP分组中TTL为3来尝试在一个端点跟踪向外的3次分组跳跃，并将跳跃的路径输出。在本示例中，尝试对www.taobao.com进行跟踪，第一跳是本机到路由器，第二跳是路由器到光猫（192.168.1.1），第三跳是住宅的光猫设备到杭州市余杭区城域网的网络设备（183.128.118.129），当然，如果taobao没有垮掉，经历若干次分组跳跃，分组最终会抵达taobao.com的www服务器。        可以看到从家中任意的一次对外访问，都要做一次无谓的跳跃，如果光猫能够让ISP放开，家庭能够在市场上自由选择购买喜爱的光猫产品，这样每家每户除了省掉这多余的一跳以外，还能节省不少线材和能耗。另外在现有拓扑关系中，就算你家的路由器再强，但如果光猫性能孱弱，也是不利于发挥性能的，同时也要注意光猫和路由器之间的连接线材，至少是5E类级别及其以上的千兆网线，短板往往存在于不经意间。 基本构成        在主机发起网络请求之前，操作系统协议栈程序会根据目标IP计算路由策略，通过目标IP找到网关（Gateway）进行路由，本机的网卡（Netif）会与对应的网关相连，在类UNIX系统中，可以通过netstat命令查看本机的路由信息。        如上图所示，使用netstat -rn，查看当前系统的路由表，可以看到Destination目标一栏中，default默认网关的IP是192.168.31.1，这正是路由器的IP。在访问外网时，比如：访问taobao.com的主机，其IP在本机的路由表目标一栏中找不到，则会选择将分组路由到默认网关。        分组中目标IP地址依旧是taobao.com的主机IP地址，但是包含分组的以太网帧，其目标MAC地址是默认网关也就是192.168.31.1的MAC地址。在子网中，根据IP地址查询MAC地址的服务叫做ARP服务，这里的细节在后续介绍到数据链路层时在详细展开。 关于ARP服务，可以通过《JavaNetwork's ARP与pcap4j》进一步了解。        路由器包含两个模块，包转发模块和端口模块，如下图所示：        路由器的端口模块支持多种通信接口，比如：ADSL、FTTH以及以太网等。有些接口可以连接到外部互联网，比如：ADSL、FTTH、Cable或者以太网的RJ45，有些接口则提供给子网设备进行接入，比如：提供Wi-Fi的WLAN或者以太网的RJ45。路由器的端口模块有连接到上游网络设备的接口，也有连接内部子网设备的接口，从角色上看，路由器本质就是一个完成上传下达的中转节点。        路由器内部有路由表，路由表包含了若干的路由规则，路由规则记录了目标地址，与之匹配网关（路由器或者主机，其实就是一个分组交换设备）的IP地址，以及路由器通过哪个接口与之（网关）相连。路由匹配的状态数据，也就是路由表有了，路由端口的设备也通过接口联通了，接下来就可以介绍路由器的运行机制。 运行机制        主机节点、交换机、路由器和光猫通过网线连接起来，这里提到了交换机，它是工作在数据链路层的设备，主要用来连接子网中所有的网络设备，一般来说路由器也会作为一台主机设备连接到交换机上，只是路由器会同时有一根线连接到了子网外部的设备，比如：光猫。现在的路由器基本都具备交换机的功能，所以它们是可以放到一起的，但是在介绍路由器运行机制时，还是需要将它们分开，至于交换机，将会在数据链路层进行介绍，在这里只用知道它是用来完成子网中任意两个设备之间进行通信的。        从主机节点发起一个对外部网络的请求，比如：打开浏览器访问www.taobao.com，这些设备连接的拓扑以及在拓扑上的请求顺序如下图所示：        如上图所示，根据编号顺序，从访问方向，参与设备以及详细执行过程几个方面进行说明，其中涉及到交换机的部分会做简要介绍。 （1）主机节点出        涉及设备：主机节点、交换机和路由器。        （1.1）主机节点根据域名www.taobao.com发起DNS解析，在操作系统启动时，会获取到DNS服务端的IP地址，一般来说它会部署在路由器上。主机节点的DNS解析通过交换机向路由器发起请求，主机节点获得www.taobao.com对应的IP地址；        （1.2）获得的IP地址结合本机的路由表进行路由计算，得到目标网关是路由器，再根据路由器的IP进行ARP查询获取路由器IP的MAC地址。一般来说交换机的端口没有分配MAC地址，是以连接到接口上的分组交换设备MAC地址为主的；        （1.3）构造好以太网帧，其中目标MAC地址是路由器的MAC地址，而以太网帧中包含IP分组中的目标IP地址是先前经过DNS解析出来的IP地址，IP分组中包含的TCP报文目标端口是80，而报文内容是HTTP请求体。 （2）交换机出        涉及设备：交换机和路由器。        （2.1）交换机根据以太网帧中的目标MAC地址查得路由器连接的端口，然后将以太网帧转发到对应端口，完成数据传输；        （2.2）路由器收到以太网帧，进行目标MAC地址核对，如果不是发给它的，则丢弃。拆开IP分组，根据目标IP地址进行路由计算，得知需要发送给光猫（路由器）。 （3）路由器出        涉及设备：路由器和光猫。        （3.1）路由器将以太网帧的头部，包括来源和目标MAC地址丢弃，将内容装到新的帧中，来源MAC地址是路由器的MAC地址，而目标MAC地址是光猫，至于怎么知道光猫的MAC地址，还是需要依靠ARP查询；        （3.2）路由器将以太网帧发送给光猫。 （4）光猫出        涉及设备：光猫。        （4.1）光猫收到以太网帧，进行目标MAC地址匹配，然后再依据其中IP分组中目标IP地址进行路由计算，得到需要使用光纤接口进行发送数据；        （4.2）光猫将比特数据进行调制编码，转换为光信号，发送之；        （4.3）光猫收到光信号，进行解调，转换为比特数据，然后再将比特数据转换为分组，IP分组中的目标IP是子网中的主机节点IP，经过路由计算，需要将分组发送给路由器。 （5）路由器出        涉及设备：光猫、路由器和主机节点。        （5.1）光猫将分组发送给路由器，路由器将以太网帧进行目标比对，通过后再获取分组中的目标IP；        （5.2）根据目标IP进行ARP转换，通过交换机广播后，路由器得知主机节点的MAC地址；        （5.3）使用主机节点的MAC地址作为目标MAC地址，重新构建以太网帧，并发送到交换机。 （6）交换机出        涉及设备：交换机和主机节点。        （6.1）交换机根据以太网帧中的目标MAC地址，通过交换机内部的对应表得到该MAC地址的设备处于哪个端口；        （6.2）将以太网帧数据发往对应端口，主机节点收到数据。        可以看到，一次普通的HTTP请求和应答，竟然如此的复杂，上述过程中还有很多细节没有展开，但是分组交换网络这种接收、存储、处理和转发的模式应该能看的很清楚。对于路由器而言，端口模块将请求收进来，转发模块会根据目标IP地址，结合自身的路由表进行计算，得到需要派发的接口，然后再委托端口模块调用驱动，将数据透过对应接口（硬件）发送出去，这些动作其实都是有开销和耗时的，与一次RPC网络调用很相似，因此，减少一个路由节点，长时间看，能效提升还是很大的。        路由器端口模块中的以太网接口，不仅会分配IP地址，还具有MAC地址，这点与交换机不同，交换机的以太网接口没有MAC地址，它是跟着连接到交换机的设备MAC地址一致的。具备MAC地址，就是一个能够被链路层标识的点，这样看来，路由器和主机没有多少区别，只是它的工作更为专注，收到分组后，进行路由转发而已。路由器是IP网络中的重要参与者，交换机只是将进来的以太网帧进行转发。 关于交换机的详细内容，到数据链路层再谈。        路由器在进行路由计算时，会忽略主机号部分，只匹配网络号部分，路由表示例如下图所示：        如上图所示，子网掩码表示匹配目标地址需要比对的比特数量，比如：255.255.255.0，代表需要比对前24个比特，也可以看到路由规则对应的目标地址是192.168.1.0。网关和接口表示需要转发的目标，一般接口常见的是本地网卡localhost0或者本机的无线网卡en0等，它们对应了硬件（或者虚拟硬件），具有调制和解调的能力，能够将比特变为电信号，或者反过来。网关是接口连接的目标，图中e1接口连接的就是一个分组交换设备，IP是192.0.2.1，一般这个设备对应的就是路由器。        路由计算策略一般遵循如下步骤： 先按照子网掩码位数最多的目标地址进行匹配； 减少子网掩码的位数再尝试匹配，不断重复步骤2； 如果目标地址中没有与之匹配的项目，则返回默认规则。 默认路由规则一般对应的是0.0.0.0（或default）的目标地址        举个例子，如果分组的目标IP地址是192.168.1.4，首先匹配255.255.255.255子网掩码对应的目标地址10.10.1.101，结果是不匹配；其次匹配255.255.255.0子网掩码对应的目标地址，192.168.1.0，也就是子网号为192.168.1的目标地址与之匹配，匹配成功，返回对应的路由规则。端口模块随后会使用接口e3将分组发送出去。        如果分组的目标IP地址是192.192.192.192，最终经过路由计算会匹配到默认规则，也就是0.0.0.0，将分组发往192.0.2.1，委托它（路由器）将分组数据送到目标IP为192.192.192.192的分组交换设备。如果路由计算出的路由规则，其网关为空或者是MAC地址，则代表目标IP已经处于当前路由器所管理的子网中，数据到站了，按照目标MAC进行转发即可。        从上述路由器的运作过程可以看出，路由器就相当于一台会进行分组转发的主机，其实本质上它就是一台主机，上面运行着一个Linux，在Linux上面运行着路由器程序而已，这个程序使用的协议栈和普通主机使用的协议栈没有多少区别，功能都是类似的，只是这个程序能够驱动多个端口以及操纵属于它的硬件设备，比如：Wi-Fi天线等，它的执行过程如下图所示：        如上图所示，从以太网接口收到帧，判断是发给自己（也就是路由器）的后，丢弃掉MAC头部，然后从IP分组中拿出目标IP，依据本地路由表进行计算，匹配到合适的路由规则，委托端口模块将数据转发到与路由规则中接口相连的网关上。        端口模块接收到IP分组以及路由计算得出的路由规则，就开始转发工作，该工作执行过程如下图所示：        如上图所示，选择路由规则中的接口，将IP分组中的TTL作修改（一般来说是减1），再根据接口MTU进行IP分片，然后再将分片后的IP分组按照接口类型（或者说数据链路层媒介）进行数据转发。这里按照MTU进行分片的原因是IP分组毕竟要跑在具体的承载网技术上，比如：以太网，如果接口是RJ45，也就是以太网，那就要求待转发的IP分组载荷不超过以太网帧的最大要求。分片后的IP分组就可以按照接口类型进行数据转发了，如果是以太网，那就加上以太网的头部以及结束帧，按照以太网帧的形式进行调制，将调制后的电信号从接口发送出去。 这个过程本质上是将IP分组装进以太网帧的数据部分，然后委托以太网进行传输，因为IP协议没有传输数据的能力。        回过头来看一下路由表，前面提到在一个分组交换设备中，保存有两个状态，一个是IP地址，另一个就是路由表。互联网中存在大量的路由器，路由器之间独立维护路由表，在一个路由器内部，路由表的维护也是同分组转发操作是相互独立的。相互连接的路由器要感知到彼此的存在，才能完成分组路由工作，路由表的维护工作可以由人工或者路由协议机制，比如：RIP、OSPF或者BGP协议，它们是如何让路由器达成共识，接下来会进行介绍。        路由器还具备路由规则的聚合与拆分功能，比如：路由器中有三个规则，分别涉及到三个子网，10.10.1.0/24，10.10.2.0/24和10.10.3.0/24，这三个规则的网关如果一样，那么就可以进行聚合，即聚合成10.10.0.0/16一条路由规则，目的就是为了减少路由规则，节约资源，提升判断的效率。拆分是针对一个子网中的单个主机，如果子网中存在多个主机，那么可以按照子网掩码255.255.255.255的形式增加一条针对具体主机的路由规则（多台主机会有多条），它的网关可以使用这台主机的MAC地址，这样在子网内部主机互访时，就变得高效许多。 达成共识        Internet上存在大量的路由器，各个路由器下有规模不一的子网，通过聚合后，有大约几十万个子网。相邻两点的路由器需要进行路由信息（或者路由表）交换，不相邻的则不必交换，因为只有通过交换路由信息，一台路由器才能对自己所处的位置有明确的认识，有了认识后才可以实现路由计算。        RIP协议用来实现路由器之间的路由信息交换，该协议的实现一般基于UDP协议，端口在520，用来完成数据传输。交换的内容是当前路由器的路由表，路由器运行时会广播请求，收到请求的路由器会单播回复该路由器自己的路由表，而发起广播的路由器会将收到的路由表做好处理并加入到自己的路由表中。这样如果当前路由器收到一个分组，其目标IP地址属于另一个路由器所管理的，那么通过路由计算，因为本地路由表已经更新，这就可以得到路由规则，从而完成路由转发工作。 一般每隔30秒会进行一次路由信息交换。        如果按照RIP的运作方式，最终互联网上的任意路由器节点都会具备全量的路由信息。路由器拥有全量的路由信息这显然不现实，因此将网络（子网）按照自治区的方式进行划分，路由信息的交换只发生在自治区的部分路由器上。可以分为自治区之间的协议和自治区内的协议，而自治区之间的路由交换协议就是BGP，边界网关协议。        BGP协议分为两种：eBGP和iBGP。eBGP，即externalBGP，目的是收集内部子网可达信息，通过eBGP告知其他自治区。iBGP，即internalBGP，任务是通知自治区内部收到的可达信息。        如上图所示，eBGP收集内部子网信息，然后做好聚集，将信息给到另一个自治区的边界网关，表示如果有分组要发给这些子网，可以由我来路由。收到信息的边界网关，通过iBGP将信息分发给自治区内的路由器，表示如果有分组要发给这些子网，可以来交给我路由。这样两个边界网关就完成了路由信息交换，自治区之间更大规模的子网信息就能达成共识。        互联网中一个个路由器如同夜晚大洋中的一座座灯塔，相邻的彼此感知到对方的存在，而传输的分组如同一艘艘小船，它们向最近的灯塔诉说自己的目的地，而身边的灯塔总会告诉它该怎样去往下一个灯塔，纵使小船需要跨过整个海洋，借助灯塔们的接力，它也能轻松做到。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-05a.html":{"url":"book/computer-network-05a.html","title":"热风-感悟《计算机网络：自顶向下》（05a.数据链路层）","keywords":"","body":"感悟《计算机网络：自顶向下》（05a.数据链路层）        数据链路层位于TCP/IP五层结构自顶向下的第四层，它在最底层的物理层与网络层之间。开发者在进行网络编程时，很少与它打交道，能让大家知晓的无非就是MAC地址了。这样看来数据链路层的角色没有那么显眼，但事实上相比传输层和网络层，后者只是一层软件抽象，而数据链路层才是构成现实网络的基础，网络设备相互交流的原语。        数据链路层之上的应用层、传输层和网络层所具备的服务，数据链路层都能给予支持。比如：子网中的两台主机进行点对点通信，或者子网中的一个节点对其他设备进行广播，这些不同的通信形式，数据链路层都需要支持。事实上数据链路层不仅限于通信，还提供了更多服务，包括：成帧（变为数据链路层的通信原语），完成相邻两点的数据传递，流控，错误检测，差错纠正以及半双工和全双工通信。        数据链路层负责通过通信媒介实现设备之间的互联互通，这里的通信媒介就是指物理形式的存在，比如：双绞线、同轴电缆、光纤以及无线电波等。如果说把这些介质中传输的光电信号与二进制的0和1进行转换是物理层的工作，那数据链路层就是将需要发送或接收的比特数据转换为固定格式的（比特）集合，这个集合称为帧。帧是网络传输中的最小单位，因此我们说数据链路层上传输的实体（帧）才是网络交互的原语。 该集合就好像字节一样，开发人员不会去操纵位（bit）那种过低级别的类型，而是关注这种类似字节一样有含义的集合。        数据链路层的帧就如同网络层的分组一样，从一个端点通过介质传送到另一个端点，如果不去探寻其中细节，在有网络层IP技术的基础上，为什么还需要数据链路层呢？原因是TCP/IP是后期产物，在它出现前，不同的数据链路层技术面对各自使用的介质，都会规划和设计相应的传输形式以及基于这些形式的编解码策略。以太网和光纤，Wi-Fi和电话线，这些不同的传输介质在用于网络通信时会配套各自的驱动以确保主机系统能够使用这些介质，如果有软件跑在以太网上，它势必就会使用以太网这个数据链路层技术的SDK，导致这个软件就被绑定在了以太网上，并且很大的概率不同以太网厂商的驱动还相互不兼容。        早期的网络程序都有这样的问题，因此在不同的数据链路层技术上构建一套中立的通信传输标准就显得非常有必要。接下来就回到我们熟悉的故事，基于依赖转置原则，不同的数据链路层技术适配对接TCP/IP规范，而应用程序直接基于后者开发，就能做到软件的自由迁移。回顾整个发展史，虽然网络层和数据链路层都是负责将数据从一个端点传输到另一个端点，但它们解决的问题和作用是不一样的，因此不冲突。 MAC地址        和IP协议一样，如果要支持设备间的通信，首先就需要标识它们，在IP协议中用IP地址，在数据链路层中则用MAC地址。由于MAC地址需要面向物理层工作，所以它和IP地址的获取动作不一样，MAC地址被烧录在网卡设备的ROM中，当操作系统启动时，可以通过设备驱动加载到它。 IP地址的获取方式，可以参考笔者所写网络层的文章。        如果MAC地址是相对固定的，不像IP地址那样飘来飘去，那就需要它尽可能的不重复，这样在一个子网中就不会同时出现多个具有相同MAC地址的设备了。标识设备最简单的方式就是使用自增的数字，如果要标识设备的数量可能很大，那就可以采用8字节的长整型，一定管够，这玩意描述人类已知宇宙中的天体都够用了，只需要有一个机构做好数字的维护就行。        简单的事情一定要做复杂，要不然体现不出智慧，本着这个理念，一定不能用一维数字，要用多个位组合成的复杂标识。既然关注设备，那就将设备厂商编号纳入其中，未来说不定还可以向这些企业收费，然后在跟上一个厂商自己的识别码，这样重复与否的皮球又踢回去了，不仅如此，标识看起来就气派多了，MAC地址按位划分如下图所示：        如上图所示，第一位为0表示该地址为单播地址，否则为多播地址，第二位为0表示为全局地址，否则是本地地址，后面两段分别是代表厂商的标识以及厂商内部的识别码，这样就能保证没有相同的MAC地址。MAC地址的长度是48位，如果采用类似IP点分十进制的形式来描述，那就是6段数字，比如：128.128.128.128.128.128.128，不过这样感觉太占位置了，如果使用16进制，每个字符描述4位，换成f0:18:98:1f:91:e4的形式，感觉就帅气一些了。目前我们都是使用后者来描述MAC地址。        在类UNIX系统的控制台，输入ifconfig命令，可以查看本机的网卡配置，如下图所示：        如上图所示，输出在控制台上的内容，形式是以设备名外加描述组成，例如：其中lo0表示本地网卡，也就是localhost0的简写，而en0则是本机物理网卡的配置信息。en0网卡的ether（以太）一栏的内容为：f0:18:98:1f:91:e4，这个就是en0网卡的MAC地址。lo0为什么没有MAC地址呢？因为它是本地网卡，其IP是本地回环地址，所以对它的访问，协议栈会将其传回给传输层，所以它不需要数据链路层的支持，因而没有MAC地址，当然这也是网络分层的好处之一，适应性是杠杠的。        访问https://itool.co/mac，可以查询MAC地址，如下图所示：        如果仔细观察这个MAC地址，对比MAC地址的格式说明，会发现有些问题，MAC地址的位没有按照规范来呀。f0:18:98:1f:91:e4的查询结果显示它是一个全局唯一的地址，也就是说MAC地址的第1和2位需要是0，但理解十六进制的我们，看到f0就知道它的二进制形式是：11110000，也就是前两位是1，这不是矛盾了吗？其实没有，因为MAC地址采用了大端描述，因此实际内容应该是0f，符合规范，可以参考下图：        上图来源：https://www.cnblogs.com/lsgxeva/p/13932262.html。MAC地址的描述形式这么不讲究，不怕有人闹吗？答案是不怕，因为关注的人不多。其实不论正反，它就是要表示一个标识，类似一个整数或者UUID，所以无所谓的。 协议        使用MAC地址可以描述网络设备后，接下来和IP协议一样，设计一款用来进行通信的约定规范，也就是需要进行数据链路层通信协议的概念设计。通信协议常见的元素包括：双方地址、协议长度（定长或者是变长）、类型或者业务标识、数据载荷以及数据校验位，当然其中的类型以及业务标识要结合协议的用途来看。数据链路层的通信协议，主要用来完成两个MAC地址之间的数据传输，以及数据传输过去后需要用哪种逻辑进行处理，因此最简单的数据链路层通信协议可以设计成以下形式，如图所示：        如上图所示，数据链路层通信协议的协议头部包含了目标和来源MAC地址、类型、数据校验位和载荷数据长度，这是一种定长头的通信协议，简单且高效。事实上仔细分析这个协议，它和UDP协议非常类似，只保有了通信所需的最基本要求，而且它与IEEE 802.3规范中的以太网协议已经非常像了。        上述设计的数据链路层协议其实不是我们日常用的协议，甚至IEEE 802.3规范中定义的以太网协议也不是，DIX协议才是现在的以太网协议，DIX分别指代DEC、Intel和Xerox三家公司，其实主要是Xerox公司的功劳。因为以太网协议只是高效的完成数据传输，没有过多的参与到上层活动中，DIX由于其简单和低成本所以成为了事实标准。对于这种事实非标准的情况，很常见，就好像J2EE和Spring的关系一样，前者最后就成了吉祥物。        至于以太网协议，我们在后面的章节做介绍，因为它是数据链路层中的一种实现技术。说到这里，一会数据链路层，一会以太网，到底它们是什么关系？以太网是数据链路层的一种实现技术，就好像同轴电缆、电话线或者光纤技术一样。任何互联互通都离不开介质，而无论是双绞线还是无线电波，这些介质都可以用来通信，而附加在这些介质上的软件含义称为数据链路层，客观存在的物理介质称为物理层。早期计算机网络实际都在围绕着介质建设不同的数据链路层和物理层，以太网技术包含了软件层面的定义、驱动、协议以及RJ45接口、双绞线、集线器和交换机，因此它是一种联通两层的技术实现，或者说具体的数据链路层技术都会有一个物理层介质与之对应，而本章就在介绍它们在软件层面的共性。        现在应该能分清楚数据链路层和以太网之间的区别了。数据链路层定义了描述设备的方式，即MAC地址，围绕MAC地址会提供适合具体物理层的数据链路层通信协议。虽然不同的数据链路层技术采用的通信协议有所不同，但是它们都需要围绕着MAC地址来设计，离不开通信协议所需的必要元素，比如：来源和目标MAC地址，以及数据负载和标识负载类型的定义，至于具体的数据链路层协议，本质上都是在最小通信协议基础上增加一些其介质特有的属性而已。 成帧        形成帧的工作主要有两类，一类是从协议栈程序委派给数据链路层的数据发送工作，它要求将数据负载按照当前数据链路层实现的协议进行编码成帧，比如：以太网；另一类是从物理层接收到信号，信号经过物理层转为0和1的比特，然后由数据链路层接收比特流并解码和校验，最终形成当前数据链路层能够识别的帧。        应用程序可以通过使用socket接口向网络中写入数据，通过前面章节的介绍，我们知道，socket接口并没有独立向网络写数据的能力，该工作需要socket接口和操作系统协议栈程序配合完成。操作系统协议栈程序属于操作系统内核，socket接口将需要写入网络的数据放入内存中，然后将需求和数据地址传递给操作系统协议栈，由后者完成数据发送。操作系统协议栈不熟悉具体网卡硬件，还需要依靠网卡的驱动程序以及硬件网卡才可以实现数据的发送，使用以太网为例，如下图所示：        如上图所示，它按照流程描述了应用程序如何驱动网卡来发送数据的，这比我们想象中调用socket接口就完成数据发送要复杂的多，不过本章对它的介绍主要集中在数据链路层，也就是MAC模块，等未来介绍物理层时，你还会看到这张图。        首先网卡上有很多部件，包括：ROM、缓冲区、MAC、MAU以及RJ45端口等，其实它也可以看作是一个专用计算机。ROM中烧录了当前网卡的MAC地址，缓存区就相当于网卡的内存，外部需要网卡发送的数据会写到缓冲区中，而网卡上MAC和MAU模块会操作缓冲区中的数据，它们一般无法操作当前计算机的内存。MAC和MAU模块是网卡的CPU，而缓冲区就是网卡的内存。MAC模块会访问缓冲区中的数据，将这些比特数据编码成帧，然后按照一定的约束调用MAU模块，MAU模块接收MAC模块的请求，将字节数据转换成电信号，通过在RJ45接口上不同引线上施加电压，实现将数据以电信号的形式进行发送。        MAC模块就是数据链路层的物理存在，而网卡驱动程序会与MAC模块做交互，MAC模块从缓冲区中取出需要发送的数据，依据当前数据链路层的协议进行帧的生成，然后按照一定约束调用MAU模块完成帧的发送。        当对端计算机通过网线传回数据时，MAC模块将比特流数据转换为帧，并将帧给到协议栈，实现数据的接收，使用以太网为例，如下图所示：        如上图所示，它按照流程描述了应用程序如何通过网卡收到数据，该过程与数据发送相比，看起来更加复杂，不过确实如此。从双绞线进入的电信号，它需要进行解调，转成计算机能够识别的字节，同时还需要从网卡内存拷贝到计算机内存，从操作系统内核态进入到应用程序用户态，这一路别提多坎坷了。虽然很复杂，但这次也只是集中注意力在数据链路层，在后续介绍物理层时，你还会看到这张图。        MAC模块从MAU模块获取一定约束的字节数据，这些数据会以字节的形式存放在缓冲区中，因为它们还不是完整的帧。随着读取的字节数据变多，MAC模块读取到了结束帧（FCS）或下一个开始帧，这代表对端传递的帧已经接收完成。虽然操作系统上的不同应用可以并行的收发数据，但是在数据链路层的发送和接收过程中，帧是最小单位，在任意时刻，双绞线上的信号（发送或接收端）只属于某一个帧。MAC模块将缓冲区中蓄积好的帧做校验，通过校验并且目标MAC地址是当前网卡的帧是需要接收的帧，如果不是则直接丢弃。        符合预期的帧放在网卡的缓冲区中，操作系统没有能力从其中获取，因此MAC模块会发起中断，当处理器响应中断后，会按照中断请求的约定调用网卡驱动，而后者具备操作网卡的实际能力。网卡驱动会访问网卡缓冲区中的帧，并将其拷贝到内存，然后通知协议栈进行处理。操作系统协议栈程序只带有TCP/IP等软件层面的定义，对于以太网（或者其他数据链路层）协议格式并不清楚，因此，在网卡驱动中也会包含对当前数据链路层协议的软件定义，并支持将其转换为IP分组。 中断是有编号的，网卡在安装时就在硬件中设置了中断号，在中断处理程序中将硬件中断号与网卡驱动绑定。        因为网卡驱动反向依赖协议栈或者TCP/IP协议，如果帧数据中的类型是IP协议，则会将帧数据转换为IP分组，然后交给协议栈中的TCP/IP模块来进行处理，这样交给协议栈的数据就是标准的分组了。 传输        MAC模块是数据链路层的核心，它通过与MAU模块的交互，将帧数据发送到网络中，同时也将接收到的帧数据交给协议栈。MAU模块会按照要求进行字节到信号（以及相反过程）的处理与发送，如果子网中所有的主机节点都在随意的发送数据，MAU模块就会向网络介质，比如：双绞线、Wi-Fi或者同轴电缆中发送电信号。可以想象这些信号会相互影响，导致接收方MAU模块无法正确的识别，这种情况肯定不是我们想要的。        如果说数据链路层的工作是帧的生成和错误检测，那么看起来操心物理层信号是否相互影响以及在影响发生时的规避策略就应该是MAU的事情，但情况没有这么简单。就像TCP要考虑传输的可靠性，而底层的IP虽然实现了主机到主机的数据传输，不过没有了TCP就显得用途不大一样，数据链路层也要为信号正确的传输努一把力。既然MAC模块能够从宏观上对MAU进行操作，那么偏向电信号传输的策略算法就由MAC模块来控制。        MAC模块进行控制的方式主要有三种：时分TDMA，频分FDMA和码分CDMA，其实它们就是着重在电磁波信号的划分上，是以时间片来做依据，还是以频率做依据。不论以时间还是频率做划分依据，其目的就是让主机发送信号时，能够有一个物理标识的维度。        以时间为例，也就是时分TDMA，它可以定义时间片，这肯定是比秒要小得多的单位。在一个时间片中，子网中只有一台主机能够发送信号，不过这不能绝对避免不同节点会一起发送的情况，但是它能保证每个主机都有一把对时间划分的尺子。当若干节点一起发送信号了，参与发送节点的MAU模块能够感知到接收线路有信号进来，这时就表明当前节点在发送时也有其他节点在同时发送，此刻只需要发送信号的多个节点执行相同策略即可。这个策略就是检测到上述冲突后，当前时间片不再发送信号，本地随机一个时间延迟，也就是在之后的P个时间片再次发送，这个P是一个随机值，不同节点会有不同的值。        通过随机延迟来解决不同节点之间的信号发送冲突问题，这个思路很简单，虽然延迟比较高，但这种先进行检测，然后发送中做好侦听，当发现冲突后进行规避的处理策略却是通用的。以太网常用的CSMA/CD，就是在发送信号前侦听是否有主机占用了信道，如果没有则进行发送的技术。        这种事前侦听，事中检测的发送方式显得很低效，仿佛在子网中加了一把全局锁，如果能够把锁的粒度变小，主机节点之间的相互干扰从物理链路上隔离开，网络的数据吞吐量就会显著增加，接下来介绍几种常见的数据链路层通信技术，看看它们是如何解决这个问题的。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-05b.html":{"url":"book/computer-network-05b.html","title":"热风-《计算机网络》第5章-数据链路层（b.以太网）","keywords":"","body":"《计算机网络》第5章-数据链路层（b.以太网）        如今是移动互联网时代，生活中用手机和Pad上网看视频，工作中用笔记本电脑连Wi-Fi办公，感觉有线网已经很少见到。人们对有线网的记忆仿佛就停留在无线路由器背后那根线一样，已经不那么重要了。        其实无线网络只是终端接入形式的转变，有线网络依旧是互联网的基础，从带宽、延迟和效率来说，有线网络都会远远强于无线网络，我们短视频程序背后所用的服务器，它们都是由有线网络连接起来的，没有这些设备之间的有线连接，是无法向消费者提供稳定可靠的网络服务。        既然有线网络这么重要，那该了解它哪些有用的信息呢？有线网络涉及的技术种类很多，比如：光纤、同轴电缆以及双绞线，在不同的介质上使用着不同的有线网络技术，而它们之间的差别也非常巨大。虽然种类很多，但使用最广泛的就是以太网技术，无论是电竞主机网卡上插的网线，还是数据中心里交换机上的线缆，它们都使用了一样的技术，即以太网技术。        以太网技术是一种具体的网络通信技术，它涵盖了TCP/IP五层结构中最下面两层，也就是数据链路层和物理层，这也是具体网络技术的特点--提供软件驱动和硬件，实现主机间的通信。本文主要从数据链路层上介绍以太网技术，主要是软件层面的协议设计。        在正式开始介绍前，还是需要声明数据链路层的职责：面向物理媒介，完成数据到帧以及帧到数据的转换与传输，其详细内容可以参考这里。        以太网协议设计的比较简单，它包括：前导码（SFD）、以太网头部和帧校验序列（FCS）构成。上述三者的关系如下图所示：        如上图所示，前导码占用了8个字节，以太网头部占用了14个字节，FCS占用了4个字节。以太网协议的特点在于加入了前导码，从字节数据来看，它是有规律的从1到0的位变换，通过连续7个字节，至少28次周期变换，目的就是告知对方电信号变化的频率，对端通过学习前导码中的变换频率，能够掌握以何种频率去度量后续收到的帧。        前导码的字节内容如下图所示。        如上图所示，当前导码最后一个字节，也就是SDF中出现了（紫色所示）连续的两个1，这代表接下来开始接收帧的本体。        从概念上理解一下前导码的作用，好比A唱一首歌给B，唱歌的速度有快慢，为了能让B更好的了解到A有没有开启2倍速唱歌，好的做法是让A先唱一段B知道正常速率的歌，这样B就对A接下来的歌有所准备了。一样的道理，当对端网卡开始接收到信号时，它会假设变换的信号要传递前导码中约定的内容，无论是快速的信号变换，还是慢速的信号变换，通过对已知内容的监听，对端网卡就能确定信号的度量分割窗口，然后使用该窗口去划分接下来帧的本体，直到FCS接收完成。        可以看到前导码为真实网络通信作出的努力，没有它，双方无法协调信号之间的语速，正确通信更是无从谈起了。常见的网络层（及其以上）协议，是见不到前导码这样的设计，原因就是数据链路层协议需要根据自身硬件特点来定义协议，既要考虑软件语义，又要考虑硬件实现，二者缺一不可。        以太网头部主要由三部分构成，它和数据负载以及FCS的格式如下图所示。        如上图所示，以太网协议是一个定长头，变长体的通信协议。这里需要传输的通信内容，也就是数据负载长度在46到1500字节之间。之前常提到的MTU大小，指的就是它，以太网的MTU是1500字节。这里有点奇怪，既然数据负载长度是变化的，那为什么以太网协议没有字段标识当前帧包含的数据负载大小呢？讲道理是应该有专门字段做长度标识才会显得科学些，在IEEE 802.3规范中确实有帧长度的字段来描述数据负载大小，但实际使用的DIX规范中没有这个字段，它是靠类型以及FCS之间是数据负载这一默认约定来实现的。        类型字段是对数据负载类型的描述，比如：0800是IP协议，86DD是IPv6协议，以太网驱动会根据具体的类型值，将数据转交给对应的协议栈进行处理。        数据发送方网卡的MAC模块将数据包装成帧，填充好帧的头部，包括：来源和目标MAC地址，并且确定好处理类型，在帧的前部增加前导码，尾部加上根据数据计算得来的FCS，这样一个帧就组装好了，随后便交给MAU模块进行信号的调制与发送。        数据接收方网卡的MAU模块收到信号，它知道对方一定会按照约定的前导码内容发送信号，因此就跟着接收到的信号进行默念，通过多次默念，对传递过来的信号有了一定的认识，当读到11时，就代表要开始接收正式内容了。信号解调成帧后放在接收方网卡的缓冲区中，其MAC模块会检查FCS，看是否有差错，如果没有再核对一下目标MAC地址是不是自己，如果一切正常，就发起中断，通知操作系统来取货了，否则就丢掉这个帧。        使用wireshark，随意抓取一个分组，可以在Ethernet一栏中依据以太网协议查看数据内容，如下图所示：        如上图所示，序号1和2标识了目标和来源MAC地址，序号3标识了类型，它表明数据负载是IP协议，可以使用IP协议对数据负载进行处理。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/computer-network-06a.html":{"url":"book/computer-network-06a.html","title":"热风-《计算机网络》第6章-物理层（a.基本概念）","keywords":"","body":"《计算机网络》第6章-物理层（a.基本概念）        在计算机网络的世界中，物理层可以说是最底层，它位于TCP/IP五层结构中的最下层，主要负责比特流（bit stream）在物理媒介（如：电缆、光纤和无线电波等）上的传输工作，将比特流从发送方传输到接收方。虽然物理层不涉及任何的数据处理或网络协议，但它却是整个网络通信过程中不可或缺的一部分，因为物理层的通信利用了物理现象，而这些物理现象是可以被外部感知和观测的。如果两个主机节点之间使用双绞线进行通信，剖开双绞线，随着数据在两点之间传输，金属导线上的电压也会出现变化，而这些变化就是物理现象，就算不使用双绞线，改为Wi-Fi进行通信，在某个频段（比如：2.4GHz）上也会有电波的传输。        计算机中的比特数据会通过物理层转化为客观物理现象，比如：光或者电，而它们会透过物理媒介传输到对端网卡，再经过翻译转换为操作系统内存中的比特数据，主机之间的通信就完成了。如果这样看，物理层还挺简单的，主机之间用电线一连就成了。可是你有没有想过，在电线上传输的信号是有含义的，它和家里220V的交流电完全不同，不是连一根线那么简单的。在计算机科学中，比特（Bits）是信息的基本单位，只有两种状态：0和1，而在物理媒介参与的情况下，0和1该如何表示呢？如果能够用某种物理现象表示数据，对端主机该怎样理解呢？不考虑物理现象传输的带宽容量，如何能保证传输的信号不失真呢？这些工作都是物理层负责的，这么看来物理层就没有这么简单了吧。        虽然已经了解到物理层挺复杂，但是还需要问：物理层进行通信要解决哪些问题呢？        计算机网络中的物理层就像联通计算机科学和物理学的桥梁，如果用计算机专业的视角来看整个物理层，一定会觉得很神秘，一时间会摸不着头脑，但如果站在使用者的角度去看，其实物理层需要解决的问题主要有两个，即信号的编（解）码和传输。前者探讨如何将比特转换为可以在物理媒介上传输的信号，后者解决如何定义和标准化物理接口，以及如何在物理媒介上有效的传输信号，这涉及到信号的衰减、噪声和失真等问题。        虽然本文是探讨物理层的基本概念，但是物理层无法脱离现实存在于虚空概念中，所以笔者选择使用双绞线作为物理媒介的以太网为参考，这样让大家更有体感一些。 信号的编（解）码        比特数据由0和1组成，而物理现象，比如：电压或者光波该如何表示0和1呢？1伏特表示1，0伏特表示0吗？显然不是。虽然如何定义和量化这些物理现象取决于规范设计，但我们也希望这些规范简单且易于操作。比特数据到物理现象的过程称为调制，反之，称为解调，接下来使用以太网作为示例，先来看看如何使用电压来定义0和1。 使用电压来描述比特        以一节五号电池为例，它有正极和负极，二者之间的电势差为1.5伏特，当用导线连接正负极时，由于电势差的存在，会产生电流，如果导线中有一个灯泡的话，它应该会亮，这代表电流在做功。同样的道理，主机到主机之间通过导线连接，当一方处于高电势，而两者之间又存在回路，就会产生电流，依靠电流就能实现通信。        电流在导线中的传输速度近似光速，所以能够以忽略不计的时间延迟将电流传递到对端，但对端的主机不能像灯泡一样亮一下就完了，它需要理解电信号并将其还原为比特数据，这就要求在电压上做些文章了。还是以五号电池为例，如果使用两个五号电池，分别将其负极和正极接地，即电势为0，那么分别联通二者的正极和负极，就会产生正负电压，该过程如下图所示：        如上图所示，如果联通左侧的开关，对端主机就会接收到正电压，我们可以将其定义为收到1，如果联通右侧的开关，对端主机就会接收到负电压，也就是收到比特数据0，可以想象，左右两个开关在同一时刻不能全部联通。从A到B的通信也能看出，双绞线不是一根简单的导线，如果只考虑单向通信，至少需要两根导线，且它们之间必须绝缘，而一旦需要两端相互通信，就需要再double，也就是至少四根导线，A的发送需要与B的接收相连，如下图所示：        可以看出，计算机通信时，在物理层，发送和接受完全独立，天生就是全双工的。使用正负电压就可以将比特数据的0和1进行很好的描述，比如：正电压一次，负电压一次，就代表二进制的10，感觉也没有那么困难嘛。细心的同学可能已经看出问题了，按照正负电压的方式，该如何表述连续的1或者0呢？这就引出了一个关键问题：对信号进行度量时间的设定。如果想要上述电压描述比特的方式能够走通，就需要约定发送和接受两端对于电压度量的时间间隔，假设是1秒，也就是每次正负电压给定的单位时间必须是1秒，不多也不少。这样二进制11000就变成了，正电压持续2秒，负电压持续3秒。使用约定时间间隔的方式看似能解决度量问题，不过随着信号传递的延迟以及时间度量的误差，该方法坚持不了多久就一定会出问题。        看来从比特数据到正负电压没有那么容易，不过关键问题已经被挖掘出来了，就是传递的物理信号该如何进行度量，即如何调制信号。 调制比特到信号        如果将比特原本的含义翻译为信号，那只能称之为转换，但如果能够在翻译的过程中加入其他信息，就如同鸡尾酒一般，这才能被称为调制。信号传输时，就需要考虑信号在对端如何度量，对端在测量电压和电流时，必须要判断出每个比特的界限在哪里。这就是之前我们遇到的那个关键问题。        简单做法是在已有数据传输的导线基础上再增加一根导线，用它来发送一组区分比特间隔的信号。这就需要在传输数据时将时间间隔也同步传递过去，不过两条线路一旦不同步，时钟偏移了，数据就全乱了。难道这无解了？不会的，既然在物理层，那就利用一些物理特性，如果说单开一条导线是计算机学科的线性思维，那利用波的叠加就实属物理学的高维理论了。        通过波的叠加，将数据信号和时钟信号叠在一起，这就仿佛在比特数据之水中撒入时间之蜜，生成的信号如同调制出来的蜜水一样，即解渴又解馋。该过程如下图所示：        如上图所示，需要发送的比特数据为1010，时钟信号表示度量的间隔，从负电压到正电压如同嘀嗒的一跳，伴随着周期性的“跳跃”与数据信号进行叠加信号就是调制出来的结果。发送方的时钟是稳定的，调制信号时只需要用这把尺子去度量数据信号，就能比对着得到叠加信号。观察叠加信号，会发现它是不断变化的，在任何一个时间间隔中都有正负电压的变化，比如：比特1，从正电压变为负电压，比特0，从负电压变为正电压，这就代表着始终采用变化的电压或者说电压变化的趋势来描述0或1，而不是简单的采用静止的正负电压。        比特数据的0或1，对应到电压上就成了变换的趋势，不论是0或1，都包含了电压的变化。因此接收方可以通过度量电压变化周期从叠加信号中分离出发送方的时钟信号，再依据时钟周期还原出数据信号，也就是比特数据了。        采用叠加方式进行数据传输的好处有很多，如果需要提升传输数据的带宽，就可以选择在单位时间里向对端发送更多的数据信号，这就会以更密集的时钟信号向对方发送数据，从物理角度看，就是信号频率变得更快。只需要两端设备的工作频率上限能够支持，数据就可以正常传输，从外界来看，如果传输的网速比较慢，设备工作频率就比较低，如果网速处于满载状态，设备就处于高频工作状态，网卡的灯就猛闪。        讲到这里，回忆一下数据链路层-以太网 中介绍以太网协议时提到的8个字节前导码，连续7次重复的byte数据10101010，目的就是让接收端能够有足够的窗口识别出时钟周期，所幸调制一个帧所用的时钟周期是一致的。 解调信号到比特        对端网卡的RJ45接口接收到传递过来的叠加信号，需要根据信号进行一段时间观察，从中找出变化的周期，通过对前导码的分析，能够很快的确定时钟间隔。当接收到前导码最后一个字节，也就是SFD时，一旦出现连续的两个1，就代表之后的信号是正文了，该过程如下图所示：        如上图所示，已知不论是0或者1，正负电压一定会发生变化，只是变化的方向不一样，0是从负电压变为正电压，而1刚好反过来。两次变化的时间间隔就在传递着时钟信号的含义，通过一次度量，就能大致算出时间间隔，也就知道了频率（每秒变化的次数）。连续7个字节，每个字节有8次变换，一个帧能够让接收方进行至少56次以上的观察，也就足够从叠加信号中准确的分离出时钟信号，从而获得数据信号，最终得到比特数据。        在帧的尾部是4个字节的FCS，它是由发送方生成的，目的就是用来检查因噪声导致波形紊乱而产生的数据错误。接收方会通过收到的数据进行计算，然后与FCS对比，判断数据是否有错误，如果发现错误，直接丢弃该帧。如果帧被丢弃了，帧所包含的IP分组也就不存在了，IP分组中的报文更是无从谈起，而数据传输的可靠性，就需要（上层）传输层的TCP协议来保障了。        掌握了信号如何进行调制，解调它也显得容易许多，不过这么复杂的工作，是网卡哪一部分来完成的呢？答案就是MAU。 打碟能手MAU        MAU负责调制数据和解调信号，以数据发送为例，在之前的数据链路层中介绍MAC时，用过一张图：        可以看到MAC将需要发送的帧，此时还是比特数据，交给MAU模块，而后者将比特数据调制成叠加信号，随着有规律的连接和断开RJ45接口上的开关，就如同发报机一样，将信号发送到物理媒介（双绞线）中。        如果以发送比特数据1为例，它需要传输的信号是在一个周期内从正电压变为负电压，这就需要MAU模块先接通正电压导线的开关，然后断开的同时再接通负电压导线的开关，变换过程如下图所示：        如上图所示，在一个周期中，两个开关均会发生连接和断开，只是面对传输的数据是0或者1时，操作开关的顺序有所不同而已。可以把这两个开关想象成DJ所用的碟，刚好一边一个碟，MAU在调制数据进行发送的过程就如同DJ在打碟一样。打碟的频率越快，曲调变化就越激烈，这与MAU发送数据也类似，频率和网络传输带宽成正比，不过频率越高，损耗越快，就好比Wi-Fi信号一样，2.4GHz速度慢，可是覆盖面广，5GHz速度快，但穿墙能力弱。        MAC将帧交给MAU进行发送，MAU会将比特数据与时钟信号进行调制，产生的叠加信号会如同DJ打碟一样操作发送线路正负电压的开关，高速的“搓碟”，将信号发送到网络。 信号的传输        MAU模块生成叠加信号，按照约定向网卡的RJ45接口施加电压，电信号会沿着插在接口上的双绞线到达对端的RJ45接口，并被对端的MAU模块感知和度量，最终电信号解调为内存中的比特数据。信号传输离不开接口与物理媒介，以太网常见的接口是RJ45，物理媒介为双绞线。 传输接口（RJ45）        RJ45，其中RJ表示已注册的插口，45表示序列，该接口是一个8针的接口，主要用来连接以太网。RJ45接口的8针会连接到双绞线中的不同导线，其中1、2、3和6号线负责信号的发送和接受，如下图所示：        如上图所示，主机节点Node上插了一块网卡，网卡的MAU模块按照正负电压发送信号，其中1和2号线用来发送本机正负电信号，3和6号线负责接收对端的正负电信号。如果A和B两台主机装备了相同的网卡，那相连之后，如果A的发送线路连接到B的发送线路，A和B对着吼，信号不就无法收到了吗？的确如此，所以要求A的发送线路与B的接收线路相连，同样，A的接收线路与B的发送线路相连。RJ45接口是规范，因此很难做更改，那就选择接口与双绞线的接法上作调整。        一根双绞线有两个水晶头，只需要其中一个选择交叉反接，就可以实现两台以太网设备之间的正确连接，如下图所示：        如上图所示，直接一一对应的接法称为MDI直连，而进行交叉的接法称为MDI-X交叉，左侧集线器的RJ45接口插入的就是采用MDI-X交叉接法的水晶头。两端设备的接口不变，只是在水晶头与双绞线的接法上作调整即可。        RJ45接口在MAU模块的指挥下，向1和2号线路发送正负电信号，同时接口将3和6号线路接收到的正负电信号传递给MAU模块进行解调。两端RJ45的连接离不开双绞线，以及双绞线水晶头的交叉接法，虽然使用双绞线传输电信号从电气特性上看与普通导线传输家用电相似，但实际上要比后者讲究和复杂的多。 传输介质（双绞线）        使用导线传输信号时，信号无外乎是方波或正弦波，这些在导线中传递的波，本质是一种能量，它会随着传输而衰减，导线越长，衰减越严重。如果信号（或者波）的频率越高，虽然带宽载荷更大，但是更容易损失能量，信号变化剧烈的部分，其拐角就会变缓，再加上噪声的影响，信号再进一步失真。        信号一旦失真，对端解调时就有概率出现对0和1的误判，这就是产生通信错误的原因。MAU将信号转换为比特数据，随后将比特数据交给MAC模块来完成帧的生成，失真信号解调出来的帧大概率无法通过对FCS的检查，出错的帧会被丢弃，虽然TCP层面可以通过重传来保证数据不丢，但重传会引发TCP的拥塞控制，表象上看就是网速变慢。        可以看到，如何避免或者减少噪声，是传输介质需要考虑的，对于噪声少的传输介质，它能够更好的服务网络上层，使数据流转变得更加通畅。双绞线包含了8根线，两根线为一组缠绕在一起，目的就是为了抑制噪声。        金属导线进行信号传输时，噪声产生的原因是导线周围的电磁波，当电磁波接触到金属导体时，会产生电流，一旦此时导线中有信号传输而传输方向与产生的电流相反，就会导致信号产生失真。影响导线的电磁波分为两种，一种是外部设备泄漏出来的，另一种是内源性噪声。        先看一下外部泄漏的电磁波，当外部电磁波与网线作用，会产生感应电流，如果我们学过高中物理，那这个物理现象应该耳熟能详，即导线切割磁感线，会产生感应电流。双绞线会将两根线进行缠绕，如果只看一根线，它在磁场下的分布是螺旋状的，如下图所示：        如上图所示，螺旋的线，各段会产生相反的感应电流，这就使得噪声被抵消了。螺旋的间隔要求在13mm以内，材质好的双绞线螺旋更均匀，间隔更小，线材使用更多，噪声会更小，从而性能更好。        接下来是内源性噪声，电流通过导线，也会产生磁场，虽然磁场很小，但是它也会形成影响其他导线的噪声。内源性噪声也被称为串扰，要抑制这种噪声，关键在于双绞线的缠绕方式，4对导线的间隔要保有一定差异，用来抵消产生的噪声。        双绞线应对外部电磁波和内源性噪声，采用了两根线缠绕为一组，组与组再缠绕的方式来抑制噪声，可以看到相比导线传输家用电确实复杂很多。线材制作工艺的提升也能支撑更高频率的信号进行传输，更高的频率意味着更高的带宽。双绞线一般使用类别（Category）来进行区分，示例图中的双绞线就是超五类线，一般常见的类别如下表： 类别 带宽 频率 CAT-5，五类 100Mbit/s，百兆 125Mhz CAT-5e，超五类 1000Mbit/s，千兆 125Mhz CAT-6，六类 10000Mbit/s，万兆 250Mhz CAT-7，七类 10000Mbit/s，万兆 600Mhz        虽然双绞线有不同的类别，但是它是支持向下兼容的，如果网卡是百兆的，使用五类线就够了，你要使用六类线也没有问题。虽然向下兼容很好，但反过来就不行了，比如：千兆网卡使用五类线，那就亏了，最高只能跑到百兆网速，所以我们在布网时不仅要关注网口速率，也要注意网线的类别，缺一不可。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/talk-about-architecture.html":{"url":"book/talk-about-architecture.html","title":"热风-聊一聊架构","keywords":"","body":"聊一聊架构        架构师一直是技术行业里炙手可热的职位，是无数开发者心中的目标。大家都希望成为架构师，架构工作给人的感觉是经验和技术的制高点，仿佛做架构就可以摆脱搬砖的命运，但事实真是如此吗？        每位技术人的职业路径都是相似的，从小工到专家，在入行初期一定会遇到团队中厉害的角色，往往就是架构师。有些同学认为架构就是一堆中间件的集合，能够用web、rpc、config以及message等框架攒一套微服务系统就是架构师，如果采用的技术越新潮，吼出来的性能指标越逆天，就代表架构师的水平越高。有些同学则认为架构就是为了服务业务，虽然业务是什么也不一定讲的明白，但是一定要有好的态度，尤其是要和产品运营搞好关系，当需求来了，能不顾死活的通过倒排和996按时把项目垒上线。虽然看似两个极端，但是这两类同学是普遍存在的，前者鄙视后者不懂技术，是业务的舔狗，后者批评前者不懂业务，是技术的囚徒。通过在不同团队或公司的历练，我们所遇到的架构师也有所不同，有些只看技术，有些则飘在空中，同样的职位，都是负责架构工作，那为什么有这么大的区别呢？想要理解区别，还是要先要搞清楚什么是架构。        架构到底是什么？有很多书籍和开放组织对架构做了深度探讨，但在充满现实主义氛围的今天，这些书籍和开放组织对于架构的定义并没有什么用，要么过时了，要么太技术框架化了，那架构就是一个虚无缥缈的东西吗？        既然每个人理解的架构都不一样，就代表在每个人心中架构解决问题的范畴是不一样的，这样看来架构就不是一个简单的一元概念，它应该是多元的，在笔者看来至少包含了三个方面：现在的情况、未来的变化以及社会性共识。这么一说，就显得架构更玄幻了，别慌，接下来笔者就从什么是架构、架构的目标以及如何做好架构来聊一聊，或许能够给你带来一个不一样的视角。 什么是架构？        架构的英文是architecture，以前读书时翻译资料，如果将它翻译为架构，就会被老板纠正为：体系结构，并被啰嗦一顿，结构代表软件系统是由元素以及元素间的关系组成，关系能够演绎系统如何处理外界的输入，同时体系代表系统整体风格是如何响应外部（需求）变化，架构这个词太单薄了，体系结构更为合适。        老板说的很对，不过架构这个词业界已经叫顺口了，很难去改，但是从体系结构的内涵可以看出架构包含了很多方面。 当前的情况        架构是当前情况的反应，它是由结构和行为组成的。结构就是组成系统的元素，它们可以是数据结构（或关系数据库表结构）以及接口，表示系统是如何定义和理解问题的。行为是系统向用户（或者外部系统）的承诺，它定义了输入和输出，在转换过程中，系统提供了价值。        结构和行为反应了系统现状，它们能够让开发者了解到系统的核心数据结构（或者领域模型）以及功能，更好的情况下，开发者通过深入了解结构和行为从而识别出系统的最大潜力和主要问题，这会为系统的演进带来有价值的指导。        对于现有系统的结构和行为有深入理解属于架构的范畴，架构不总是用来做新东西，对已有系统的认识和理解是架构最优先要解决的。 未来的变化        架构除了需要理解当前的情况，还需要拥有超越当下，应对未来变化的能力。架构基于现有系统的情况，识别出哪些是易变的，哪些是稳定的，以及哪些数据结构是最重要的。数据结构是否足够的抽象，其中抽象的部分是否能够有利于消除变化。        架构应该能够定义出系统在未来业务发展过程中可能出现变化的主要方向，并围绕这些方向建立起应对变化的原则，而任何在系统上的变更都应该符合这些原则。        建立系统应对未来变化的原则属于架构的范畴，这些原则在概念上包括但不限于：系统拆分、数据结构抽象、功能解耦、性能优化以及新技术选型与引入，在行动上体现为：完成业务需求的同时，能够按照计划逐步进行系统优化迭代。 社会性共识        架构不只是当前的软件系统和面向未来的原则，还包括在架构以及系统上工作的人。一个开发团队由Leader、资深开发、普通开发以及新人组成，他们就是一个社会性的组织，需要对架构达成共识。        如果新人进入团队，没有文档用于了解系统的主要结构和业务知识，只能从代码中逆向得到启发，这不是不能做，只是效率非常低下，而且对新人非常不友好。对于一个需求如何实现，普通开发和资深开发对于系统的改动点有非常大的出入，这不是观点的百花齐放，而是架构工作没有深入人心。        建设对系统理解的共识属于架构的范畴，架构不仅用来治理系统，也会用来治理维护系统的人，虽然维护系统的人由于角色的不同对于架构的理解有深浅，但是组织中的人对任何需求的理解以及设计输出都是具有一定包含性的，不是对立的。 架构的目标        架构由现在、未来和组织多元所构成，它的目标是减少变化带来的影响和成本，简单的说，就是系统能够以低成本的变动来满足需求。        如果是躺平的架构，不重视技术演进的架构，一定不能以低代价来满足变化，如果侥幸做到，这种守株待兔的成功也不会长久。        为了达到架构的目标，就需要架构时刻处于进化之中，只有不断向前演进和完善的架构，才是活的架构，也只有活的架构才能以低成本的变动高效的实现需求，甚至超预期的完成任务。 如何做好架构        架构的目标是减少变化带来的影响，而架构本身是当前的情况、未来的变化与社会性共识的体现，既然架构这么复杂，那该如何做好架构呢？有太多的著作来向你兜售如何做好架构，至于有没有效不得而知，但是就像架构的概念一样，它涉及了多个方面，很难一以贯之。因此，做好架构也没有一套包治百病的万精油打法，笔者认为做好架构需要注意以下几个方面。 识别与完善实体        架构的灵魂就是领域实体，在工作过程中需要时刻梳理、修改和完善领域实体。举个例子：假设构建一个商品系统，一开始可能只有一张简单的商品表，但是随着商品发布、审核以及编辑等操作出现，就需要从中提炼出商品草稿，商品操作记录等商品领域实体，其中一个商品有多个草稿和操作记录。如果后续有对商品系统提出更复杂的运营操作需求时，就可以基于商品操作记录来完善打磨。反之，如果不去在实体上下功夫，只是简单的在商品表中新增几个字段，那么随着几轮需求实现，商品系统架构就会慢慢劣化，最终只能依靠一个个看不下去的补丁活着。        对于领域实体的识别和完善需要时刻进行，就算需求没有产生，也需要不断的精化领域实体，去除多余的属性，比如：使用业务垂直表来降低核心数据结构的复杂度等。        沉淀下来的领域实体需要文档化，也会形成PPT向外部团队进行宣讲，这样对内能够统一思想，对外可以提升沟通效率。文档的沉淀不是一次性工作，要随着需求时刻进行。 采用好的模式        好的模式不只是在编码中引入设计模式来提升程序的扩展性，还包括运用一些分布式系统模式。举个例子：定义维护好商品系统的领域实体很重要，但是也需要梳理和抽象所有的商品领域事件，发布商品、修改商品、审核商品以及商品过期等等，使用领域事件以及分布式消息可以将商品核心系统与商品非核心功能实现解耦。比如：商品发布后会重新计算商品分数，如果将计算分数的逻辑放在发布商品代码的末尾，这倒没有什么问题，但是后续如果需要在商品修改后再重新计算分数，就需要改代码了。利用分布式消息以及商品领域事件就可以将商品发布和商品分数计算分离开，提升了商品系统的架构韧性。        除了异步消息，将名词接口转换为名词行为接口也会让架构变得清晰，通过实施接口隔离原则，能够提升内聚性以及单一职责性。举个例子：一个ProductService肯定跟不上ProductReadService、ProductAuditService和ProductUpdateService等名词行为接口的组合。        架构师需要掌握尽可能多的模式，并结合自己业务的特点，将模式运用到架构和系统中，解决问题，提升效率和质量。 注重代码审查        代码审查是建立社会性共识的绝佳场景，好团队都非常在意代码审查，不重视技术的团队只会在意项目的上线时间，不会关注实现过程。        对于Leader而言，如果不去关注实现过程，就无法做到有效的考核。代码审查是一个拉近架构师和一线开发同学的好机会，从如何编写健壮的代码，到如何理解架构，该怎样更好的完成工作，以及未来的架构是什么样的，在未来的某一天能够更好、更容易的完成工作。最重要的是，能够让被review的同学看到自己的工作在架构中的位置，知道自己也在为这座大厦添砖加瓦。        架构师要重视代码审查，如果架构师不写代码，就不能被称为架构师，因为他丧失了面对现实的体感，在代码审查中，除了修代码，还能修人。 持续优化技术债        技术债是架构腐化的表现形式，需要在组织中有效的发动成员识别技术债，管理技术债。举个例子：面对经常出现的线上问题，可能是由于系统交互不稳定导致，也有可能是产品做的不够完善，可以通过在后台系统（BOPS）中增加一个页面，提供给运营同学一个查询和修正问题的功能，而避免每次耗时且危险的数据订正。        有后台功能解放开发人员，组织也要对主动开发这些功能的同学加以奖励，并给予他们一些时间，同时还要跟进这些问题最终从产品层面去解决。        磨刀不误砍柴工，作为架构师和团队Leader要能够顶住压力，将团队工作时间的20%左右，用来持续偿还技术债，只有这样，团队才能越跑越快，而不是被债务拖累。 定期分享机制        团队内部（和外部）要定期进行分享，需要设立系统负责人，负责人不是在名义上挂一两个系统，而是要对系统足够了解，做好系统变更决策的一号负责人。系统负责人对当前系统架构，以及团队负责系统的整体架构有清晰的认识，能够将自己出色的设计与团队内部同学进行分享。分享的内容不仅仅是技术，也包括运行在系统上的业务数据（量级与趋势），系统负责人对于业务数据指标、增量和风险点有清楚的认识，可以选择在周会时对这些数据进行通晒。        一次分享活动，收获最大的不是听众，而是分享者。通过定期分享，不仅主动刺激分享者需要更全面的了解、剖析和解决问题，还锻炼了分享者的表述能力，以及发掘自身不足的机会。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/javac-release-option.html":{"url":"book/javac-release-option.html","title":"热风-使用Javac's Release选项","keywords":"","body":"使用Javac's Release选项        如果在编程语言中选择一个版本帝的话，Java绝对是最有力的竞争者。拜模块化技术所赐，从Java9之后，每隔6个月Java就会发布一个新版本，从底层VM到上层语法特性都会进行特性的更新（以及删除）。在以前的Java8时代，Java开发人员下载一个JDK（Java Development Kit）能用好久，但随着Java版本的快速发布，就需要区分需要哪个版本的JDK：自己学习使用的JDK版本，生产环境上用的JDK版本，以及哪个版本是LTS（Long Term Support）的。为什么这么麻烦呢？原因是低版本的JVM无法运行高版本class文件。        向前兼容是Java的核心特性，Java21的JVM可以运行Java8的class文件，问题是：可以使用JDK21的编译器生成Java8的class文件，并顺利运行在Java8的JVM上吗？讲道理是可以的，也是兑现向前兼容特性的要求之一。如果一切顺利的话，Java开发同学就只用选择最新的JDK版本，然后按需编译生成目标版本的class文件即可，那为什么还有那么多的Java开发同学会在机器上准备那么多不同版本的JDK呢？事实就是Java并不会完全兑现向前兼容，只是说尽可能的做到向前兼容。 在Mac上使用Eclipse Termurin，不同版本JDK切换的知识可以参考这里。        在利用javac编译生成class文件时，可以通过指定source和target两个选项来选择目标class的版本，在JDK8下，尝试以Java7的语法来源作为输入，Java7的目标class作为输出。 % openjdk8 % javac -version javac 1.8.0_422 % javac -source 1.7 -target 1.7 T.java 警告: [options] 未与 -source 1.7 一起设置引导类路径 注: T.java使用了未经检查或不安全的操作。 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。 1 个警告 % java T [A]        其中T.java的内容为： public class T { public static void main(String[] args) { java.util.concurrent.ConcurrentHashMap map = new java.util.concurrent.ConcurrentHashMap(); map.put(\"A\", \"B\"); System.out.println(map.keySet()); } }        利用source和target选项，可以在JDK8下实现编译输出Java7可以运行的class，感觉这看起来很正常。如果我们在maven中，可以通过配置compile插件来做到一样的效果，如下所示： org.apache.maven.plugins maven-compiler-plugin 1.7 1.7        这一切看着都像一回事，甚至通过javap去观察T.class时，也可以看到class的major version值是51，妥妥的Java7，但事实上如果你在Java7的环境中运行T，就会得到一个类找不到的错误，原因就是ConcurrentHashMap的keySet()方法返回的类型是KeySetView，一个Java8新增的类型。        使用javap仔细观察T.class，可以看到如下端倪。 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=2, args_size=1 0: new #2 // class java/util/concurrent/ConcurrentHashMap 3: dup 4: invokespecial #3 // Method java/util/concurrent/ConcurrentHashMap.\"\":()V 7: astore_1 8: aload_1 9: ldc #4 // String A 11: ldc #5 // String B 13: invokevirtual #6 // Method java/util/concurrent/ConcurrentHashMap.put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 16: pop 17: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 20: aload_1 21: invokevirtual #8 // Method java/util/concurrent/ConcurrentHashMap.keySet:()Ljava/util/concurrent/ConcurrentHashMap$KeySetView; 24: invokevirtual #9 // Method java/io/PrintStream.println:(Ljava/lang/Object;)V 27: return LineNumberTable: line 3: 0 line 4: 8 line 5: 17 line 6: 27        第21行指令调用ConcurrentHashMap的keySet()方法，返回类型为Java8新增的KeySetView。一个Java7的class，在Java7的JVM上运行，结果就报错了，世界就是这么一个草台班子。这么看来，如果要杜绝这种问题，只能根据目标Java版本选择对应的JDK了。这的确是一个方案，如果是Mac用户，可以参考这篇文章，除此之外，还有没有更简单的办法呢？        答案是：使用（Java9新增的）release选项可以更好的编译生成class文件。 实验一：使用ConcurrentHashMap的keySet()方法，输出Java7的class        由于是Java9新增的，所以切换到JDK17，该版本是最后几个可以输出Java7的JDK了，如果你使用JDK21，就无法输出major version为51的class了。 % openjdk17 % javac --version javac 17.0.12 % javac -version javac 17.0.12 % javac --release 7 T.java 警告: [options] 源值7已过时, 将在未来所有发行版中删除 警告: [options] 目标值7已过时, 将在未来所有发行版中删除 警告: [options] 要隐藏有关已过时选项的警告, 请使用 -Xlint:-options。 注: T.java使用了未经检查或不安全的操作。 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。 3 个警告 % java T [A]        在通过javap观察一下main(String[] args)方法中的指令。 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=2, args_size=1 0: new #7 // class java/util/concurrent/ConcurrentHashMap 3: dup 4: invokespecial #9 // Method java/util/concurrent/ConcurrentHashMap.\"\":()V 7: astore_1 8: aload_1 9: ldc #10 // String A 11: ldc #12 // String B 13: invokevirtual #14 // Method java/util/concurrent/ConcurrentHashMap.put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 16: pop 17: getstatic #18 // Field java/lang/System.out:Ljava/io/PrintStream; 20: aload_1 21: invokevirtual #24 // Method java/util/concurrent/ConcurrentHashMap.keySet:()Ljava/util/Set; 24: invokevirtual #28 // Method java/io/PrintStream.println:(Ljava/lang/Object;)V 27: return LineNumberTable: line 3: 0 line 4: 8 line 5: 17 line 6: 27        可以看到，第21行指令，调用ConcurrentHashMap的keySet()方法，返回的是java.util.Set接口，这样该class一定可以运行在Java7上。从这个实验可以看出release相比较source和target的组合而言，能够做到更好的向前兼容，事实上release确实修复了不少Java编译器的bug，也是被用作解放source和target的。 实验二：ByteBuffer的flip方法，输出Java8的class        在Java的nio中，ByteBuffer是Buffer的子类，其中Buffer具有flip()方法，在Java8中，它是这样定义的：public final Buffer flip()。这就使得ByteBuffer也继承了flip()方法，调用后会返回自己的超类Buffer。这一切在Java9中有所改变，首先超类Buffer的flip()方法没有了final修饰，子类ByteBuffer扩展了它。        Buffer中的定义：public Buffer flip()，再看ByteBuffer的扩展代码。 public ByteBuffer flip() { super.flip(); return this; }        这样改动的目的是为了ByteBuffer在调用flip()方法后返回ByteBuffer类型，这样外部就不需要再次转型，根本上讲就是早期Buffer类型没有设计好。接下来看一下这段代码： import java.nio.ByteBuffer; public class B { public static void main(String[] args) { ByteBuffer bb = ByteBuffer.allocate(16); bb.flip(); } }        flip()方法被调用，这就需要看B的class文件中对flip()方法的链接是否正确，如果能够在Java8下运行，那就需要使用Buffer的flip()方法，但是如果稍有不慎，喜欢虚方法的Java就会将其链接到ByteBuffer的flip()方法上，这就会导致出现问题。        切换到JDK21，然后使用source和target选项编译生成一个Java8的class，然后再切换回Java8去运行该class，结果如何？ % openjdk21 % javac --version javac 21.0.4 % javac -source 8 -target 8 B.java 警告: [options] 未与 -source 8 一起设置引导类路径 警告: [options] 源值 8 已过时，将在未来发行版中删除 警告: [options] 目标值 8 已过时，将在未来发行版中删除 警告: [options] 要隐藏有关已过时选项的警告, 请使用 -Xlint:-options。 4 个警告 % openjdk8 % java B Exception in thread \"main\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer; at B.main(B.java:7)        可以看到结果是找不到方法flip()，它需要返回ByteBuffer类型的flip()方法，但是Java8中ByteBuffer实际没有该方法，它只有一个返回超类Buffer的flip()方法。虽然使用source和target选项，要求编译生成的class文件能够运行在Java8上，但是JDK编译器还是蠢蠢的将Java9中的改动输出到自己以为能够运行在Java8上的class中。        这就是一个Bug，一个JDK编译器的Bug。是不是Oracle或者社区修复它就好了？估计他们想着如果修复了这个问题，可能会导致问题，所以干脆不要改了，做一个新的，也就是release选项，用它来搞定。        还是切换到JDK21，然后用release选项编译生成一个Java8的class，重新做一下测试看看。 % openjdk21 % javac --version javac 21.0.4 % javac --release 8 B.java 警告: [options] 源值 8 已过时，将在未来发行版中删除 警告: [options] 目标值 8 已过时，将在未来发行版中删除 警告: [options] 要隐藏有关已过时选项的警告, 请使用 -Xlint:-options。 3 个警告 % openjdk8 % java -version openjdk version \"1.8.0_422\" OpenJDK Runtime Environment (Temurin)(build 1.8.0_422-b05) OpenJDK 64-Bit Server VM (Temurin)(build 25.422-b05, mixed mode) % java B        正常执行并返回，再使用javap观察一下class文件。 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=2, args_size=1 0: bipush 16 2: invokestatic #7 // Method java/nio/ByteBuffer.allocate:(I)Ljava/nio/ByteBuffer; 5: astore_1 6: aload_1 7: invokevirtual #13 // Method java/nio/ByteBuffer.flip:()Ljava/nio/Buffer; 10: pop 11: return LineNumberTable: line 6: 0 line 7: 6 line 8: 11        其中第7行指令，调用的方法就是超类Buffer的flip()方法，从字节码层面看，是符合预期的。这样看来在Java9之后，source和target选项就应该成为历史了，使用release选项会更好一些。        在maven中使用release选项也很简单，修改一下配置即可。 org.apache.maven.plugins maven-compiler-plugin 8        JDK17可以使用release选项输出Java7的字节码，而JDK21最低只能输出Java8的字节码，能够看出来随着JDK的继续演进，通过release选项输出的最低字节码版本也在逐渐升高。这种有策略，工业化的语言演进机制，也只有唯一完成模块化改造的主流编程语言Java所具备。不要再固守Java8了，赶快升级吧。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/talk-about-design-paradigm.html":{"url":"book/talk-about-design-paradigm.html","title":"热风-谈一谈设计范式","keywords":"","body":"谈一谈设计范式        “四人组”（Gang of Four，GoF）在1995年出版了《设计模式：可复用面向对象软件的基础》一书，该书首次系统的介绍了23种设计模式，这些模式被认为是面向对象设计的经典，它与罗伯特·马丁的《敏捷软件开发──原则、模式与实践》一起被称为“软工双雄”。设计模式对软件开发的影响是极为重大的，现如今优秀的软件中都可以找到它们的影子，比如：Java开发者常用的Spring框架中，就使用了多种设计模式，使用这些模式的目的是为了框架获得优秀的扩展能力，Spring使用了大量的设计模式，而它也以优秀的扩展性而闻名。        对于Spring框架的开发者而言，Java语言是一种编程模型，而如何使用Java去构建Spring框架就是一种设计范式。回过头来，想一下自己平日所做的工作，以及花费精力所取得的成果，其中有哪些是依赖技术特性的？又有哪些是（有创新性的）使用这些技术的策略和方法？答案应该是后者，这是一种软实力，也就是你在这个领域积累的设计范式。再举个例子，比如Java的模块化技术JPMS，如何通过关键字定义模块是一种编程模型，而如何更好的使用模块去构建系统，以及面对问题该拆分出哪些模块，使得系统获得非模块化系统所不具备的优势，这些就是设计范式。市面上关于编程模型这些形而上的东西很多，而只可意会不可言传的设计范式却很少讨论。        OSGi不是不好，搞不下去的原因就在于其设计范式没有深入人心。EJB不是没有价值，很多开发人员缺少有效使用这项技术的设计能力，没有设计范式支撑的它也注定被丢弃。我们可以通过技术文档和书籍来学习技术，但是该怎么提升技术范式呢？答案是没有现成的知识让你直接掌握技术范式，这个活儿需要你自己来。        在工作和学习中，对于技术的使用过程中，除了使用它解决问题，还要刻意的思考：这么用对吗？有没有更好的使用方式？它还能解决了哪些问题？在构建解决方案的同时，也就是在完善自己的技术范式。        任何技术产品都是以产品（或者运行时）形态来展示其强大的特性，比如：WebLogic Server应用服务器，就算是OpenAI的chatGPT也是一样，这些产品能够带来革命性的价值，而要获得这价值，就需要依赖技术以及掌握这些技术的人，如下图所示：        如上图所示，可以更加体会到设计范式的重要性。任何重要的产品都离不开技术的革命，而它们交付给大众的形式都是产品，就像iPhone一样，其革命性来自于AppStore，在AppStore后面，是数以百万计的开发者。可以看到革命性的产品在通过向用户提供价值的同时，也需要构建一个吸引开发者的生态环境。        在这个生态环境中，有面向编程模型的SDK，也有如何高效使用编程模型的设计范式。在任何技术生态中，设计范式都是至关重要的，它需要足够的简单明了，使得开发者能够实现迁移。当开发者进入这个领域，一些他们所熟悉的设计范式能够与新环境中的范式所兼容，他们就会喜欢这里，从而诞生伟大的作品，同样，掌握更多的设计范式会让你在面对任何工作时都会显得游刃有余。        注重积累设计范式，做好分类整理，时常做好复习。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/talk-about-system-complex.html":{"url":"book/talk-about-system-complex.html","title":"热风-说一说系统复杂性","keywords":"","body":"说一说系统复杂性        业务在不断的发展，支持它的系统也变得越来越复杂，尤其是软件技术公司更是如此。规模巨大的科技企业更是深受复杂性所累，一个看似很小的功能，却在众多系统中来回穿梭，修改一次就如同翻江倒海，耗费了大量的人力和物力。为了抵抗复杂性，人们就摩拳擦掌跃跃欲试，甚至有些人为此专门成立了公司来售卖方法论，什么面向对象技术、面向服务体系结构以及微服务架构层出不穷，它们无一不打着降低复杂度和提升效率的幌子，但是没有一个能够兑现承诺。        业务发展赚来银子，雇佣更多的开发人员，而他们却深陷系统的复杂性之中，效率无法让业务得到满足，这难道就是技术的锅吗？是开发人员拙劣的设计带来的吗？这不是一家技术公司面临的问题，而这个广泛的问题是不是代表着大多数开发人员水平都是拙劣的呢？答案：并不是。系统变复杂的源头还是业务，本质就是业务人员自己没有理顺逻辑关系，做东西没有考虑旧有的规则以及如何应对用户的真实需求，而这些混乱的业务思考在只认逻辑的技术系统中落地，结果就是不断增加的复杂度。开发人员的低效，就是在同业务带来的复杂性斗争，在同没有想清楚的业务做斗争，费力的编故事来圆业务的锅，在做代偿。        既然问题源头不在技术，那开发人员是不是可以躺平了？非也。业务不按逻辑性办事，不代表技术不讲工程性，在技术工作中如果能够关注和治理系统复杂性，是不是可以减少一些苦难呢？        虽然复杂性的起源来自业务，但是开发人员在设计和实现系统时仍然需要对复杂性进行关注和治理，同时好的开发人员也会发现业务中存在的冗余和矛盾，推动业务人员不能以成本视角观察技术，而需要改进业务，避免所有涉众都会为混乱的业务以及产品架构买单。 复杂性是个必然        只要业务存活，系统的复杂性就成了必然，讽刺的是，只有不断增加的复杂性才代表活着。就拿简中互联网中的BAT三家来说，Alibaba的系统已经做的很具扩展性了，而当下ByteDance的系统越做越复杂，从体感上讲，后者要活的比前者滋润的多。        以Spring框架为例，从发布初期0.9.1版本的一万多行代码，到2.5版本的七万多行代码，复杂度增加了不少，但是从另一个方面看，它的使用人群变得非常广泛。业务和产品的演化是预料之中的，也是值得期待的，但它一定会导致复杂性。        面对复杂性，首先要理解它存在的必然性，观察它产生的原因，要比立刻着手修复它要重要的多。就像治理水患一样，疏要比堵更为有效，找到产生复杂性的原因，理解产生的原因，再动手也不迟。 必然会产生债务        如果所在的系统不会变的复杂，或者说不会持续给你带来治理复杂性的挑战，那就代表着业务已经停滞。发展中的业务，必然会带来系统上的复杂性，而这种必然也会同时带来债务。Ward Cunningham提出了一个说法，叫：技术债，它用来描述为了满足进度或用户期望而作出的设计让步。Deadline项目大部分都不会取得既定的业务指标，而它们给系统造成的伤害以及复杂性的上升却需要几倍的人力去挽回。        既然是债，就需要还，如果不还，就有风险或者问题。技术债是标的在技术产出物上的债务，技术产出物一般包括了硬件、操作系统、软件环境和应用程序。其中前三点，尤其是第二和三点，会构成隐形的技术债，最后一点会形成一般意义上显性的技术债。        隐形的技术债会带来隐形的成本，比如：使用较低版本的Java或者低版本的中间件，带来的问题就是需要花费更多的计算成本，如果能够定时升级，就会使得硬件的利用率得到提升，同时避免一些安全风险。显性的技术债往往是在应用程序中引入的，比如：由于项目时间过紧导致存在风险或者不合理的设计，以及由于项目技术人员能力问题引入的风险设计，无法应对实际系统（当下或短期可见）需要的计算指标。        这些技术债会拖慢后续系统的开发效率，增大应用的维护成本，就算使用良好的文档记录也无济于事，需要在技术债出现时给予评级，保证能够在未来的时间加以修复。隐形的技术债对使用者是透明的，但显性的则不然，它们大都是在应用的需求设计阶段引入的，没有系统的实现能够跑在业务需求前面，所以如何能够减少技术债的引入是考量一个架构师是否能够全面思考的关键点，通过一定模式化、具备防御性的架构设计，能够在略微增加项目时间的情况下，做到少负债，使得后续能够偿还债务，毕竟，有些债务的偿还成本过高，甚至难以偿还。 债务需要去偿还        如果不去管理和偿还债务，就会如同当下的经济一样，陷入通缩，这种螺旋式下坠会让人万劫不复。既然复杂性必然要来，那就要科学应对，既不做涸泽而渔的刺激性工作，也不要搞运动式竞赛，妄图不放弃现有利益分配和路径的前提下，通过奋力一击就能回到原有状态，这就是痴人说梦了。业务人员要审视自己负责的业务，哪些存在冗余，产品人员要整理自己负责的产品，哪些没有人用，所有人员都要参与其中，组织中没有一种角色天生就是成本。        将债务分门别类的登记和管理好，搞清楚债务的起因，看看能否在来源上加以控制。要解决开发人员的效率问题，就要视拖慢业务节奏的慢发展为常态，这是在给业务还债的周期，需要多方理解和配合。通过合理的流程设计，降低开发人员负载，给予开发人员时间能够将手工操作转变为系统操作，避免出现问题。虽然开发人员的需求吞吐下降了，但是系统的稳定性提升了，而且开发人员也会在这个阶段帮助业务找到很多问题。新需求的设计方案以及旧有系统的改进方案都需要准备好扩展性，清晰的API设计以及可能扩展的SPI预留，模块化设计，这些都可以帮助开发人员整理和归纳好技术资产，目的就是未来奔跑过程中不会出现绊脚的情况。归纳好，整理好，屋子不能乱，好架构师就是好物业。        债务偿还周期需要多方相互理解，这也是对组织的终极考验，顺风仗容易打，但是逆风局才是考验真本事和定力的时候，不成熟的组织将会在债务偿还周期中消亡。如果组织没能突破这个必然，也要想开些，万物的发展就是这样，突破必然的事物也会必然出现，正所谓：沉舟侧畔千帆过，病树前头万木春。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "},"book/how-do-i-write-a-design-doc.html":{"url":"book/how-do-i-write-a-design-doc.html","title":"热风-我怎么写概要设计？","keywords":"","body":"我怎么写概要设计？        软件开发既是一门艺术，也是一种工程。艺术性体现在它的抽象以及从无到有的创作性工作，工程性体现在它的产出物不仅需要有实用价值，而且产出的过程需要做到标准化、可度量和能复制。如果从工程化的角度去观察软件开发，会发现它是由：需求分析、概要设计、详细设计、开发测试和部署维护等多个阶段组成。这些阶段中，设计阶段，尤其是概要设计，是从无到有的阶段。建筑工人按照图纸去盖房子，软件工程师会按照设计文档去搭建系统，而在此之前，建筑师在图纸上已经建起了概念中的房子，软件设计师也是在设计文档中描述了系统的蓝图，二者的共性在于，他们都在心里建立了原型，只有想得越清楚，做的才会越好。        一般没有无缘而起的软件开发，它们都属于项目的一部分，项目的目标是完成任务和创造利润，除了软件开发，还包括：客户需求、合同签署以及交付维保等。软件开发被项目流程中的其他部分夹在中间，是最容易盘剥的。软件开发在中国很容易被视为成本，而项目各部分涉及到了很多角色，比如：客户、销售、运营、产品、商务以及董事长等，这里面最终容易欺负的角色就是研发人员了，细心的你会发现涉众角色里没有研发人员，因为中国公司一向把研发视为成本资源，是上不了台面的，“它们”只是作为达成目的临时合作的人而已。中国公司还是会把形象做足，会宣导软件交付的结果，创造了所谓客户价值，但是软件如何设计开发，就是研发人员自己的事。这种资源型看法和做事方式，就会导致研发人员对设计过程能省则省，由于时间被压缩，很多内容都是边开发边想（或者设计），而这种中国式敏捷，会带领行业走向冥界。客观地讲，设计之于软件开发，是极为重要的，明知道设计很重要，但是轻设计的目的究竟是为了省资源，还是满足管理人员那狭隘的心里呢？我想两者都会有吧。        轻设计是在这种环境下的自然选择，但我还是想说，软件在更新维护过程中，由于需求遗漏或设计缺失引出的问题，最终还是需要研发人员来做代偿。软件的问题，不应该是研发人员自己负责的吗？出了问题就扣钱惩罚。如果简单思考确实如此，但是如果不压榨这个环节，问题是不是会少一些？心虽不平，但现实如此，毕竟系统的维护还是研发人员在做，为了减少痛苦，就需要提升设计效率，虽然有很多指导设计的书籍，以及方法，比如：RUP，但是这些体系化的做法很难在有限的时间里充分展开。有没有好的方法或者模式，能够在相同的时间下，进行更有效的概要设计呢？        用一套简单且必要的概要设计模版来指导设计，会是一个好的解决方案。 概要设计模版        每个公司和组织都可能有属于自己的项目设计模版，如果是项目制的公司，可能连模版都没有。不论有没有模版，使用模版来思考概要设计，会使得思考相对完善，避免后续出现因为设计缺失而导致的返工问题。以下是一个可供参考的概要设计模版，以Markdown的形式呈现： # 当前问题 # 解决方案 # 设计目标 # [总体|主要]架构（如果客户端和服务端都有，选择：总体，否则：主要） # 系统用例（可选，一般有两端多种涉众参与才会需要） # [客户端｜服务端]概念模型(如果只有一端，则可以都不选择，另外可以按需增加子主题) # [客户端｜服务端]模块说明(如果只有一端，则可以都不选择，另外可以按需增加子主题) # [客户端｜服务端]关键点（可选，另外可以按需增加子主题）        可以看到，概要设计模版包含了8个部分，从当前问题开始，到关键点结束。当前问题部分是简要的说明项目需要解决什么问题，以及解决问题需要采取何种解决方案，针对这个方案以及项目，设计目标是什么，前三点的目的是解决是什么、为什么以及怎么办。接下来的内容是系统架构、用例、模型和模块，是解决方案从整体到细节的拆分。整体内容会形成文字以及表格，基于这些内容可以提炼出PPT。 概要设计示例        根据概要设计模版，接下来展示一些示例，这样会更容易理解一些。示例中的案例摘自一些项目，包括：客户端ioc容器、关系数据库服务和分布式缓存服务等。 概要设计示例-当前问题        项目：客户端IoC容器。 客户端组件，尤其是中间件客户端，其自身如何组织形式，管理依赖以及生命周期是一个难以回避的问题。虽然在Java生态中，有Spring框架来实现对象的依赖注入和生命周期管理，但是客户端组件不建议直接依赖Spring。假设中间件客户端组件依赖了Spring框架，那么就会将Spring容器的版本隐形的传递给客户端的使用者，一来违反了迪米特法则，让中间件客户端感知了过多的依赖，二来Spring功能过于丰富，仅为了依赖注入和生命周期功能而引入Spring有些得不偿失。 可以选择不使用任何IoC（Inversion of Control）框架来组织中间件客户端，从用户使用角度上看，没有任何问题，但是客户端自身的扩展性以及维护性都会降低，随着客户端的不断迭代，代码量的增长，没有统一的依赖注入和生命周期管理会成为制约客户端发展的一个重要因素。 ## 依赖注入 客户端的实现由容器进行组装，同时用户对客户端的扩展也会由容器载入，除了方便用户扩展框架，还实现了框架代码与用户扩展代码平权。 ## 生命周期 容器负责实例化客户端，同时也会维护客户端对象的生命周期，不仅能够保证用户能够使用状态安全的客户端，而且对客户端扩展更加友好。        当前问题这一部分，提出和描述清楚需要解决的问题即可，概要设计需要针对问题进行阐述，没有明确需要解决的问题，很难在后续维护过程中翻看设计时产生共情。 概要设计示例-解决方案        项目：客户端IoC容器。 构建一个支持依赖注入和生命周期管理的IoC容器，该容器应该支持Java9+之后的模块化，采用构造函数和setter注入。中间件客户端依赖该IoC容器，使用其依赖注入服务，同时依靠它的生命周期管理来实现启动和停止等核心逻辑。        可以看到针对问题有各个层面的解法，从而汇集成为解决方案。解决方案最好以图形化的方式加以呈现。 概要设计示例-设计目标        项目：关系数据库服务。        针对解决方案，概要设计需要在设计目标中声明需要达成的技术目标，比如：M个N型规格实例下，做到不低于X个TPS的服务能力，99.9%均时访问最大延迟不超过W个毫秒。设计目标是为了更好的检测概要设计是否完备，同时也定义好了与上游的契约。可以看到，文字版的概要设计，也很容易转换为PPT。 概要设计示例-架构        项目：客户端IoC容器。 面向服务设计容器功能，使用扩展点以及责任链模式连接各服务实例。        概要设计的架构部分需要按模块并使用分层描述，标注出主要的实体、扩展点以及服务。每一层的职责和粗粒度模块的功能需要描述清楚。概要设计文档不仅是用来描述如何解决问题的思路，也需要作为后续系统更新维护的参考。 概要设计示例-系统用例        项目：分布式缓存服务。        系统如果涉及多种用户，在概要设计文档中需要描述用例，最起码是粗粒度的用例。以参与者的视角观察系统，对设计系统很有裨益，针对不同的涉众设计开发不同的接口，将会有利于后续的系统维护和扩展，减少相互影响的情况。 概要设计示例-概念模型        项目：关系数据库服务。        概要设计文档对于概念模型的描述需要包含主要的模型以及模型之间的关系，这些模型可以理解为领域对象，目的是通过演绎模型来解决问题。 概要设计示例-模块说明        项目：客户端IoC容器。 # 模块说明 一个支持ioc的容器，有5个模块组成： 1. xworks-ioc-bean：bean的定义，包括bean的生成，管理与获取； 2. xworks-ioc-core：resource的定义，是对资源的抽象以及基础服务，比如：pipeline服务； 3. xworks-ioc-container：container的定义，包括获取bean以及生命周期维护； 4. xworks-ioc-common：common部分，包含了ServiceRegistry； 5. xworks-ioc-test：test部分，支持测试用例的编写。 ## 模块说明：xworks-ioc-common 1. 简要描述 提供了面向ServiceLoader的服务加载和管理功能，以单例的形式进行创建与管理，支持自引用对服务的暴露。 2. 领域实体 Service：服务，xworks-ioc框架中承担核心逻辑的实体。 3. 接口服务 ServiceRegistry：构建服务注册，同时将服务引用管理在自身，用户通过构造ServiceRegistry来获得服务。ServiceRegistry托管的服务均为单例。 方法 描述 getService(Class clazz): T 根据类型获取服务，如果没有返回空 getService(String name, Class clazz): T 根据指定的名称和类型获取服务实例，如果没有返回空 listServices(Class clazz) 根据类型获取服务列表，使用@Order进行了排序，排序方式为升序 4. 扩展服务 ServiceRegistryAware：对于ServiceRegistry托管的服务，如果类型实现了该接口，将会得到ServiceRegistry的引用。 方法 描述 setServiceRegistry(ServiceRegistry serviceRegistry): void 设置ServiceRegistry 5. 依赖组件 组件 描述 xworks-common-model 基础支持 xworks-common-tool 基础支持 slf4j-api 日志        模块说明需要描述主要的模块以及关系，模块可以理解为maven中的一个artifact或者一个jar。除了分别介绍模块的主要职责和领域实体，还需要包括模块提供的接口服务、扩展以及主要依赖。 概要设计示例-关键点        项目：客户端IoC容器。 ## 关键点：容器初始化 需要依靠BeanContainerBuilder来进行容器的构建，在BeanContainerBuilder中，会开始创建ServiceRegistry，这一步没有放到BeanContainer中，其目的就是为了让BeanContainer也被托管在ServiceRegistry中，兑现ioc个服务的平权。完成Container的构建工作，当构建完成Container后，就会运行该阶段，与Spring容器中的refresh过程类似。 需要实现扩展接口：ContainerInitStage 步骤 备注 检查状态，只有NEW状态可以初始化，也就是compareAndSet(NEW, INIT) 不属于stage，而是getBean(String)中的内容。 解析并加载配置文件 当前BeanFactory中没有定义，才会找寻parent，No BeanDefinition。不属于stage，而是getBean(String)中的内容。需要创建Bean时，开始走stage。 完成BeanDefinition的注册 设置当前BeanFactory#parent 如果BeanContainer有parent，则会设置当前BeanContainer中的BeanFactory服务 实例化当前容器中的所有单例Bean 通过遍历所有的BeanDefinition来获取一遍所有的Bean 按照顺序初始化所有的单例Bean 使用LifecycleRegistry进行初始化Bean回调        概要设计中一般对关键细节的描述会以关键点的形式来呈现，因为关键细节决定成败，所以对这些关键点做较为详细的描述是非常有必要的，往往这也是系统中核心创新性的体现。 概要设计策略        概要设计的策略在于：对于目标需要明确，模块拆分需要与模型设计结合着做，关键细节一定要完成推演，尽可能的使用图与表的形式。 By hot-wind，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2024-12-02 03:24:02 "}}